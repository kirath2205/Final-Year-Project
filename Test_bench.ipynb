{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Test_bench.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOFDFyyykGgz7gXQ1vU2e8e",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kirath2205/Final-Year-Project/blob/Test_bench/Test_bench.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcyH2uCmqDOG"
      },
      "source": [
        "import keras,os\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential,Model\n",
        "from keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from keras.initializers import glorot_uniform\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVVahuJ1t7HD"
      },
      "source": [
        "def select_dataset(index):\n",
        "\n",
        "  if(index==1):\n",
        "    fashion_mnist = tf.keras.datasets.fashion_mnist.load_data()\n",
        "    (X_train, y_train), (X_test, y_test) = fashion_mnist\n",
        "\n",
        "  elif(index==2):\n",
        "    cifar_10 = tf.keras.datasets.cifar10.load_data()\n",
        "    (X_train, y_train), (X_test, y_test) = cifar_10\n",
        "  \n",
        "  elif(index==3):\n",
        "    cifar_100 = tf.keras.datasets.cifar100.load_data()\n",
        "    (X_train, y_train), (X_test, y_test) = cifar_100\n",
        "\n",
        "    \n",
        "  return X_train,y_train,X_test,y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQV52_6GvrRS"
      },
      "source": [
        "def pre_processing_fashion_mnist(X_train,y_train,X_test,y_test,image_channel):\n",
        "  img_width = X_train.shape[1]\n",
        "  img_height = X_train.shape[2]\n",
        "  input_shape = (img_width, img_height, image_channel)\n",
        "  \n",
        "  # normalize data\n",
        "  X_train, X_test = X_train / 255, X_test / 255\n",
        "\n",
        "  # reshape input \n",
        "  X_train = X_train.reshape(X_train.shape[0], *input_shape)\n",
        "  X_test = X_test.reshape(X_test.shape[0], *input_shape)\n",
        "\n",
        "  # one-hot\n",
        "  y_train = tf.keras.utils.to_categorical(y_train)\n",
        "  y_test = tf.keras.utils.to_categorical(y_test)\n",
        "\n",
        "  return X_train,y_train,X_test,y_test,input_shape\n",
        "\n",
        "def pre_processing_cifar10(X_train,y_train,X_test,y_test,image_channel):\n",
        "  X_train = X_train.astype('float32')\n",
        "  X_test = X_test.astype('float32')\n",
        "\n",
        "  y_train = keras.utils.to_categorical(y_train)\n",
        "  y_test = keras.utils.to_categorical(y_test)\n",
        "\n",
        "  img_width = X_train[0].shape[0]\n",
        "  img_height = X_train[0].shape[1]\n",
        "  image_channel = image_channel\n",
        "\n",
        "  input_shape=(img_width,img_height,image_channel) \n",
        "  print(input_shape)\n",
        "  return X_train,y_train,X_test,y_test,input_shape\n",
        "  \n",
        "def pre_processing_cifar100(X_train,y_train,X_test,y_test,image_channel):\n",
        "  X_train = X_train.astype('float32')\n",
        "  X_test = X_test.astype('float32')\n",
        "\n",
        "  y_train = keras.utils.to_categorical(y_train)\n",
        "  y_test = keras.utils.to_categorical(y_test)\n",
        "\n",
        "  img_width = X_train[0].shape[0]\n",
        "  img_height = X_train[0].shape[1]\n",
        "  image_channel = image_channel\n",
        "\n",
        "  input_shape=(img_width,img_height,image_channel) \n",
        "  print(input_shape)\n",
        "  return X_train,y_train,X_test,y_test,input_shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDvpMVLzu3if"
      },
      "source": [
        "def define_model_vgg16(image_shape,total_classes):\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(input_shape=image_shape,filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "  model.add(Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "  model.add(MaxPool2D(pool_size=(2,2),strides=(2,2),padding='same'))\n",
        "  model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "  model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "  model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
        "  model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "  model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "  model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "  model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
        "  model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "  model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "  model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "  model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
        "  model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "  model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "  model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "  model.add(MaxPool2D(pool_size=(2,2),strides=(2,2),padding='same'))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(units=4096,activation=\"relu\"))\n",
        "  model.add(Dense(units=4096,activation=\"relu\"))\n",
        "  model.add(Dense(units=total_classes, activation=\"softmax\"))\n",
        "  opt = keras.optimizers.Adam(learning_rate=0.001)\n",
        "  model.compile(optimizer=opt, loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])\n",
        "  \n",
        "  return model\n",
        "\n",
        "def define_model_standard_cnn(image_shape,total_classes):\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=image_shape))\n",
        "  model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "  model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
        "  model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "  model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
        "  model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(256, activation='relu'))\n",
        "  model.add(Dense(128, activation='relu'))\n",
        "  model.add(Dense(total_classes, activation='softmax'))\n",
        "  opt = keras.optimizers.Adam(learning_rate=0.001)\n",
        "  model.compile(optimizer=opt,loss=keras.losses.categorical_crossentropy,metrics=['accuracy'])\n",
        "  return model\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89MAaxp2hqAR"
      },
      "source": [
        "def identity_block(X, f, filters, stage, block):\n",
        "    # defining name basis\n",
        "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
        "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
        "\n",
        "    # Retrieve Filters\n",
        "    F1, F2, F3 = filters\n",
        "\n",
        "    # Save the input value. We'll need this later to add back to the main path. \n",
        "    X_shortcut = X\n",
        "\n",
        "    # First component of main path\n",
        "    X = Conv2D(filters = F1, kernel_size = (1, 1), strides = (1,1), padding = 'valid', name = conv_name_base + '2a', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3, name = bn_name_base + '2a')(X)\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    # Second component of main path\n",
        "    X = Conv2D(filters = F2, kernel_size = (f, f), strides = (1,1), padding = 'same', name = conv_name_base + '2b', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3, name = bn_name_base + '2b')(X)\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    # Third component of main path\n",
        "    X = Conv2D(filters = F3, kernel_size = (1, 1), strides = (1,1), padding = 'valid', name = conv_name_base + '2c', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3, name = bn_name_base + '2c')(X)\n",
        "\n",
        "    # Final step: Add shortcut value to main path, and pass it through a RELU activation\n",
        "    X = Add()([X, X_shortcut])\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    return X\n",
        "\n",
        "def convolutional_block(X, f, filters, stage, block, s = 2):\n",
        "    # defining name basis\n",
        "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
        "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
        "    \n",
        "    # Retrieve Filters\n",
        "    F1, F2, F3 = filters\n",
        "    \n",
        "    # Save the input value\n",
        "    X_shortcut = X\n",
        "\n",
        "\n",
        "    ##### MAIN PATH #####\n",
        "    # First component of main path \n",
        "    X = Conv2D(F1, (1, 1), strides = (s,s), name = conv_name_base + '2a', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3, name = bn_name_base + '2a')(X)\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    # Second component of main path\n",
        "    X = Conv2D(filters=F2, kernel_size=(f, f), strides=(1, 1), padding='same', name=conv_name_base + '2b', kernel_initializer=glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis=3, name=bn_name_base + '2b')(X)\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    # Third component of main path\n",
        "    X = Conv2D(filters=F3, kernel_size=(1, 1), strides=(1, 1), padding='valid', name=conv_name_base + '2c', kernel_initializer=glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis=3, name=bn_name_base + '2c')(X)\n",
        "\n",
        "    \n",
        "    ##### SHORTCUT PATH ####\n",
        "    X_shortcut = Conv2D(F3, (1, 1), strides = (s,s), name = conv_name_base + '1', kernel_initializer = glorot_uniform(seed=0))(X_shortcut)\n",
        "    X_shortcut = BatchNormalization(axis = 3, name = bn_name_base + '1')(X_shortcut)\n",
        "\n",
        "    # Final step: Add shortcut value to main path, and pass it through a RELU activation\n",
        "    X = Add()([X, X_shortcut])\n",
        "    X = Activation('relu')(X)\n",
        "    \n",
        "    return X\n",
        "\n",
        "def ResNet50(input_shape , total_classes ):   \n",
        "    \n",
        "    X_input = Input(input_shape)\n",
        "    # Zero-Padding\n",
        "    X = ZeroPadding2D((3, 3))(X_input)\n",
        "    \n",
        "    # Stage 1\n",
        "    X = Conv2D(32, (7, 7), strides = (1, 1), name = 'conv1', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3, name = 'bn_conv1')(X)\n",
        "    X = Activation('relu')(X)\n",
        "    X = MaxPooling2D((3, 3))(X)\n",
        "\n",
        "    # Stage 2\n",
        "    X = convolutional_block(X, f = 3, filters = [32, 32, 128], stage = 2, block='a', s = 1)\n",
        "    X = identity_block(X, 3, [32, 32, 128], stage=2, block='b')\n",
        "    X = identity_block(X, 3, [32, 32, 128], stage=2, block='c')\n",
        "\n",
        "    # Stage 3\n",
        "    X = convolutional_block(X, f = 3, filters = [64, 64, 256], stage = 3, block='a', s = 2)\n",
        "    X = identity_block(X, 3, [64, 64, 256], stage=3, block='b')\n",
        "    X = identity_block(X, 3, [64, 64, 256], stage=3, block='c')\n",
        "    X = identity_block(X, 3, [64, 64, 256], stage=3, block='d')\n",
        "\n",
        "    # Stage 4 \n",
        "    X = convolutional_block(X, f = 3, filters = [128, 128, 512], stage = 4, block='a', s = 2)\n",
        "    X = identity_block(X, 3, [128, 128, 512], stage=4, block='b')\n",
        "    X = identity_block(X, 3, [128, 128, 512], stage=4, block='c')\n",
        "    X = identity_block(X, 3, [128, 128, 512], stage=4, block='d')\n",
        "    X = identity_block(X, 3, [128, 128, 512], stage=4, block='e')\n",
        "    X = identity_block(X, 3, [128, 128, 512], stage=4, block='f')\n",
        "\n",
        "    # Stage 5 \n",
        "    X = convolutional_block(X, f = 3, filters = [256,256, 1024], stage = 5, block='a', s = 2)\n",
        "    X = identity_block(X, 3, [256,256, 1024], stage=5, block='b')\n",
        "    X = identity_block(X, 3, [256,256, 1024], stage=5, block='c')\n",
        "\n",
        "    # AVGPOOL\n",
        "    X = AveragePooling2D(pool_size=(2,2), name='avg_pool')(X)\n",
        "    \n",
        "    # output layer\n",
        "    X = Flatten()(X)\n",
        "    X = Dense(total_classes, activation='softmax', name='fc' + str(total_classes), kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "\n",
        "    model = Model(inputs = X_input, outputs = X, name='ResNet50')\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaRAX4NsO-qR"
      },
      "source": [
        "def identity_block(layer,f,nfilters):\n",
        "    Xshortcut=layer\n",
        "    F1,F2,F3=nfilters\n",
        "    layer=Conv2D(F1,(1,1),padding='valid')(layer)\n",
        "    layer=BatchNormalization(axis=-1)(layer)\n",
        "    layer=Activation('relu')(layer)\n",
        "    \n",
        "    layer=Conv2D(F2,(f,f),padding='same')(layer)\n",
        "    layer=BatchNormalization(axis=-1)(layer)\n",
        "    layer=Activation('relu')(layer)\n",
        "    \n",
        "    layer=Conv2D(F3,(1,1),padding='valid')(layer)\n",
        "    layer=BatchNormalization(axis=-1)(layer)\n",
        "    \n",
        "    layer=Add()([Xshortcut,layer])\n",
        "    layer=Activation('relu')(layer)\n",
        "    return(layer)\n",
        "\n",
        "def Conv_block(layer,f,nfilters,s=2):\n",
        "    Xshortcut=layer\n",
        "    F1,F2,F3=nfilters\n",
        "    layer=Conv2D(F1,(1,1),strides=(s,s))(layer)\n",
        "    layer=BatchNormalization(axis=-1)(layer)\n",
        "    layer=Activation('relu')(layer)\n",
        "    \n",
        "    layer=Conv2D(F2,(f,f),padding='same')(layer)\n",
        "    layer=BatchNormalization(axis=-1)(layer)\n",
        "    layer=Activation('relu')(layer)\n",
        "    \n",
        "    layer=Conv2D(F3,(1,1))(layer)\n",
        "    layer=BatchNormalization(axis=-1)(layer)\n",
        "    \n",
        "    Xshortcut=Conv2D(F3,(1,1),strides=(s,s))(Xshortcut)\n",
        "    Xshortcut=BatchNormalization(axis=-1)(Xshortcut)\n",
        "    \n",
        "    layer=Add()([Xshortcut,layer])\n",
        "    layer=Activation('relu')(layer)\n",
        "    return(layer)\n",
        "\n",
        "def Resnet50(img_shape,numclasses):\n",
        "    img_inp=Input(shape=img_shape,dtype=tf.float32)\n",
        "    X=ZeroPadding2D((3,3))(img_inp)\n",
        "    X=Conv2D(64,(7,7),strides=(2,2))(X)\n",
        "    X=BatchNormalization(axis=-1)(X)\n",
        "    X=Activation('relu')(X)\n",
        "    \n",
        "    X=MaxPooling2D((3,3),strides=(2,2),padding='same')(X)\n",
        "    X=Conv_block(X,3,[64,64,256],s=1)\n",
        "    X=identity_block(X,3,[64,64,256])\n",
        "    X=identity_block(X,3,[64,64,256])\n",
        "    \n",
        "    X=Conv_block(X,3,[128,128,512],s=2)\n",
        "    X=identity_block(X,3,[128,128,512])\n",
        "    X=identity_block(X,3,[128,128,512])\n",
        "    X=identity_block(X,3,[128,128,512])\n",
        "    \n",
        "    X=Conv_block(X,3,[256,256,1024],s=2)\n",
        "    X=identity_block(X,3,[256,256,1024])\n",
        "    X=identity_block(X,3,[256,256,1024])\n",
        "    X=identity_block(X,3,[256,256,1024])\n",
        "    X=identity_block(X,3,[256,256,1024])\n",
        "    X=identity_block(X,3,[256,256,1024])\n",
        "    \n",
        "    X=Conv_block(X,3,[512,512,2048],s=2)\n",
        "    X=identity_block(X,3,[512,512,2048])\n",
        "    X=identity_block(X,3,[512,512,2048])\n",
        "    \n",
        "    X=AveragePooling2D((7,7))(X)\n",
        "    \n",
        "    X=Flatten()(X)\n",
        "    X=Dense(numclasses,activation='softmax')(X)\n",
        "    model=Model(inputs=img_inp,outputs=X)\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    \n",
        "    return(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jgsl1X29yf2k"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "def train_model_vgg16(X_train,y_train,X_test,y_test,epochs=10,batch_size=256):\n",
        "  \n",
        "  callbacks = [ \n",
        "    tf.keras.callbacks.ModelCheckpoint('best_model', monitor='val_accuracy', verbose=1, save_best_only=True),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau( factor = 0.1, patience = 3, min_lr = 0.00001, verbose = 1 )\n",
        "  ]\n",
        "  history=model.fit(X_train, y_train, epochs = epochs, batch_size = batch_size,callbacks = callbacks, verbose = 1,validation_data=(X_test,y_test))\n",
        "  return history\n",
        "\n",
        "def train_model_standard_cnn(X_train,y_train,X_test,y_test,epochs=10,batch_size=256):\n",
        "\n",
        "  history=model.fit(X_train , y_train , epochs = epochs , validation_data=(X_test,y_test),batch_size=batch_size)\n",
        "  return history\n",
        "\n",
        "def train_model_resnet50(X_train,y_train,X_test,y_test,epochs=10):\n",
        "\n",
        "  history=model.fit(X_train , y_train , epochs = epochs , validation_data=(X_test,y_test),batch_size=256)\n",
        "  return history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeJvjs7VO9-u"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6AWutrFy4V8"
      },
      "source": [
        "def make_prediction(model,X_test,index):\n",
        "\n",
        "  return model.predict(X_test(index))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0UiQLmEn1vuV",
        "outputId": "a2198bad-d047-43aa-ad63-013755af07c0"
      },
      "source": [
        "\n",
        "'''X_train,y_train,X_test,y_test=select_dataset(1)\n",
        "X_train,y_train,X_test,y_test,image_shape=pre_processing_fashion_mnist(X_train,y_train,X_test,y_test,1)\n",
        "model=define_model(image_shape,10)\n",
        "train_model(X_train,y_train,X_test,y_test)'''\n",
        "\n",
        "'''X_train,y_train,X_test,y_test=select_dataset(2)\n",
        "X_train,y_train,X_test,y_test,image_shape=pre_processing_cifar10(X_train,y_train,X_test,y_test,3)\n",
        "model=define_model_vgg16(image_shape,10)\n",
        "train_model_vgg16(X_train,y_train,X_test,y_test,epochs=1)\n",
        "'''\n",
        "X_train,y_train,X_test,y_test=select_dataset(3)\n",
        "X_train,y_train,X_test,y_test,image_shape=pre_processing_cifar100(X_train,y_train,X_test,y_test,3)\n",
        "model=ResNet50(image_shape,100)\n",
        "history=train_model_resnet50(X_train,y_train,X_test,y_test,epochs=100)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "169009152/169001437 [==============================] - 11s 0us/step\n",
            "169017344/169001437 [==============================] - 11s 0us/step\n",
            "(32, 32, 3)\n",
            "Epoch 1/100\n",
            "196/196 [==============================] - 72s 177ms/step - loss: 4.2713 - accuracy: 0.0785 - val_loss: 34.5616 - val_accuracy: 0.0328\n",
            "Epoch 2/100\n",
            "196/196 [==============================] - 32s 166ms/step - loss: 3.7096 - accuracy: 0.1506 - val_loss: 305.6469 - val_accuracy: 0.0178\n",
            "Epoch 3/100\n",
            "196/196 [==============================] - 32s 166ms/step - loss: 3.4023 - accuracy: 0.1924 - val_loss: 3.5158 - val_accuracy: 0.1681\n",
            "Epoch 4/100\n",
            "196/196 [==============================] - 32s 166ms/step - loss: 3.0800 - accuracy: 0.2535 - val_loss: 3.2940 - val_accuracy: 0.2118\n",
            "Epoch 5/100\n",
            "196/196 [==============================] - 32s 165ms/step - loss: 2.8295 - accuracy: 0.3022 - val_loss: 2.9999 - val_accuracy: 0.2669\n",
            "Epoch 6/100\n",
            "196/196 [==============================] - 32s 165ms/step - loss: 2.6172 - accuracy: 0.3406 - val_loss: 2.9340 - val_accuracy: 0.2915\n",
            "Epoch 7/100\n",
            "196/196 [==============================] - 32s 165ms/step - loss: 2.4411 - accuracy: 0.3755 - val_loss: 3.9177 - val_accuracy: 0.2896\n",
            "Epoch 8/100\n",
            "196/196 [==============================] - 32s 164ms/step - loss: 2.2567 - accuracy: 0.4127 - val_loss: 2.8340 - val_accuracy: 0.3117\n",
            "Epoch 9/100\n",
            "196/196 [==============================] - 32s 164ms/step - loss: 2.0585 - accuracy: 0.4566 - val_loss: 3.0459 - val_accuracy: 0.3305\n",
            "Epoch 10/100\n",
            "196/196 [==============================] - 32s 165ms/step - loss: 1.7675 - accuracy: 0.5214 - val_loss: 3.0083 - val_accuracy: 0.3420\n",
            "Epoch 11/100\n",
            "196/196 [==============================] - 32s 165ms/step - loss: 1.4844 - accuracy: 0.5867 - val_loss: 3.0877 - val_accuracy: 0.3404\n",
            "Epoch 12/100\n",
            "196/196 [==============================] - 32s 165ms/step - loss: 1.2166 - accuracy: 0.6505 - val_loss: 3.9135 - val_accuracy: 0.3429\n",
            "Epoch 13/100\n",
            "196/196 [==============================] - 32s 165ms/step - loss: 0.9707 - accuracy: 0.7194 - val_loss: 4.1995 - val_accuracy: 0.3176\n",
            "Epoch 14/100\n",
            "196/196 [==============================] - 32s 165ms/step - loss: 0.7724 - accuracy: 0.7755 - val_loss: 6.7970 - val_accuracy: 0.3365\n",
            "Epoch 15/100\n",
            "196/196 [==============================] - 32s 165ms/step - loss: 0.6107 - accuracy: 0.8188 - val_loss: 3.5023 - val_accuracy: 0.3531\n",
            "Epoch 16/100\n",
            "196/196 [==============================] - 33s 169ms/step - loss: 0.3948 - accuracy: 0.8867 - val_loss: 3.9234 - val_accuracy: 0.3475\n",
            "Epoch 17/100\n",
            "196/196 [==============================] - 32s 165ms/step - loss: 0.3146 - accuracy: 0.9098 - val_loss: 4.0078 - val_accuracy: 0.3470\n",
            "Epoch 18/100\n",
            "196/196 [==============================] - 32s 165ms/step - loss: 0.2863 - accuracy: 0.9194 - val_loss: 4.5049 - val_accuracy: 0.3364\n",
            "Epoch 19/100\n",
            "196/196 [==============================] - 32s 165ms/step - loss: 0.2459 - accuracy: 0.9300 - val_loss: 4.5878 - val_accuracy: 0.3355\n",
            "Epoch 20/100\n",
            "196/196 [==============================] - 32s 165ms/step - loss: 0.2461 - accuracy: 0.9287 - val_loss: 4.6562 - val_accuracy: 0.3537\n",
            "Epoch 21/100\n",
            "196/196 [==============================] - 32s 164ms/step - loss: 0.2495 - accuracy: 0.9279 - val_loss: 4.4029 - val_accuracy: 0.3516\n",
            "Epoch 22/100\n",
            "196/196 [==============================] - 32s 165ms/step - loss: 0.2676 - accuracy: 0.9241 - val_loss: 4.5806 - val_accuracy: 0.3386\n",
            "Epoch 23/100\n",
            "196/196 [==============================] - 32s 164ms/step - loss: 0.2159 - accuracy: 0.9391 - val_loss: 4.9555 - val_accuracy: 0.3403\n",
            "Epoch 24/100\n",
            "196/196 [==============================] - 32s 164ms/step - loss: 0.1930 - accuracy: 0.9475 - val_loss: 5.9288 - val_accuracy: 0.3363\n",
            "Epoch 25/100\n",
            "196/196 [==============================] - 32s 165ms/step - loss: 0.1654 - accuracy: 0.9547 - val_loss: 5.3310 - val_accuracy: 0.3122\n",
            "Epoch 26/100\n",
            "196/196 [==============================] - 33s 169ms/step - loss: 0.1485 - accuracy: 0.9589 - val_loss: 4.8053 - val_accuracy: 0.3490\n",
            "Epoch 27/100\n",
            "196/196 [==============================] - 32s 164ms/step - loss: 0.1483 - accuracy: 0.9591 - val_loss: 4.8381 - val_accuracy: 0.3454\n",
            "Epoch 28/100\n",
            "196/196 [==============================] - 32s 165ms/step - loss: 0.1691 - accuracy: 0.9502 - val_loss: 8.6644 - val_accuracy: 0.3276\n",
            "Epoch 29/100\n",
            "196/196 [==============================] - 32s 164ms/step - loss: 0.1871 - accuracy: 0.9451 - val_loss: 5.1903 - val_accuracy: 0.3430\n",
            "Epoch 30/100\n",
            "196/196 [==============================] - 32s 165ms/step - loss: 0.1781 - accuracy: 0.9485 - val_loss: 4.7594 - val_accuracy: 0.3540\n",
            "Epoch 31/100\n",
            "196/196 [==============================] - 32s 165ms/step - loss: 0.1431 - accuracy: 0.9586 - val_loss: 5.6418 - val_accuracy: 0.3356\n",
            "Epoch 32/100\n",
            "196/196 [==============================] - 32s 165ms/step - loss: 0.1201 - accuracy: 0.9658 - val_loss: 5.2440 - val_accuracy: 0.3378\n",
            "Epoch 33/100\n",
            "196/196 [==============================] - 32s 165ms/step - loss: 0.1121 - accuracy: 0.9671 - val_loss: 5.1738 - val_accuracy: 0.3413\n",
            "Epoch 34/100\n",
            "196/196 [==============================] - 32s 164ms/step - loss: 0.1195 - accuracy: 0.9650 - val_loss: 5.5746 - val_accuracy: 0.3371\n",
            "Epoch 35/100\n",
            "196/196 [==============================] - 32s 164ms/step - loss: 0.1445 - accuracy: 0.9558 - val_loss: 5.4793 - val_accuracy: 0.3352\n",
            "Epoch 36/100\n",
            "196/196 [==============================] - 32s 164ms/step - loss: 0.1407 - accuracy: 0.9590 - val_loss: 5.4287 - val_accuracy: 0.3522\n",
            "Epoch 37/100\n",
            "196/196 [==============================] - 32s 164ms/step - loss: 0.2453 - accuracy: 0.9265 - val_loss: 4.8758 - val_accuracy: 0.3540\n",
            "Epoch 38/100\n",
            "196/196 [==============================] - 32s 164ms/step - loss: 0.0965 - accuracy: 0.9728 - val_loss: 4.8176 - val_accuracy: 0.3631\n",
            "Epoch 39/100\n",
            "196/196 [==============================] - 32s 165ms/step - loss: 0.0687 - accuracy: 0.9821 - val_loss: 4.9620 - val_accuracy: 0.3647\n",
            "Epoch 40/100\n",
            "196/196 [==============================] - 32s 165ms/step - loss: 0.0572 - accuracy: 0.9850 - val_loss: 5.2702 - val_accuracy: 0.3521\n",
            "Epoch 41/100\n",
            "196/196 [==============================] - 32s 165ms/step - loss: 0.0660 - accuracy: 0.9824 - val_loss: 5.2942 - val_accuracy: 0.3556\n",
            "Epoch 42/100\n",
            "196/196 [==============================] - 32s 165ms/step - loss: 0.1130 - accuracy: 0.9661 - val_loss: 5.2408 - val_accuracy: 0.3406\n",
            "Epoch 43/100\n",
            "196/196 [==============================] - 32s 165ms/step - loss: 0.1634 - accuracy: 0.9493 - val_loss: 5.4951 - val_accuracy: 0.3263\n",
            "Epoch 44/100\n",
            "196/196 [==============================] - 32s 165ms/step - loss: 0.1228 - accuracy: 0.9621 - val_loss: 5.2389 - val_accuracy: 0.3497\n",
            "Epoch 45/100\n",
            "196/196 [==============================] - 32s 165ms/step - loss: 0.1014 - accuracy: 0.9694 - val_loss: 5.3052 - val_accuracy: 0.3420\n",
            "Epoch 46/100\n",
            "196/196 [==============================] - 33s 166ms/step - loss: 0.0801 - accuracy: 0.9772 - val_loss: 5.0085 - val_accuracy: 0.3668\n",
            "Epoch 47/100\n",
            "196/196 [==============================] - 33s 167ms/step - loss: 0.0687 - accuracy: 0.9805 - val_loss: 6.2623 - val_accuracy: 0.3582\n",
            "Epoch 48/100\n",
            "196/196 [==============================] - 33s 171ms/step - loss: 0.0734 - accuracy: 0.9792 - val_loss: 5.9699 - val_accuracy: 0.3469\n",
            "Epoch 49/100\n",
            "196/196 [==============================] - 32s 165ms/step - loss: 0.0829 - accuracy: 0.9760 - val_loss: 5.4705 - val_accuracy: 0.3469\n",
            "Epoch 50/100\n",
            "196/196 [==============================] - 32s 165ms/step - loss: 0.1086 - accuracy: 0.9666 - val_loss: 5.9742 - val_accuracy: 0.3186\n",
            "Epoch 51/100\n",
            "196/196 [==============================] - 33s 170ms/step - loss: 0.1237 - accuracy: 0.9611 - val_loss: 5.3517 - val_accuracy: 0.3456\n",
            "Epoch 52/100\n",
            "196/196 [==============================] - 32s 165ms/step - loss: 0.0886 - accuracy: 0.9733 - val_loss: 5.4122 - val_accuracy: 0.3569\n",
            "Epoch 53/100\n",
            "196/196 [==============================] - 32s 165ms/step - loss: 0.0750 - accuracy: 0.9770 - val_loss: 5.6956 - val_accuracy: 0.3337\n",
            "Epoch 54/100\n",
            "196/196 [==============================] - 32s 166ms/step - loss: 0.0641 - accuracy: 0.9814 - val_loss: 5.3510 - val_accuracy: 0.3658\n",
            "Epoch 55/100\n",
            "196/196 [==============================] - 32s 165ms/step - loss: 0.0494 - accuracy: 0.9853 - val_loss: 5.5220 - val_accuracy: 0.3533\n",
            "Epoch 56/100\n",
            "196/196 [==============================] - 32s 165ms/step - loss: 0.0503 - accuracy: 0.9845 - val_loss: 5.5129 - val_accuracy: 0.3489\n",
            "Epoch 57/100\n",
            "196/196 [==============================] - 32s 165ms/step - loss: 0.0702 - accuracy: 0.9785 - val_loss: 5.5770 - val_accuracy: 0.3477\n",
            "Epoch 58/100\n",
            "196/196 [==============================] - 32s 165ms/step - loss: 0.0763 - accuracy: 0.9760 - val_loss: 5.7633 - val_accuracy: 0.3443\n",
            "Epoch 59/100\n",
            "196/196 [==============================] - 32s 165ms/step - loss: 0.0937 - accuracy: 0.9705 - val_loss: 5.4996 - val_accuracy: 0.3570\n",
            "Epoch 60/100\n",
            "196/196 [==============================] - 32s 165ms/step - loss: 0.0840 - accuracy: 0.9737 - val_loss: 5.7170 - val_accuracy: 0.3503\n",
            "Epoch 61/100\n",
            "196/196 [==============================] - 32s 166ms/step - loss: 0.0617 - accuracy: 0.9809 - val_loss: 5.5540 - val_accuracy: 0.3536\n",
            "Epoch 62/100\n",
            "196/196 [==============================] - 33s 170ms/step - loss: 0.0485 - accuracy: 0.9857 - val_loss: 5.4269 - val_accuracy: 0.3619\n",
            "Epoch 63/100\n",
            "196/196 [==============================] - 32s 166ms/step - loss: 0.0501 - accuracy: 0.9843 - val_loss: 5.5730 - val_accuracy: 0.3483\n",
            "Epoch 64/100\n",
            "196/196 [==============================] - 32s 165ms/step - loss: 0.0681 - accuracy: 0.9789 - val_loss: 5.7446 - val_accuracy: 0.3370\n",
            "Epoch 65/100\n",
            "196/196 [==============================] - 32s 165ms/step - loss: 0.0706 - accuracy: 0.9773 - val_loss: 5.5666 - val_accuracy: 0.3565\n",
            "Epoch 66/100\n",
            "196/196 [==============================] - 32s 166ms/step - loss: 0.0690 - accuracy: 0.9784 - val_loss: 5.6726 - val_accuracy: 0.3455\n",
            "Epoch 67/100\n",
            "196/196 [==============================] - 32s 166ms/step - loss: 0.0689 - accuracy: 0.9779 - val_loss: 5.5417 - val_accuracy: 0.3542\n",
            "Epoch 68/100\n",
            "196/196 [==============================] - 32s 166ms/step - loss: 0.0530 - accuracy: 0.9839 - val_loss: 5.8758 - val_accuracy: 0.3508\n",
            "Epoch 69/100\n",
            "196/196 [==============================] - 32s 166ms/step - loss: 0.0431 - accuracy: 0.9870 - val_loss: 5.5523 - val_accuracy: 0.3637\n",
            "Epoch 70/100\n",
            "196/196 [==============================] - 32s 165ms/step - loss: 0.0493 - accuracy: 0.9846 - val_loss: 5.8991 - val_accuracy: 0.3532\n",
            "Epoch 71/100\n",
            "196/196 [==============================] - 32s 165ms/step - loss: 0.0768 - accuracy: 0.9756 - val_loss: 5.6797 - val_accuracy: 0.3510\n",
            "Epoch 72/100\n",
            "196/196 [==============================] - 32s 165ms/step - loss: 0.0690 - accuracy: 0.9783 - val_loss: 5.6317 - val_accuracy: 0.3522\n",
            "Epoch 73/100\n",
            "196/196 [==============================] - 32s 165ms/step - loss: 0.0625 - accuracy: 0.9804 - val_loss: 5.6310 - val_accuracy: 0.3511\n",
            "Epoch 74/100\n",
            "196/196 [==============================] - 32s 165ms/step - loss: 0.0352 - accuracy: 0.9890 - val_loss: 5.6384 - val_accuracy: 0.3561\n",
            "Epoch 75/100\n",
            "196/196 [==============================] - 32s 166ms/step - loss: 0.0263 - accuracy: 0.9924 - val_loss: 5.5241 - val_accuracy: 0.3757\n",
            "Epoch 76/100\n",
            "196/196 [==============================] - 32s 165ms/step - loss: 0.0339 - accuracy: 0.9899 - val_loss: 5.7378 - val_accuracy: 0.3620\n",
            "Epoch 77/100\n",
            "196/196 [==============================] - 32s 165ms/step - loss: 0.0552 - accuracy: 0.9815 - val_loss: 6.1231 - val_accuracy: 0.3371\n",
            "Epoch 78/100\n",
            "196/196 [==============================] - 32s 165ms/step - loss: 0.0752 - accuracy: 0.9753 - val_loss: 6.1099 - val_accuracy: 0.3249\n",
            "Epoch 79/100\n",
            "196/196 [==============================] - 32s 165ms/step - loss: 0.0696 - accuracy: 0.9761 - val_loss: 5.7515 - val_accuracy: 0.3509\n",
            "Epoch 80/100\n",
            "196/196 [==============================] - 32s 164ms/step - loss: 0.0600 - accuracy: 0.9798 - val_loss: 6.5483 - val_accuracy: 0.3013\n",
            "Epoch 81/100\n",
            "196/196 [==============================] - 32s 165ms/step - loss: 0.0416 - accuracy: 0.9869 - val_loss: 5.6258 - val_accuracy: 0.3592\n",
            "Epoch 82/100\n",
            "196/196 [==============================] - 32s 165ms/step - loss: 0.0276 - accuracy: 0.9918 - val_loss: 5.8783 - val_accuracy: 0.3545\n",
            "Epoch 83/100\n",
            "196/196 [==============================] - 32s 165ms/step - loss: 0.0202 - accuracy: 0.9937 - val_loss: 5.7211 - val_accuracy: 0.3676\n",
            "Epoch 84/100\n",
            "196/196 [==============================] - 32s 165ms/step - loss: 0.0198 - accuracy: 0.9939 - val_loss: 5.7733 - val_accuracy: 0.3773\n",
            "Epoch 85/100\n",
            "196/196 [==============================] - 32s 165ms/step - loss: 0.0396 - accuracy: 0.9875 - val_loss: 5.9853 - val_accuracy: 0.3506\n",
            "Epoch 86/100\n",
            "196/196 [==============================] - 32s 165ms/step - loss: 0.0859 - accuracy: 0.9717 - val_loss: 5.9406 - val_accuracy: 0.3363\n",
            "Epoch 87/100\n",
            "196/196 [==============================] - 32s 166ms/step - loss: 0.0850 - accuracy: 0.9731 - val_loss: 5.7445 - val_accuracy: 0.3489\n",
            "Epoch 88/100\n",
            "196/196 [==============================] - 32s 166ms/step - loss: 0.0513 - accuracy: 0.9832 - val_loss: 5.7238 - val_accuracy: 0.3571\n",
            "Epoch 89/100\n",
            "196/196 [==============================] - 32s 165ms/step - loss: 0.0270 - accuracy: 0.9915 - val_loss: 5.5447 - val_accuracy: 0.3697\n",
            "Epoch 90/100\n",
            "196/196 [==============================] - 32s 165ms/step - loss: 0.0155 - accuracy: 0.9951 - val_loss: 5.7548 - val_accuracy: 0.3701\n",
            "Epoch 91/100\n",
            "196/196 [==============================] - 32s 165ms/step - loss: 0.0133 - accuracy: 0.9963 - val_loss: 5.9198 - val_accuracy: 0.3544\n",
            "Epoch 92/100\n",
            "196/196 [==============================] - 32s 165ms/step - loss: 0.0261 - accuracy: 0.9918 - val_loss: 5.9289 - val_accuracy: 0.3662\n",
            "Epoch 93/100\n",
            "196/196 [==============================] - 32s 165ms/step - loss: 0.0489 - accuracy: 0.9844 - val_loss: 6.0424 - val_accuracy: 0.3521\n",
            "Epoch 94/100\n",
            "196/196 [==============================] - 32s 165ms/step - loss: 0.0846 - accuracy: 0.9725 - val_loss: 5.8734 - val_accuracy: 0.3569\n",
            "Epoch 95/100\n",
            "196/196 [==============================] - 32s 165ms/step - loss: 0.0811 - accuracy: 0.9729 - val_loss: 5.6955 - val_accuracy: 0.3602\n",
            "Epoch 96/100\n",
            "196/196 [==============================] - 32s 165ms/step - loss: 0.0287 - accuracy: 0.9909 - val_loss: 5.6416 - val_accuracy: 0.3703\n",
            "Epoch 97/100\n",
            "196/196 [==============================] - 32s 166ms/step - loss: 0.0140 - accuracy: 0.9961 - val_loss: 5.6621 - val_accuracy: 0.3728\n",
            "Epoch 98/100\n",
            "196/196 [==============================] - 32s 166ms/step - loss: 0.0147 - accuracy: 0.9958 - val_loss: 5.7946 - val_accuracy: 0.3705\n",
            "Epoch 99/100\n",
            "196/196 [==============================] - 32s 166ms/step - loss: 0.0113 - accuracy: 0.9967 - val_loss: 6.5952 - val_accuracy: 0.3318\n",
            "Epoch 100/100\n",
            "196/196 [==============================] - 32s 166ms/step - loss: 0.1268 - accuracy: 0.9602 - val_loss: 5.8566 - val_accuracy: 0.3541\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nByF_LBZLjlT"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')\n",
        "\n",
        "# Visualize history\n",
        "# Plot history: Loss\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Validation loss history')\n",
        "plt.ylabel('Loss value')\n",
        "plt.xlabel('No. epoch')\n",
        "plt.show()\n",
        "\n",
        "# Plot history: Accuracy\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Validation accuracy history')\n",
        "plt.ylabel('Accuracy value (%)')\n",
        "plt.xlabel('No. epoch')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOD9JcqI26C2"
      },
      "source": [
        "!pip freeze > requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}