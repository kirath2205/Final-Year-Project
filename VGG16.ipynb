{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VGG16.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOVudlafxthYXrDHQo3jEDD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kirath2205/Final-Year-Project/blob/main/VGG16.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "id": "-nXL13Ca3JXK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_random_eraser(p=0.5, s_l=0.02, s_h=0.4, r_1=0.3, r_2=1/0.3, v_l=0, v_h=255, pixel_level=False):\n",
        "    def eraser(input_img):\n",
        "        img_h, img_w, img_c = input_img.shape\n",
        "        p_1 = np.random.rand()\n",
        "\n",
        "        if p_1 > p:\n",
        "            return input_img\n",
        "\n",
        "        while True:\n",
        "            s = np.random.uniform(s_l, s_h) * img_h * img_w\n",
        "            r = np.random.uniform(r_1, r_2)\n",
        "            w = int(np.sqrt(s / r))\n",
        "            h = int(np.sqrt(s * r))\n",
        "            left = np.random.randint(0, img_w)\n",
        "            top = np.random.randint(0, img_h)\n",
        "\n",
        "            if left + w <= img_w and top + h <= img_h:\n",
        "                break\n",
        "\n",
        "        if pixel_level:\n",
        "            c = np.random.uniform(v_l, v_h, (h, w, img_c))\n",
        "        else:\n",
        "            c = np.random.uniform(v_l, v_h)\n",
        "\n",
        "        input_img[top:top + h, left:left + w, :] = c\n",
        "\n",
        "        return input_img\n",
        "\n",
        "    return eraser"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Keras-applications"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dba_nGLt3ZUr",
        "outputId": "6056e02f-d59b-4cbb-cc69-8b24557108b2"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: Keras-applications in /usr/local/lib/python3.6/dist-packages (1.0.8)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from Keras-applications) (1.19.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras-applications) (2.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->Keras-applications) (1.15.0)\n",
            "\u001b[33mWARNING: You are using pip version 20.2.2; however, version 21.3.1 is available.\n",
            "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from keras.layers import Add, Activation, ZeroPadding2D, Conv2D, AveragePooling2D, MaxPooling2D \n",
        "from tensorflow.keras.layers import Input, UpSampling2D, Flatten, BatchNormalization, Dense, Dropout, GlobalAveragePooling2D , UpSampling3D\n",
        "from tensorflow.keras import optimizers\n",
        "from keras.datasets import cifar100,cifar10,fashion_mnist\n",
        "import tensorflow as tf\n",
        "from keras.utils import np_utils\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from skimage.transform import resize\n",
        "from keras import applications\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50 , preprocess_input , decode_predictions\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras import regularizers"
      ],
      "metadata": {
        "id": "4tScFJL93zeo"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def select_dataset(index=1): #1 for cifar10 , 2 for cifar100 , 3 for fashion mnist\n",
        "  if(index == 1):\n",
        "    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "    channel = 3\n",
        "    num_classes = 10\n",
        "  if(index == 2):\n",
        "    (x_train, y_train), (x_test, y_test) = cifar100.load_data()\n",
        "    num_classes = 100\n",
        "    channel = 3\n",
        "  if(index == 3):\n",
        "    (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "    x_train = x_train.reshape((60000, 28, 28, 1))\n",
        "    x_test =  x_test.reshape((10000, 28, 28, 1))\n",
        "    num_classes = 10\n",
        "    y_train = np_utils.to_categorical(y_train, num_classes)\n",
        "    y_test = np_utils.to_categorical(y_test, num_classes)\n",
        "    channel = 1\n",
        "    return (x_train , y_train , x_test , y_test , num_classes , channel)\n",
        "\n",
        "  #Pre-process the data\n",
        "  x_train = preprocess_input(x_train)\n",
        "  x_test = preprocess_input(x_test)\n",
        "\n",
        "  datagen = ImageDataGenerator(preprocessing_function=get_random_eraser(v_l=0, v_h=1, pixel_level=True))\n",
        "  datagen.fit(x_train)\n",
        "  y_train = np_utils.to_categorical(y_train, num_classes)\n",
        "  y_test = np_utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "  return (x_train , y_train , x_test , y_test , num_classes , channel , datagen)"
      ],
      "metadata": {
        "id": "HiJ-GXMq3bHP"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def VGG16(num_classes , channel=3):\n",
        "  model = Sequential()\n",
        "  weight_decay = 0.0005\n",
        "  learning_rate = 0.1\n",
        "  lr_decay = 1e-6\n",
        "  lr_drop = 20\n",
        "  if(channel == 3):\n",
        "    image_shape = (160, 160, 3)\n",
        "    model.add(UpSampling2D((5,5)))\n",
        "  else:\n",
        "    image_shape = (112, 112, 3)\n",
        "    model.add(UpSampling3D((4,4,3)))\n",
        "\n",
        "  model.add(Conv2D(64, (3, 3), padding='same',input_shape=image_shape,kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Activation('relu'))\n",
        "\n",
        "  model.add(Conv2D(64, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "\n",
        "  model.add(Conv2D(128, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Activation('relu'))\n",
        "\n",
        "  model.add(Conv2D(128, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Activation('relu'))\n",
        "\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "\n",
        "  model.add(Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Activation('relu'))\n",
        "\n",
        "  model.add(Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Activation('relu'))\n",
        "\n",
        "  model.add(Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Activation('relu'))\n",
        "\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "\n",
        "  model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Activation('relu'))\n",
        "\n",
        "  model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Activation('relu'))\n",
        "\n",
        "  model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Activation('relu'))\n",
        "\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "\n",
        "  model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Activation('relu'))\n",
        "\n",
        "  model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Activation('relu'))\n",
        "\n",
        "  model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Activation('relu'))\n",
        "\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(512,kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Activation('relu'))\n",
        "\n",
        "  model.add(Dropout(0.25))\n",
        "  model.add(Dense(num_classes))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Activation('softmax'))\n",
        "  sgd = keras.optimizers.SGD(lr=learning_rate, decay=lr_decay, momentum=0.9, nesterov=True)\n",
        "  model.compile(optimizer=sgd, loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])\n",
        "  \n",
        "  return model\n"
      ],
      "metadata": {
        "id": "cqrCI1rQ3dFg"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = 1\n",
        "x_train , y_train , x_test , y_test , num_classes ,channel , datagen = select_dataset(index)\n",
        "model = VGG16(num_classes,channel)\n",
        "model_name = 'desktop/Trained_models/vgg16_cifar10'\n",
        "model_path = 'desktop/Trained_models/vgg16_cifar10.h5'"
      ],
      "metadata": {
        "id": "tZYnvxgn5iWV"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = 2\n",
        "x_train , y_train , x_test , y_test , num_classes ,channel , datagen = select_dataset(index)\n",
        "print(num_classes)\n",
        "model = VGG16(num_classes,channel)\n",
        "model_name = 'desktop/Trained_models/vgg16_cifar100'\n",
        "model_path = 'desktop/Trained_models/vgg16_cifar100.h5'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-Jhlhqi5yw-",
        "outputId": "cefb9b53-70d0-4537-c782-a73a58097512"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index = 3\n",
        "x_train , y_train , x_test , y_test , num_classes ,channel  = select_dataset(index)\n",
        "model = VGG16(num_classes,channel)\n",
        "model_name = 'desktop/Trained_models/vgg16_mnist'\n",
        "model_path = 'desktop/Trained_models/vgg16_mnist.h5'"
      ],
      "metadata": {
        "id": "h_lCf9Re541f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32 # 256 for mnist , 128 for cifar-10 , 64 for cifar-100\n",
        "epochs=300\n",
        "callbacks = [ \n",
        "    tf.keras.callbacks.ModelCheckpoint(model_name, monitor='val_accuracy', verbose=1, save_best_only=True),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau( factor = 0.9, patience = 7, min_lr = 0.000001 ),\n",
        "    #tf.keras.callbacks.EarlyStopping(monitor = 'val_accuracy' , patience = 15)\n",
        "  ]\n",
        "if(channel == 3):\n",
        "  history = model.fit_generator(datagen.flow(x_train, y_train,\n",
        "                                  batch_size=batch_size),\n",
        "                                  steps_per_epoch=x_train.shape[0] // batch_size,\n",
        "                                  epochs=epochs,\n",
        "                                  validation_data=(x_test, y_test),\n",
        "                                  callbacks = callbacks)\n",
        "\n",
        "  model.save(model_path)\n",
        "else:\n",
        "  history = model.fit(x_train , y_train , batch_size=batch_size ,steps_per_epoch=x_train.shape[0] // batch_size, epochs=epochs, validation_data=(x_test, y_test),callbacks=callbacks)\n",
        "  model.save(model_path)"
      ],
      "metadata": {
        "id": "UcvNadj35_uW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Visualize history\n",
        "# Plot history: Loss\n",
        "plt.figure(figsize = (10,8))\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.plot(history.history['loss'],label='Train loss')\n",
        "plt.title('Validation loss history vgg16')\n",
        "plt.ylabel('Loss value')\n",
        "plt.xlabel('No. epoch')\n",
        "plt.show()\n",
        "\n",
        "# Plot history: Accuracy\n",
        "plt.figure(figsize = (10,8))\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.plot(history.history['accuracy'],label='Train accuracy')\n",
        "plt.title('Validation accuracy history vgg16')\n",
        "plt.ylabel('Accuracy value (%)')\n",
        "plt.xlabel('No. epoch')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Dg8MSQzh6Tbl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras,os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential,Model\n",
        "from keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D , Dropout\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from keras.initializers import glorot_uniform\n",
        "from keras import applications\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50 , preprocess_input\n",
        "from keras import regularizers\n",
        "from absl import app, flags\n",
        "\n",
        "# Install bleeding edge version of cleverhans\n",
        "!pip install git+https://github.com/tensorflow/cleverhans.git#egg=cleverhans\n",
        "from cleverhans.tf2.attacks.projected_gradient_descent import projected_gradient_descent\n",
        "from cleverhans.tf2.attacks.fast_gradient_method import fast_gradient_method"
      ],
      "metadata": {
        "id": "JcQE-CkzRIUw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77fc51be-ddb4-4946-e51f-ea27e488b361"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: cleverhans from git+https://github.com/tensorflow/cleverhans.git#egg=cleverhans in ./.local/lib/python3.6/site-packages (4.0.0)\n",
            "Requirement already satisfied: pycodestyle in ./.local/lib/python3.6/site-packages (from cleverhans) (2.8.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cleverhans) (1.15.0)\n",
            "Requirement already satisfied: joblib in ./.local/lib/python3.6/site-packages (from cleverhans) (1.1.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from cleverhans) (0.10.0)\n",
            "Requirement already satisfied: tensorflow-probability in ./.local/lib/python3.6/site-packages (from cleverhans) (0.14.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from cleverhans) (3.3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from cleverhans) (1.19.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from cleverhans) (1.4.1)\n",
            "Requirement already satisfied: mnist in ./.local/lib/python3.6/site-packages (from cleverhans) (0.2.2)\n",
            "Requirement already satisfied: easydict in ./.local/lib/python3.6/site-packages (from cleverhans) (1.9)\n",
            "Requirement already satisfied: nose in ./.local/lib/python3.6/site-packages (from cleverhans) (1.3.7)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in ./.local/lib/python3.6/site-packages (from tensorflow-probability->cleverhans) (2.0.0)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability->cleverhans) (0.3.3)\n",
            "Requirement already satisfied: decorator in ./.local/lib/python3.6/site-packages (from tensorflow-probability->cleverhans) (4.4.2)\n",
            "Requirement already satisfied: dm-tree in ./.local/lib/python3.6/site-packages (from tensorflow-probability->cleverhans) (0.1.6)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->cleverhans) (1.2.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.6/dist-packages (from matplotlib->cleverhans) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->cleverhans) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->cleverhans) (2.8.1)\n",
            "Requirement already satisfied: certifi>=2020.06.20 in /usr/local/lib/python3.6/dist-packages (from matplotlib->cleverhans) (2020.6.20)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.6/dist-packages (from matplotlib->cleverhans) (7.2.0)\n",
            "\u001b[33mWARNING: You are using pip version 20.2.2; however, version 21.3.1 is available.\n",
            "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def make_prediction(model , image , true_value):\n",
        "  true_label_index = -1\n",
        "  for i in range(len(true_value)):\n",
        "\n",
        "    if(true_value[i]==1):\n",
        "      true_label_index = i\n",
        "      break\n",
        "\n",
        "  prediction = model.predict(image)[0]\n",
        "  probability = float('-inf')\n",
        "  predicted_label_index = -1\n",
        "\n",
        "  for i in range(len(prediction)):\n",
        "\n",
        "    if(prediction[i]>probability):\n",
        "      probability = prediction[i]\n",
        "      predicted_label_index = i\n",
        "\n",
        "  if(true_label_index!=predicted_label_index):\n",
        "    return 1\n",
        "  return 0"
      ],
      "metadata": {
        "id": "SjTOAz-uQ1rg"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.load_model(model_path)"
      ],
      "metadata": {
        "id": "ovkUDKagU3Xp"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cifar-100\n"
      ],
      "metadata": {
        "id": "mkuzknVeqYet"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Yjwl08XIqYD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import img_to_array\n",
        "\n",
        "epsilon_end = 0.8\n",
        "#logits_model = tf.keras.Model(model.input,model.layers[-1].output)\n",
        "total_images = 200\n",
        "\n",
        "result={}\n",
        "epsilon = 0.1\n",
        "while(epsilon <= epsilon_end):\n",
        "\n",
        "  fgsm_counter = 0\n",
        "  pgd_counter = 0\n",
        "  print(\"Epsilon value - \",epsilon)\n",
        "  print()\n",
        "  for image_index in range(total_images):\n",
        "  \n",
        "  \n",
        "    image = x_test[image_index]\n",
        "    image = img_to_array(image)\n",
        "    image = image.reshape(1, 32, 32, 3)\n",
        "    true_value = y_test[image_index]\n",
        "    original_prediction = make_prediction(model,image,true_value)\n",
        "    fgsm_sample = fast_gradient_method(model, image, epsilon, np.inf, targeted=False)\n",
        "    pgd_sample = projected_gradient_descent(model, image, epsilon, 0.01, 40, np.inf)\n",
        "    print(epsilon,' ',image_index)\n",
        "    fgsm_prediction = make_prediction(model , fgsm_sample , true_value)\n",
        "    pgd_prediction = make_prediction(model , pgd_sample , true_value)\n",
        "    pgd_counter+=pgd_prediction\n",
        "    fgsm_counter+=fgsm_prediction\n",
        "    print(pgd_counter,' ',fgsm_counter)\n",
        "  result[epsilon] = (fgsm_counter , pgd_counter)\n",
        "  epsilon+=0.1\n"
      ],
      "metadata": {
        "id": "C1C-6c5hRQoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for key in result.keys():\n",
        "  print(key,' ',result[key])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNcSFot-RrLm",
        "outputId": "47bdda66-4f6c-4902-d983-1d1a2208cb14"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.1   (71, 71)\n",
            "0.2   (68, 66)\n",
            "0.30000000000000004   (70, 71)\n",
            "0.4   (72, 75)\n",
            "0.5   (80, 75)\n",
            "0.6   (89, 75)\n",
            "0.7   (98, 75)\n",
            "0.7999999999999999   (102, 75)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "cifar-10\n"
      ],
      "metadata": {
        "id": "1v4t73iyqf0T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.load_model(model_path)"
      ],
      "metadata": {
        "id": "M3F4f71wqnYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epsilon_end = 0.8\n",
        "#logits_model = tf.keras.Model(model.input,model.layers[-1].output)\n",
        "total_images = 200\n",
        "\n",
        "result_cifar10={}\n",
        "epsilon = 0.1\n",
        "while(epsilon <= epsilon_end):\n",
        "\n",
        "  fgsm_counter = 0\n",
        "  pgd_counter = 0\n",
        "  print(\"Epsilon value - \",epsilon)\n",
        "  print()\n",
        "  for image_index in range(total_images):\n",
        "  \n",
        "  \n",
        "    image = x_test[image_index]\n",
        "    image = img_to_array(image)\n",
        "    image = image.reshape(1, 32, 32, 3)\n",
        "    true_value = y_test[image_index]\n",
        "    original_prediction = make_prediction(model,image,true_value)\n",
        "    fgsm_sample = fast_gradient_method(model, image, epsilon, np.inf, targeted=False)\n",
        "    pgd_sample = projected_gradient_descent(model, image, epsilon, 0.01, 40, np.inf)\n",
        "    print(epsilon,' ',image_index)\n",
        "    fgsm_prediction = make_prediction(model , fgsm_sample , true_value)\n",
        "    pgd_prediction = make_prediction(model , pgd_sample , true_value)\n",
        "    pgd_counter+=pgd_prediction\n",
        "    fgsm_counter+=fgsm_prediction\n",
        "    print(pgd_counter,' ',fgsm_counter)\n",
        "  result_cifar10[epsilon] = (fgsm_counter , pgd_counter)\n",
        "  epsilon+=0.1"
      ],
      "metadata": {
        "id": "Ki0fW5YfdaSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for key in result_cifar10.keys():\n",
        "  print(key,' ',result_cifar10[key])"
      ],
      "metadata": {
        "id": "1xZClhbNq1Fi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}