{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VGG16.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPu+PTwtpMKOYoRiSnQesCJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kirath2205/Final-Year-Project/blob/main/VGG16.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-nXL13Ca3JXK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_random_eraser(p=0.5, s_l=0.02, s_h=0.4, r_1=0.3, r_2=1/0.3, v_l=0, v_h=255, pixel_level=False):\n",
        "    def eraser(input_img):\n",
        "        img_h, img_w, img_c = input_img.shape\n",
        "        p_1 = np.random.rand()\n",
        "\n",
        "        if p_1 > p:\n",
        "            return input_img\n",
        "\n",
        "        while True:\n",
        "            s = np.random.uniform(s_l, s_h) * img_h * img_w\n",
        "            r = np.random.uniform(r_1, r_2)\n",
        "            w = int(np.sqrt(s / r))\n",
        "            h = int(np.sqrt(s * r))\n",
        "            left = np.random.randint(0, img_w)\n",
        "            top = np.random.randint(0, img_h)\n",
        "\n",
        "            if left + w <= img_w and top + h <= img_h:\n",
        "                break\n",
        "\n",
        "        if pixel_level:\n",
        "            c = np.random.uniform(v_l, v_h, (h, w, img_c))\n",
        "        else:\n",
        "            c = np.random.uniform(v_l, v_h)\n",
        "\n",
        "        input_img[top:top + h, left:left + w, :] = c\n",
        "\n",
        "        return input_img\n",
        "\n",
        "    return eraser"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Keras-applications"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dba_nGLt3ZUr",
        "outputId": "116f5a0c-6402-47ec-9bac-8c7c94bded33"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: Keras-applications in /usr/local/lib/python3.6/dist-packages (1.0.8)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from Keras-applications) (1.19.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras-applications) (2.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->Keras-applications) (1.15.0)\n",
            "\u001b[33mWARNING: You are using pip version 20.2.2; however, version 21.3.1 is available.\n",
            "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from keras.layers import Add, Activation, ZeroPadding2D, Conv2D, AveragePooling2D, MaxPooling2D \n",
        "from tensorflow.keras.layers import Input, UpSampling2D, Flatten, BatchNormalization, Dense, Dropout, GlobalAveragePooling2D , UpSampling3D\n",
        "from tensorflow.keras import optimizers\n",
        "from keras.datasets import cifar100,cifar10,fashion_mnist\n",
        "import tensorflow as tf\n",
        "from keras.utils import np_utils\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from skimage.transform import resize\n",
        "from keras import applications\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50 , preprocess_input , decode_predictions\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras import regularizers"
      ],
      "metadata": {
        "id": "4tScFJL93zeo"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def select_dataset(index=1): #1 for cifar10 , 2 for cifar100 , 3 for fashion mnist\n",
        "  if(index == 1):\n",
        "    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "    channel = 3\n",
        "    num_classes = 10\n",
        "  if(index == 2):\n",
        "    (x_train, y_train), (x_test, y_test) = cifar100.load_data()\n",
        "    num_classes = 100\n",
        "    channel = 3\n",
        "  if(index == 3):\n",
        "    (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "    x_train = x_train.reshape((60000, 28, 28, 1))\n",
        "    x_test =  x_test.reshape((10000, 28, 28, 1))\n",
        "    num_classes = 10\n",
        "    y_train = np_utils.to_categorical(y_train, num_classes)\n",
        "    y_test = np_utils.to_categorical(y_test, num_classes)\n",
        "    channel = 1\n",
        "    return (x_train , y_train , x_test , y_test , num_classes , channel)\n",
        "\n",
        "  #Pre-process the data\n",
        "  x_train = preprocess_input(x_train)\n",
        "  x_test = preprocess_input(x_test)\n",
        "\n",
        "  datagen = ImageDataGenerator(preprocessing_function=get_random_eraser(v_l=0, v_h=1, pixel_level=True))\n",
        "  datagen.fit(x_train)\n",
        "  y_train = np_utils.to_categorical(y_train, num_classes)\n",
        "  y_test = np_utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "  return (x_train , y_train , x_test , y_test , num_classes , channel , datagen)"
      ],
      "metadata": {
        "id": "HiJ-GXMq3bHP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def VGG16(num_classes , channel=3):\n",
        "  model = Sequential()\n",
        "  weight_decay = 0.0005\n",
        "  learning_rate = 0.1\n",
        "  lr_decay = 1e-6\n",
        "  lr_drop = 20\n",
        "  if(channel == 3):\n",
        "    image_shape = (160, 160, 3)\n",
        "    model.add(UpSampling2D((5,5)))\n",
        "  else:\n",
        "    image_shape = (112, 112, 3)\n",
        "    model.add(UpSampling3D((4,4,3)))\n",
        "\n",
        "  model.add(Conv2D(64, (3, 3), padding='same',input_shape=image_shape,kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Activation('relu'))\n",
        "\n",
        "  model.add(Conv2D(64, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "\n",
        "  model.add(Conv2D(128, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Activation('relu'))\n",
        "\n",
        "  model.add(Conv2D(128, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Activation('relu'))\n",
        "\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "\n",
        "  model.add(Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Activation('relu'))\n",
        "\n",
        "  model.add(Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Activation('relu'))\n",
        "\n",
        "  model.add(Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Activation('relu'))\n",
        "\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "\n",
        "  model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Activation('relu'))\n",
        "\n",
        "  model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Activation('relu'))\n",
        "\n",
        "  model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Activation('relu'))\n",
        "\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "\n",
        "  model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Activation('relu'))\n",
        "\n",
        "  model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Activation('relu'))\n",
        "\n",
        "  model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Activation('relu'))\n",
        "\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(512,kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Activation('relu'))\n",
        "\n",
        "  model.add(Dropout(0.25))\n",
        "  model.add(Dense(num_classes))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Activation('softmax'))\n",
        "  sgd = keras.optimizers.SGD(lr=learning_rate, decay=lr_decay, momentum=0.9, nesterov=True)\n",
        "  model.compile(optimizer=sgd, loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])\n",
        "  \n",
        "  return model\n"
      ],
      "metadata": {
        "id": "cqrCI1rQ3dFg"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = 1\n",
        "x_train , y_train , x_test , y_test , num_classes ,channel , datagen = select_dataset(index)\n",
        "model = VGG16(num_classes,channel)\n",
        "model_name = 'desktop/Trained_models/vgg16_cifar10'\n",
        "model_path = 'desktop/Trained_models/vgg16_cifar10.h5'"
      ],
      "metadata": {
        "id": "tZYnvxgn5iWV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = 2\n",
        "x_train , y_train , x_test , y_test , num_classes ,channel , datagen = select_dataset(index)\n",
        "print(num_classes)\n",
        "model = VGG16(num_classes,channel)\n",
        "model_name = 'desktop/Trained_models/vgg16_cifar100'\n",
        "model_path = 'desktop/Trained_models/vgg16_cifar100.h5'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-Jhlhqi5yw-",
        "outputId": "cefb9b53-70d0-4537-c782-a73a58097512"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index = 3\n",
        "x_train , y_train , x_test , y_test , num_classes ,channel  = select_dataset(index)\n",
        "model = VGG16(num_classes,channel)\n",
        "model_name = 'desktop/Trained_models/vgg16_mnist'\n",
        "model_path = 'desktop/Trained_models/vgg16_mnist.h5'"
      ],
      "metadata": {
        "id": "h_lCf9Re541f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128 # 256 for mnist , 128 for cifar-10 , 32 for cifar-100\n",
        "epochs=200\n",
        "callbacks = [ \n",
        "    tf.keras.callbacks.ModelCheckpoint(model_name, monitor='val_accuracy', verbose=1, save_best_only=True),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau( factor = 0.9, patience = 7, min_lr = 0.000001 ),\n",
        "    #tf.keras.callbacks.EarlyStopping(monitor = 'val_accuracy' , patience = 15)\n",
        "  ]\n",
        "if(channel == 3):\n",
        "  history = model.fit_generator(datagen.flow(x_train, y_train,\n",
        "                                  batch_size=batch_size),\n",
        "                                  steps_per_epoch=x_train.shape[0] // batch_size,\n",
        "                                  epochs=epochs,\n",
        "                                  validation_data=(x_test, y_test),\n",
        "                                  callbacks = callbacks)\n",
        "\n",
        "  model.save(model_path)\n",
        "else:\n",
        "  history = model.fit(x_train , y_train , batch_size=batch_size ,steps_per_epoch=x_train.shape[0] // batch_size, epochs=epochs, validation_data=(x_test, y_test),callbacks=callbacks)\n",
        "  model.save(model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UcvNadj35_uW",
        "outputId": "8dad58c2-f406-4172-d1dd-bedce2664762"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From <ipython-input-7-6de768d38cc2>:14: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use Model.fit, which supports generators.\n",
            "Epoch 1/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 4.2326 - accuracy: 0.3574\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.28930, saving model to desktop/Trained_models/vgg16_cifar10\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "INFO:tensorflow:Assets written to: desktop/Trained_models/vgg16_cifar10/assets\n",
            "390/390 [==============================] - 177s 453ms/step - loss: 4.2326 - accuracy: 0.3574 - val_loss: 4.0103 - val_accuracy: 0.2893 - lr: 0.1000\n",
            "Epoch 2/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 2.4826 - accuracy: 0.5661\n",
            "Epoch 00002: val_accuracy did not improve from 0.28930\n",
            "390/390 [==============================] - 172s 442ms/step - loss: 2.4826 - accuracy: 0.5661 - val_loss: 4.5547 - val_accuracy: 0.2218 - lr: 0.1000\n",
            "Epoch 3/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.6730 - accuracy: 0.6768\n",
            "Epoch 00003: val_accuracy improved from 0.28930 to 0.59510, saving model to desktop/Trained_models/vgg16_cifar10\n",
            "INFO:tensorflow:Assets written to: desktop/Trained_models/vgg16_cifar10/assets\n",
            "390/390 [==============================] - 177s 454ms/step - loss: 1.6730 - accuracy: 0.6768 - val_loss: 1.8359 - val_accuracy: 0.5951 - lr: 0.1000\n",
            "Epoch 4/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.3250 - accuracy: 0.7313\n",
            "Epoch 00004: val_accuracy improved from 0.59510 to 0.68040, saving model to desktop/Trained_models/vgg16_cifar10\n",
            "INFO:tensorflow:Assets written to: desktop/Trained_models/vgg16_cifar10/assets\n",
            "390/390 [==============================] - 176s 451ms/step - loss: 1.3250 - accuracy: 0.7313 - val_loss: 1.4648 - val_accuracy: 0.6804 - lr: 0.1000\n",
            "Epoch 5/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.1774 - accuracy: 0.7554\n",
            "Epoch 00005: val_accuracy improved from 0.68040 to 0.71650, saving model to desktop/Trained_models/vgg16_cifar10\n",
            "INFO:tensorflow:Assets written to: desktop/Trained_models/vgg16_cifar10/assets\n",
            "390/390 [==============================] - 176s 451ms/step - loss: 1.1774 - accuracy: 0.7554 - val_loss: 1.3009 - val_accuracy: 0.7165 - lr: 0.1000\n",
            "Epoch 6/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.1099 - accuracy: 0.7710\n",
            "Epoch 00006: val_accuracy did not improve from 0.71650\n",
            "390/390 [==============================] - 172s 440ms/step - loss: 1.1099 - accuracy: 0.7710 - val_loss: 1.7327 - val_accuracy: 0.5807 - lr: 0.1000\n",
            "Epoch 7/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.0640 - accuracy: 0.7879\n",
            "Epoch 00007: val_accuracy did not improve from 0.71650\n",
            "390/390 [==============================] - 171s 440ms/step - loss: 1.0640 - accuracy: 0.7879 - val_loss: 1.4217 - val_accuracy: 0.6898 - lr: 0.1000\n",
            "Epoch 8/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.0551 - accuracy: 0.7945\n",
            "Epoch 00008: val_accuracy did not improve from 0.71650\n",
            "390/390 [==============================] - 171s 439ms/step - loss: 1.0551 - accuracy: 0.7945 - val_loss: 1.6137 - val_accuracy: 0.6283 - lr: 0.1000\n",
            "Epoch 9/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.0477 - accuracy: 0.8000\n",
            "Epoch 00009: val_accuracy improved from 0.71650 to 0.76590, saving model to desktop/Trained_models/vgg16_cifar10\n",
            "INFO:tensorflow:Assets written to: desktop/Trained_models/vgg16_cifar10/assets\n",
            "390/390 [==============================] - 175s 450ms/step - loss: 1.0477 - accuracy: 0.8000 - val_loss: 1.1744 - val_accuracy: 0.7659 - lr: 0.1000\n",
            "Epoch 10/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.0472 - accuracy: 0.8050\n",
            "Epoch 00010: val_accuracy did not improve from 0.76590\n",
            "390/390 [==============================] - 171s 439ms/step - loss: 1.0472 - accuracy: 0.8050 - val_loss: 1.3092 - val_accuracy: 0.7266 - lr: 0.1000\n",
            "Epoch 11/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.0406 - accuracy: 0.8149\n",
            "Epoch 00011: val_accuracy improved from 0.76590 to 0.77780, saving model to desktop/Trained_models/vgg16_cifar10\n",
            "INFO:tensorflow:Assets written to: desktop/Trained_models/vgg16_cifar10/assets\n",
            "390/390 [==============================] - 176s 451ms/step - loss: 1.0406 - accuracy: 0.8149 - val_loss: 1.1506 - val_accuracy: 0.7778 - lr: 0.1000\n",
            "Epoch 12/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.0498 - accuracy: 0.8148\n",
            "Epoch 00012: val_accuracy did not improve from 0.77780\n",
            "390/390 [==============================] - 172s 440ms/step - loss: 1.0498 - accuracy: 0.8148 - val_loss: 1.4360 - val_accuracy: 0.7026 - lr: 0.1000\n",
            "Epoch 13/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.0449 - accuracy: 0.8201\n",
            "Epoch 00013: val_accuracy did not improve from 0.77780\n",
            "390/390 [==============================] - 171s 440ms/step - loss: 1.0449 - accuracy: 0.8201 - val_loss: 1.5183 - val_accuracy: 0.6812 - lr: 0.1000\n",
            "Epoch 14/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.0453 - accuracy: 0.8258\n",
            "Epoch 00014: val_accuracy did not improve from 0.77780\n",
            "390/390 [==============================] - 171s 439ms/step - loss: 1.0453 - accuracy: 0.8258 - val_loss: 1.4707 - val_accuracy: 0.6910 - lr: 0.1000\n",
            "Epoch 15/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.0496 - accuracy: 0.8266\n",
            "Epoch 00015: val_accuracy did not improve from 0.77780\n",
            "390/390 [==============================] - 171s 439ms/step - loss: 1.0496 - accuracy: 0.8266 - val_loss: 1.8471 - val_accuracy: 0.6205 - lr: 0.1000\n",
            "Epoch 16/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.0578 - accuracy: 0.8288\n",
            "Epoch 00016: val_accuracy did not improve from 0.77780\n",
            "390/390 [==============================] - 172s 440ms/step - loss: 1.0578 - accuracy: 0.8288 - val_loss: 1.2767 - val_accuracy: 0.7498 - lr: 0.1000\n",
            "Epoch 17/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.0560 - accuracy: 0.8337\n",
            "Epoch 00017: val_accuracy did not improve from 0.77780\n",
            "390/390 [==============================] - 171s 440ms/step - loss: 1.0560 - accuracy: 0.8337 - val_loss: 1.3069 - val_accuracy: 0.7539 - lr: 0.1000\n",
            "Epoch 18/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.0616 - accuracy: 0.8363\n",
            "Epoch 00018: val_accuracy did not improve from 0.77780\n",
            "390/390 [==============================] - 171s 439ms/step - loss: 1.0616 - accuracy: 0.8363 - val_loss: 1.3848 - val_accuracy: 0.7320 - lr: 0.1000\n",
            "Epoch 19/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.0311 - accuracy: 0.8435\n",
            "Epoch 00019: val_accuracy did not improve from 0.77780\n",
            "390/390 [==============================] - 171s 439ms/step - loss: 1.0311 - accuracy: 0.8435 - val_loss: 1.2515 - val_accuracy: 0.7701 - lr: 0.0900\n",
            "Epoch 20/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.0272 - accuracy: 0.8442\n",
            "Epoch 00020: val_accuracy did not improve from 0.77780\n",
            "390/390 [==============================] - 171s 440ms/step - loss: 1.0272 - accuracy: 0.8442 - val_loss: 1.6548 - val_accuracy: 0.6660 - lr: 0.0900\n",
            "Epoch 21/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.0308 - accuracy: 0.8468\n",
            "Epoch 00021: val_accuracy improved from 0.77780 to 0.82170, saving model to desktop/Trained_models/vgg16_cifar10\n",
            "INFO:tensorflow:Assets written to: desktop/Trained_models/vgg16_cifar10/assets\n",
            "390/390 [==============================] - 176s 452ms/step - loss: 1.0308 - accuracy: 0.8468 - val_loss: 1.1197 - val_accuracy: 0.8217 - lr: 0.0900\n",
            "Epoch 22/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.0392 - accuracy: 0.8479\n",
            "Epoch 00022: val_accuracy did not improve from 0.82170\n",
            "390/390 [==============================] - 172s 440ms/step - loss: 1.0392 - accuracy: 0.8479 - val_loss: 1.8366 - val_accuracy: 0.6563 - lr: 0.0900\n",
            "Epoch 23/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.0380 - accuracy: 0.8514\n",
            "Epoch 00023: val_accuracy did not improve from 0.82170\n",
            "390/390 [==============================] - 172s 440ms/step - loss: 1.0380 - accuracy: 0.8514 - val_loss: 1.2826 - val_accuracy: 0.7859 - lr: 0.0900\n",
            "Epoch 24/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.0466 - accuracy: 0.8518\n",
            "Epoch 00024: val_accuracy did not improve from 0.82170\n",
            "390/390 [==============================] - 172s 440ms/step - loss: 1.0466 - accuracy: 0.8518 - val_loss: 1.7634 - val_accuracy: 0.6815 - lr: 0.0900\n",
            "Epoch 25/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.0475 - accuracy: 0.8543\n",
            "Epoch 00025: val_accuracy did not improve from 0.82170\n",
            "390/390 [==============================] - 171s 440ms/step - loss: 1.0475 - accuracy: 0.8543 - val_loss: 1.3356 - val_accuracy: 0.7728 - lr: 0.0900\n",
            "Epoch 26/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.0611 - accuracy: 0.8552\n",
            "Epoch 00026: val_accuracy did not improve from 0.82170\n",
            "390/390 [==============================] - 172s 440ms/step - loss: 1.0611 - accuracy: 0.8552 - val_loss: 1.2743 - val_accuracy: 0.7925 - lr: 0.0900\n",
            "Epoch 27/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.0670 - accuracy: 0.8560\n",
            "Epoch 00027: val_accuracy did not improve from 0.82170\n",
            "390/390 [==============================] - 172s 440ms/step - loss: 1.0670 - accuracy: 0.8560 - val_loss: 1.9312 - val_accuracy: 0.6558 - lr: 0.0900\n",
            "Epoch 28/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.0703 - accuracy: 0.8562\n",
            "Epoch 00028: val_accuracy did not improve from 0.82170\n",
            "390/390 [==============================] - 171s 440ms/step - loss: 1.0703 - accuracy: 0.8562 - val_loss: 1.4125 - val_accuracy: 0.7503 - lr: 0.0900\n",
            "Epoch 29/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.0296 - accuracy: 0.8673\n",
            "Epoch 00029: val_accuracy did not improve from 0.82170\n",
            "390/390 [==============================] - 171s 439ms/step - loss: 1.0296 - accuracy: 0.8673 - val_loss: 1.6487 - val_accuracy: 0.6776 - lr: 0.0810\n",
            "Epoch 30/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.0297 - accuracy: 0.8674\n",
            "Epoch 00030: val_accuracy did not improve from 0.82170\n",
            "390/390 [==============================] - 171s 440ms/step - loss: 1.0297 - accuracy: 0.8674 - val_loss: 1.3191 - val_accuracy: 0.7935 - lr: 0.0810\n",
            "Epoch 31/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.0383 - accuracy: 0.8675\n",
            "Epoch 00031: val_accuracy did not improve from 0.82170\n",
            "390/390 [==============================] - 171s 439ms/step - loss: 1.0383 - accuracy: 0.8675 - val_loss: 1.8918 - val_accuracy: 0.6449 - lr: 0.0810\n",
            "Epoch 32/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.0467 - accuracy: 0.8696\n",
            "Epoch 00032: val_accuracy did not improve from 0.82170\n",
            "390/390 [==============================] - 171s 440ms/step - loss: 1.0467 - accuracy: 0.8696 - val_loss: 1.2229 - val_accuracy: 0.8136 - lr: 0.0810\n",
            "Epoch 33/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.0426 - accuracy: 0.8733\n",
            "Epoch 00033: val_accuracy did not improve from 0.82170\n",
            "390/390 [==============================] - 172s 440ms/step - loss: 1.0426 - accuracy: 0.8733 - val_loss: 1.6304 - val_accuracy: 0.7097 - lr: 0.0810\n",
            "Epoch 34/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.0587 - accuracy: 0.8692\n",
            "Epoch 00034: val_accuracy did not improve from 0.82170\n",
            "390/390 [==============================] - 172s 440ms/step - loss: 1.0587 - accuracy: 0.8692 - val_loss: 1.3173 - val_accuracy: 0.7981 - lr: 0.0810\n",
            "Epoch 35/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.0580 - accuracy: 0.8725\n",
            "Epoch 00035: val_accuracy did not improve from 0.82170\n",
            "390/390 [==============================] - 172s 440ms/step - loss: 1.0580 - accuracy: 0.8725 - val_loss: 1.6010 - val_accuracy: 0.7105 - lr: 0.0810\n",
            "Epoch 36/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.0267 - accuracy: 0.8808\n",
            "Epoch 00036: val_accuracy did not improve from 0.82170\n",
            "390/390 [==============================] - 171s 440ms/step - loss: 1.0267 - accuracy: 0.8808 - val_loss: 1.4122 - val_accuracy: 0.7643 - lr: 0.0729\n",
            "Epoch 37/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.0098 - accuracy: 0.8850\n",
            "Epoch 00037: val_accuracy did not improve from 0.82170\n",
            "390/390 [==============================] - 171s 440ms/step - loss: 1.0098 - accuracy: 0.8850 - val_loss: 1.3364 - val_accuracy: 0.7834 - lr: 0.0729\n",
            "Epoch 38/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.0278 - accuracy: 0.8810\n",
            "Epoch 00038: val_accuracy improved from 0.82170 to 0.83430, saving model to desktop/Trained_models/vgg16_cifar10\n",
            "INFO:tensorflow:Assets written to: desktop/Trained_models/vgg16_cifar10/assets\n",
            "390/390 [==============================] - 176s 451ms/step - loss: 1.0278 - accuracy: 0.8810 - val_loss: 1.1815 - val_accuracy: 0.8343 - lr: 0.0729\n",
            "Epoch 39/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.0371 - accuracy: 0.8838\n",
            "Epoch 00039: val_accuracy did not improve from 0.83430\n",
            "390/390 [==============================] - 172s 440ms/step - loss: 1.0371 - accuracy: 0.8838 - val_loss: 1.4207 - val_accuracy: 0.7698 - lr: 0.0729\n",
            "Epoch 40/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.0441 - accuracy: 0.8830\n",
            "Epoch 00040: val_accuracy did not improve from 0.83430\n",
            "390/390 [==============================] - 172s 440ms/step - loss: 1.0441 - accuracy: 0.8830 - val_loss: 1.3256 - val_accuracy: 0.8003 - lr: 0.0729\n",
            "Epoch 41/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.0578 - accuracy: 0.8833\n",
            "Epoch 00041: val_accuracy did not improve from 0.83430\n",
            "390/390 [==============================] - 171s 440ms/step - loss: 1.0578 - accuracy: 0.8833 - val_loss: 1.2975 - val_accuracy: 0.8156 - lr: 0.0729\n",
            "Epoch 42/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.0573 - accuracy: 0.8860\n",
            "Epoch 00042: val_accuracy did not improve from 0.83430\n",
            "390/390 [==============================] - 172s 440ms/step - loss: 1.0573 - accuracy: 0.8860 - val_loss: 1.6801 - val_accuracy: 0.7272 - lr: 0.0729\n",
            "Epoch 43/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.0277 - accuracy: 0.8930\n",
            "Epoch 00043: val_accuracy did not improve from 0.83430\n",
            "390/390 [==============================] - 172s 440ms/step - loss: 1.0277 - accuracy: 0.8930 - val_loss: 1.3094 - val_accuracy: 0.8060 - lr: 0.0656\n",
            "Epoch 44/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.0097 - accuracy: 0.8967\n",
            "Epoch 00044: val_accuracy did not improve from 0.83430\n",
            "390/390 [==============================] - 171s 440ms/step - loss: 1.0097 - accuracy: 0.8967 - val_loss: 1.2646 - val_accuracy: 0.8341 - lr: 0.0656\n",
            "Epoch 45/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.0187 - accuracy: 0.8965\n",
            "Epoch 00045: val_accuracy did not improve from 0.83430\n",
            "390/390 [==============================] - 171s 440ms/step - loss: 1.0187 - accuracy: 0.8965 - val_loss: 1.3048 - val_accuracy: 0.8052 - lr: 0.0656\n",
            "Epoch 46/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.0205 - accuracy: 0.8978\n",
            "Epoch 00046: val_accuracy did not improve from 0.83430\n",
            "390/390 [==============================] - 171s 439ms/step - loss: 1.0205 - accuracy: 0.8978 - val_loss: 1.3169 - val_accuracy: 0.8076 - lr: 0.0656\n",
            "Epoch 47/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.0323 - accuracy: 0.8963\n",
            "Epoch 00047: val_accuracy did not improve from 0.83430\n",
            "390/390 [==============================] - 171s 438ms/step - loss: 1.0323 - accuracy: 0.8963 - val_loss: 1.6255 - val_accuracy: 0.7331 - lr: 0.0656\n",
            "Epoch 48/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.0469 - accuracy: 0.8954\n",
            "Epoch 00048: val_accuracy did not improve from 0.83430\n",
            "390/390 [==============================] - 171s 440ms/step - loss: 1.0469 - accuracy: 0.8954 - val_loss: 1.5359 - val_accuracy: 0.7537 - lr: 0.0656\n",
            "Epoch 49/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.0488 - accuracy: 0.8969\n",
            "Epoch 00049: val_accuracy did not improve from 0.83430\n",
            "390/390 [==============================] - 171s 440ms/step - loss: 1.0488 - accuracy: 0.8969 - val_loss: 1.2776 - val_accuracy: 0.8336 - lr: 0.0656\n",
            "Epoch 50/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.0103 - accuracy: 0.9067\n",
            "Epoch 00050: val_accuracy did not improve from 0.83430\n",
            "390/390 [==============================] - 172s 441ms/step - loss: 1.0103 - accuracy: 0.9067 - val_loss: 1.5533 - val_accuracy: 0.7659 - lr: 0.0590\n",
            "Epoch 51/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.0049 - accuracy: 0.9050\n",
            "Epoch 00051: val_accuracy did not improve from 0.83430\n",
            "390/390 [==============================] - 171s 440ms/step - loss: 1.0049 - accuracy: 0.9050 - val_loss: 1.3318 - val_accuracy: 0.8138 - lr: 0.0590\n",
            "Epoch 52/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.0208 - accuracy: 0.9038\n",
            "Epoch 00052: val_accuracy did not improve from 0.83430\n",
            "390/390 [==============================] - 171s 439ms/step - loss: 1.0208 - accuracy: 0.9038 - val_loss: 1.6781 - val_accuracy: 0.7319 - lr: 0.0590\n",
            "Epoch 53/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.0102 - accuracy: 0.9091\n",
            "Epoch 00053: val_accuracy did not improve from 0.83430\n",
            "390/390 [==============================] - 171s 439ms/step - loss: 1.0102 - accuracy: 0.9091 - val_loss: 1.4459 - val_accuracy: 0.7945 - lr: 0.0590\n",
            "Epoch 54/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.0269 - accuracy: 0.9041\n",
            "Epoch 00054: val_accuracy did not improve from 0.83430\n",
            "390/390 [==============================] - 171s 438ms/step - loss: 1.0269 - accuracy: 0.9041 - val_loss: 1.3956 - val_accuracy: 0.7967 - lr: 0.0590\n",
            "Epoch 55/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.0135 - accuracy: 0.9092\n",
            "Epoch 00055: val_accuracy did not improve from 0.83430\n",
            "390/390 [==============================] - 171s 440ms/step - loss: 1.0135 - accuracy: 0.9092 - val_loss: 1.4350 - val_accuracy: 0.7893 - lr: 0.0590\n",
            "Epoch 56/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.0413 - accuracy: 0.9043\n",
            "Epoch 00056: val_accuracy improved from 0.83430 to 0.84490, saving model to desktop/Trained_models/vgg16_cifar10\n",
            "INFO:tensorflow:Assets written to: desktop/Trained_models/vgg16_cifar10/assets\n",
            "390/390 [==============================] - 176s 451ms/step - loss: 1.0413 - accuracy: 0.9043 - val_loss: 1.2858 - val_accuracy: 0.8449 - lr: 0.0590\n",
            "Epoch 57/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.9991 - accuracy: 0.9162\n",
            "Epoch 00057: val_accuracy did not improve from 0.84490\n",
            "390/390 [==============================] - 171s 439ms/step - loss: 0.9991 - accuracy: 0.9162 - val_loss: 1.3230 - val_accuracy: 0.8256 - lr: 0.0531\n",
            "Epoch 58/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.9869 - accuracy: 0.9169\n",
            "Epoch 00058: val_accuracy did not improve from 0.84490\n",
            "390/390 [==============================] - 172s 440ms/step - loss: 0.9869 - accuracy: 0.9169 - val_loss: 1.2792 - val_accuracy: 0.8241 - lr: 0.0531\n",
            "Epoch 59/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.9863 - accuracy: 0.9170\n",
            "Epoch 00059: val_accuracy did not improve from 0.84490\n",
            "390/390 [==============================] - 171s 439ms/step - loss: 0.9863 - accuracy: 0.9170 - val_loss: 1.3429 - val_accuracy: 0.8126 - lr: 0.0531\n",
            "Epoch 60/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.9991 - accuracy: 0.9134\n",
            "Epoch 00060: val_accuracy did not improve from 0.84490\n",
            "390/390 [==============================] - 172s 440ms/step - loss: 0.9991 - accuracy: 0.9134 - val_loss: 1.3540 - val_accuracy: 0.8231 - lr: 0.0531\n",
            "Epoch 61/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.0104 - accuracy: 0.9142\n",
            "Epoch 00061: val_accuracy did not improve from 0.84490\n",
            "390/390 [==============================] - 172s 440ms/step - loss: 1.0104 - accuracy: 0.9142 - val_loss: 1.3655 - val_accuracy: 0.8218 - lr: 0.0531\n",
            "Epoch 62/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.0089 - accuracy: 0.9155\n",
            "Epoch 00062: val_accuracy did not improve from 0.84490\n",
            "390/390 [==============================] - 172s 440ms/step - loss: 1.0089 - accuracy: 0.9155 - val_loss: 1.3561 - val_accuracy: 0.8163 - lr: 0.0531\n",
            "Epoch 63/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.0107 - accuracy: 0.9148\n",
            "Epoch 00063: val_accuracy did not improve from 0.84490\n",
            "390/390 [==============================] - 172s 440ms/step - loss: 1.0107 - accuracy: 0.9148 - val_loss: 1.3873 - val_accuracy: 0.8147 - lr: 0.0531\n",
            "Epoch 64/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.9744 - accuracy: 0.9258\n",
            "Epoch 00064: val_accuracy did not improve from 0.84490\n",
            "390/390 [==============================] - 171s 438ms/step - loss: 0.9744 - accuracy: 0.9258 - val_loss: 1.4435 - val_accuracy: 0.8003 - lr: 0.0478\n",
            "Epoch 65/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.9560 - accuracy: 0.9249\n",
            "Epoch 00065: val_accuracy did not improve from 0.84490\n",
            "390/390 [==============================] - 171s 439ms/step - loss: 0.9560 - accuracy: 0.9249 - val_loss: 1.3637 - val_accuracy: 0.8131 - lr: 0.0478\n",
            "Epoch 66/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.9623 - accuracy: 0.9243\n",
            "Epoch 00066: val_accuracy did not improve from 0.84490\n",
            "390/390 [==============================] - 171s 439ms/step - loss: 0.9623 - accuracy: 0.9243 - val_loss: 1.3065 - val_accuracy: 0.8313 - lr: 0.0478\n",
            "Epoch 67/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.9600 - accuracy: 0.9245\n",
            "Epoch 00067: val_accuracy did not improve from 0.84490\n",
            "390/390 [==============================] - 171s 440ms/step - loss: 0.9600 - accuracy: 0.9245 - val_loss: 1.3025 - val_accuracy: 0.8236 - lr: 0.0478\n",
            "Epoch 68/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.9759 - accuracy: 0.9220\n",
            "Epoch 00068: val_accuracy did not improve from 0.84490\n",
            "390/390 [==============================] - 171s 439ms/step - loss: 0.9759 - accuracy: 0.9220 - val_loss: 1.3514 - val_accuracy: 0.8205 - lr: 0.0478\n",
            "Epoch 69/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.9688 - accuracy: 0.9252\n",
            "Epoch 00069: val_accuracy did not improve from 0.84490\n",
            "390/390 [==============================] - 171s 439ms/step - loss: 0.9688 - accuracy: 0.9252 - val_loss: 1.3360 - val_accuracy: 0.8273 - lr: 0.0478\n",
            "Epoch 70/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.9834 - accuracy: 0.9244\n",
            "Epoch 00070: val_accuracy improved from 0.84490 to 0.85650, saving model to desktop/Trained_models/vgg16_cifar10\n",
            "INFO:tensorflow:Assets written to: desktop/Trained_models/vgg16_cifar10/assets\n",
            "390/390 [==============================] - 176s 451ms/step - loss: 0.9834 - accuracy: 0.9244 - val_loss: 1.2144 - val_accuracy: 0.8565 - lr: 0.0478\n",
            "Epoch 71/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.9323 - accuracy: 0.9347\n",
            "Epoch 00071: val_accuracy did not improve from 0.85650\n",
            "390/390 [==============================] - 172s 441ms/step - loss: 0.9323 - accuracy: 0.9347 - val_loss: 1.3216 - val_accuracy: 0.8332 - lr: 0.0430\n",
            "Epoch 72/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.9201 - accuracy: 0.9310\n",
            "Epoch 00072: val_accuracy did not improve from 0.85650\n",
            "390/390 [==============================] - 172s 440ms/step - loss: 0.9201 - accuracy: 0.9310 - val_loss: 1.2803 - val_accuracy: 0.8427 - lr: 0.0430\n",
            "Epoch 73/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.9146 - accuracy: 0.9331\n",
            "Epoch 00073: val_accuracy did not improve from 0.85650\n",
            "390/390 [==============================] - 171s 439ms/step - loss: 0.9146 - accuracy: 0.9331 - val_loss: 1.2280 - val_accuracy: 0.8474 - lr: 0.0430\n",
            "Epoch 74/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.9272 - accuracy: 0.9301\n",
            "Epoch 00074: val_accuracy did not improve from 0.85650\n",
            "390/390 [==============================] - 171s 439ms/step - loss: 0.9272 - accuracy: 0.9301 - val_loss: 1.5227 - val_accuracy: 0.7969 - lr: 0.0430\n",
            "Epoch 75/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.9358 - accuracy: 0.9285\n",
            "Epoch 00075: val_accuracy did not improve from 0.85650\n",
            "390/390 [==============================] - 171s 439ms/step - loss: 0.9358 - accuracy: 0.9285 - val_loss: 1.3032 - val_accuracy: 0.8241 - lr: 0.0430\n",
            "Epoch 76/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.9357 - accuracy: 0.9311\n",
            "Epoch 00076: val_accuracy did not improve from 0.85650\n",
            "390/390 [==============================] - 172s 440ms/step - loss: 0.9357 - accuracy: 0.9311 - val_loss: 1.2058 - val_accuracy: 0.8530 - lr: 0.0430\n",
            "Epoch 77/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.9293 - accuracy: 0.9327\n",
            "Epoch 00077: val_accuracy did not improve from 0.85650\n",
            "390/390 [==============================] - 171s 439ms/step - loss: 0.9293 - accuracy: 0.9327 - val_loss: 1.3347 - val_accuracy: 0.8117 - lr: 0.0430\n",
            "Epoch 78/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.8960 - accuracy: 0.9398\n",
            "Epoch 00078: val_accuracy improved from 0.85650 to 0.86200, saving model to desktop/Trained_models/vgg16_cifar10\n",
            "INFO:tensorflow:Assets written to: desktop/Trained_models/vgg16_cifar10/assets\n",
            "390/390 [==============================] - 176s 451ms/step - loss: 0.8960 - accuracy: 0.9398 - val_loss: 1.1440 - val_accuracy: 0.8620 - lr: 0.0387\n",
            "Epoch 79/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.8944 - accuracy: 0.9355\n",
            "Epoch 00079: val_accuracy did not improve from 0.86200\n",
            "390/390 [==============================] - 172s 440ms/step - loss: 0.8944 - accuracy: 0.9355 - val_loss: 1.2781 - val_accuracy: 0.8305 - lr: 0.0387\n",
            "Epoch 80/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.8989 - accuracy: 0.9357\n",
            "Epoch 00080: val_accuracy did not improve from 0.86200\n",
            "390/390 [==============================] - 171s 438ms/step - loss: 0.8989 - accuracy: 0.9357 - val_loss: 1.4250 - val_accuracy: 0.8093 - lr: 0.0387\n",
            "Epoch 81/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.8843 - accuracy: 0.9393\n",
            "Epoch 00081: val_accuracy improved from 0.86200 to 0.86580, saving model to desktop/Trained_models/vgg16_cifar10\n",
            "INFO:tensorflow:Assets written to: desktop/Trained_models/vgg16_cifar10/assets\n",
            "390/390 [==============================] - 176s 450ms/step - loss: 0.8843 - accuracy: 0.9393 - val_loss: 1.1791 - val_accuracy: 0.8658 - lr: 0.0387\n",
            "Epoch 82/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.8865 - accuracy: 0.9398\n",
            "Epoch 00082: val_accuracy did not improve from 0.86580\n",
            "390/390 [==============================] - 171s 439ms/step - loss: 0.8865 - accuracy: 0.9398 - val_loss: 1.3391 - val_accuracy: 0.8227 - lr: 0.0387\n",
            "Epoch 83/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.9020 - accuracy: 0.9357\n",
            "Epoch 00083: val_accuracy did not improve from 0.86580\n",
            "390/390 [==============================] - 172s 440ms/step - loss: 0.9020 - accuracy: 0.9357 - val_loss: 1.2776 - val_accuracy: 0.8470 - lr: 0.0387\n",
            "Epoch 84/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.9031 - accuracy: 0.9354\n",
            "Epoch 00084: val_accuracy did not improve from 0.86580\n",
            "390/390 [==============================] - 172s 440ms/step - loss: 0.9031 - accuracy: 0.9354 - val_loss: 1.2557 - val_accuracy: 0.8336 - lr: 0.0387\n",
            "Epoch 85/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.8642 - accuracy: 0.9455\n",
            "Epoch 00085: val_accuracy did not improve from 0.86580\n",
            "390/390 [==============================] - 171s 439ms/step - loss: 0.8642 - accuracy: 0.9455 - val_loss: 1.1850 - val_accuracy: 0.8464 - lr: 0.0349\n",
            "Epoch 86/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.8478 - accuracy: 0.9450\n",
            "Epoch 00086: val_accuracy did not improve from 0.86580\n",
            "390/390 [==============================] - 171s 440ms/step - loss: 0.8478 - accuracy: 0.9450 - val_loss: 1.1662 - val_accuracy: 0.8512 - lr: 0.0349\n",
            "Epoch 87/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.8482 - accuracy: 0.9442\n",
            "Epoch 00087: val_accuracy did not improve from 0.86580\n",
            "390/390 [==============================] - 171s 439ms/step - loss: 0.8482 - accuracy: 0.9442 - val_loss: 1.2752 - val_accuracy: 0.8329 - lr: 0.0349\n",
            "Epoch 88/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.8510 - accuracy: 0.9426\n",
            "Epoch 00088: val_accuracy did not improve from 0.86580\n",
            "390/390 [==============================] - 171s 439ms/step - loss: 0.8510 - accuracy: 0.9426 - val_loss: 1.1696 - val_accuracy: 0.8536 - lr: 0.0349\n",
            "Epoch 89/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.8481 - accuracy: 0.9431\n",
            "Epoch 00089: val_accuracy did not improve from 0.86580\n",
            "390/390 [==============================] - 171s 440ms/step - loss: 0.8481 - accuracy: 0.9431 - val_loss: 1.2925 - val_accuracy: 0.8241 - lr: 0.0349\n",
            "Epoch 90/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.8485 - accuracy: 0.9417\n",
            "Epoch 00090: val_accuracy did not improve from 0.86580\n",
            "390/390 [==============================] - 172s 440ms/step - loss: 0.8485 - accuracy: 0.9417 - val_loss: 1.2306 - val_accuracy: 0.8297 - lr: 0.0349\n",
            "Epoch 91/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.8489 - accuracy: 0.9446\n",
            "Epoch 00091: val_accuracy did not improve from 0.86580\n",
            "390/390 [==============================] - 171s 439ms/step - loss: 0.8489 - accuracy: 0.9446 - val_loss: 1.3238 - val_accuracy: 0.8404 - lr: 0.0349\n",
            "Epoch 92/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.8301 - accuracy: 0.9491\n",
            "Epoch 00092: val_accuracy did not improve from 0.86580\n",
            "390/390 [==============================] - 171s 440ms/step - loss: 0.8301 - accuracy: 0.9491 - val_loss: 1.4971 - val_accuracy: 0.7779 - lr: 0.0314\n",
            "Epoch 93/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.7964 - accuracy: 0.9526\n",
            "Epoch 00093: val_accuracy did not improve from 0.86580\n",
            "390/390 [==============================] - 171s 440ms/step - loss: 0.7964 - accuracy: 0.9526 - val_loss: 1.2362 - val_accuracy: 0.8425 - lr: 0.0314\n",
            "Epoch 94/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.8034 - accuracy: 0.9486\n",
            "Epoch 00094: val_accuracy did not improve from 0.86580\n",
            "390/390 [==============================] - 171s 439ms/step - loss: 0.8034 - accuracy: 0.9486 - val_loss: 1.1742 - val_accuracy: 0.8600 - lr: 0.0314\n",
            "Epoch 95/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.8036 - accuracy: 0.9478\n",
            "Epoch 00095: val_accuracy improved from 0.86580 to 0.87140, saving model to desktop/Trained_models/vgg16_cifar10\n",
            "INFO:tensorflow:Assets written to: desktop/Trained_models/vgg16_cifar10/assets\n",
            "390/390 [==============================] - 176s 451ms/step - loss: 0.8036 - accuracy: 0.9478 - val_loss: 1.0674 - val_accuracy: 0.8714 - lr: 0.0314\n",
            "Epoch 96/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.8117 - accuracy: 0.9468\n",
            "Epoch 00096: val_accuracy did not improve from 0.87140\n",
            "390/390 [==============================] - 171s 440ms/step - loss: 0.8117 - accuracy: 0.9468 - val_loss: 1.1232 - val_accuracy: 0.8566 - lr: 0.0314\n",
            "Epoch 97/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.8071 - accuracy: 0.9477\n",
            "Epoch 00097: val_accuracy did not improve from 0.87140\n",
            "390/390 [==============================] - 172s 440ms/step - loss: 0.8071 - accuracy: 0.9477 - val_loss: 1.1520 - val_accuracy: 0.8583 - lr: 0.0314\n",
            "Epoch 98/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.8097 - accuracy: 0.9465\n",
            "Epoch 00098: val_accuracy did not improve from 0.87140\n",
            "390/390 [==============================] - 171s 439ms/step - loss: 0.8097 - accuracy: 0.9465 - val_loss: 1.1637 - val_accuracy: 0.8514 - lr: 0.0314\n",
            "Epoch 99/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.8137 - accuracy: 0.9467\n",
            "Epoch 00099: val_accuracy did not improve from 0.87140\n",
            "390/390 [==============================] - 171s 439ms/step - loss: 0.8137 - accuracy: 0.9467 - val_loss: 1.1037 - val_accuracy: 0.8664 - lr: 0.0314\n",
            "Epoch 100/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.8101 - accuracy: 0.9488\n",
            "Epoch 00100: val_accuracy did not improve from 0.87140\n",
            "390/390 [==============================] - 171s 439ms/step - loss: 0.8101 - accuracy: 0.9488 - val_loss: 1.1791 - val_accuracy: 0.8562 - lr: 0.0314\n",
            "Epoch 101/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.8127 - accuracy: 0.9480\n",
            "Epoch 00101: val_accuracy did not improve from 0.87140\n",
            "390/390 [==============================] - 171s 438ms/step - loss: 0.8127 - accuracy: 0.9480 - val_loss: 1.1697 - val_accuracy: 0.8596 - lr: 0.0314\n",
            "Epoch 102/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.8110 - accuracy: 0.9490\n",
            "Epoch 00102: val_accuracy did not improve from 0.87140\n",
            "390/390 [==============================] - 171s 440ms/step - loss: 0.8110 - accuracy: 0.9490 - val_loss: 1.1022 - val_accuracy: 0.8644 - lr: 0.0314\n",
            "Epoch 103/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.7888 - accuracy: 0.9549\n",
            "Epoch 00103: val_accuracy did not improve from 0.87140\n",
            "390/390 [==============================] - 172s 440ms/step - loss: 0.7888 - accuracy: 0.9549 - val_loss: 1.1221 - val_accuracy: 0.8613 - lr: 0.0282\n",
            "Epoch 104/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.7655 - accuracy: 0.9553\n",
            "Epoch 00104: val_accuracy did not improve from 0.87140\n",
            "390/390 [==============================] - 171s 440ms/step - loss: 0.7655 - accuracy: 0.9553 - val_loss: 1.1690 - val_accuracy: 0.8463 - lr: 0.0282\n",
            "Epoch 105/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.7601 - accuracy: 0.9544\n",
            "Epoch 00105: val_accuracy did not improve from 0.87140\n",
            "390/390 [==============================] - 171s 439ms/step - loss: 0.7601 - accuracy: 0.9544 - val_loss: 1.1631 - val_accuracy: 0.8419 - lr: 0.0282\n",
            "Epoch 106/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.7638 - accuracy: 0.9525\n",
            "Epoch 00106: val_accuracy did not improve from 0.87140\n",
            "390/390 [==============================] - 171s 439ms/step - loss: 0.7638 - accuracy: 0.9525 - val_loss: 1.2008 - val_accuracy: 0.8437 - lr: 0.0282\n",
            "Epoch 107/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.7700 - accuracy: 0.9529\n",
            "Epoch 00107: val_accuracy did not improve from 0.87140\n",
            "390/390 [==============================] - 171s 438ms/step - loss: 0.7700 - accuracy: 0.9529 - val_loss: 1.0675 - val_accuracy: 0.8668 - lr: 0.0282\n",
            "Epoch 108/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.7705 - accuracy: 0.9520\n",
            "Epoch 00108: val_accuracy did not improve from 0.87140\n",
            "390/390 [==============================] - 171s 439ms/step - loss: 0.7705 - accuracy: 0.9520 - val_loss: 1.1553 - val_accuracy: 0.8492 - lr: 0.0282\n",
            "Epoch 109/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.7726 - accuracy: 0.9519\n",
            "Epoch 00109: val_accuracy did not improve from 0.87140\n",
            "390/390 [==============================] - 172s 440ms/step - loss: 0.7726 - accuracy: 0.9519 - val_loss: 1.1607 - val_accuracy: 0.8479 - lr: 0.0282\n",
            "Epoch 110/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.7497 - accuracy: 0.9578\n",
            "Epoch 00110: val_accuracy improved from 0.87140 to 0.87720, saving model to desktop/Trained_models/vgg16_cifar10\n",
            "INFO:tensorflow:Assets written to: desktop/Trained_models/vgg16_cifar10/assets\n",
            "390/390 [==============================] - 176s 451ms/step - loss: 0.7497 - accuracy: 0.9578 - val_loss: 1.0310 - val_accuracy: 0.8772 - lr: 0.0254\n",
            "Epoch 111/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.7242 - accuracy: 0.9611\n",
            "Epoch 00111: val_accuracy did not improve from 0.87720\n",
            "390/390 [==============================] - 171s 438ms/step - loss: 0.7242 - accuracy: 0.9611 - val_loss: 1.0390 - val_accuracy: 0.8764 - lr: 0.0254\n",
            "Epoch 112/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.7226 - accuracy: 0.9573\n",
            "Epoch 00112: val_accuracy did not improve from 0.87720\n",
            "390/390 [==============================] - 172s 440ms/step - loss: 0.7226 - accuracy: 0.9573 - val_loss: 1.1017 - val_accuracy: 0.8548 - lr: 0.0254\n",
            "Epoch 113/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.7188 - accuracy: 0.9574\n",
            "Epoch 00113: val_accuracy did not improve from 0.87720\n",
            "390/390 [==============================] - 171s 439ms/step - loss: 0.7188 - accuracy: 0.9574 - val_loss: 1.1069 - val_accuracy: 0.8613 - lr: 0.0254\n",
            "Epoch 114/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.7152 - accuracy: 0.9587\n",
            "Epoch 00114: val_accuracy improved from 0.87720 to 0.87810, saving model to desktop/Trained_models/vgg16_cifar10\n",
            "INFO:tensorflow:Assets written to: desktop/Trained_models/vgg16_cifar10/assets\n",
            "390/390 [==============================] - 176s 451ms/step - loss: 0.7152 - accuracy: 0.9587 - val_loss: 1.0564 - val_accuracy: 0.8781 - lr: 0.0254\n",
            "Epoch 115/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.7185 - accuracy: 0.9559\n",
            "Epoch 00115: val_accuracy did not improve from 0.87810\n",
            "390/390 [==============================] - 172s 440ms/step - loss: 0.7185 - accuracy: 0.9559 - val_loss: 1.0988 - val_accuracy: 0.8546 - lr: 0.0254\n",
            "Epoch 116/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.7284 - accuracy: 0.9545\n",
            "Epoch 00116: val_accuracy did not improve from 0.87810\n",
            "390/390 [==============================] - 172s 440ms/step - loss: 0.7284 - accuracy: 0.9545 - val_loss: 1.0563 - val_accuracy: 0.8667 - lr: 0.0254\n",
            "Epoch 117/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.7187 - accuracy: 0.9571\n",
            "Epoch 00117: val_accuracy did not improve from 0.87810\n",
            "390/390 [==============================] - 171s 440ms/step - loss: 0.7187 - accuracy: 0.9571 - val_loss: 1.0730 - val_accuracy: 0.8692 - lr: 0.0254\n",
            "Epoch 118/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.6975 - accuracy: 0.9621\n",
            "Epoch 00118: val_accuracy did not improve from 0.87810\n",
            "390/390 [==============================] - 171s 440ms/step - loss: 0.6975 - accuracy: 0.9621 - val_loss: 1.0542 - val_accuracy: 0.8650 - lr: 0.0229\n",
            "Epoch 119/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.6849 - accuracy: 0.9625\n",
            "Epoch 00119: val_accuracy did not improve from 0.87810\n",
            "390/390 [==============================] - 171s 439ms/step - loss: 0.6849 - accuracy: 0.9625 - val_loss: 1.1510 - val_accuracy: 0.8493 - lr: 0.0229\n",
            "Epoch 120/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.6798 - accuracy: 0.9612\n",
            "Epoch 00120: val_accuracy did not improve from 0.87810\n",
            "390/390 [==============================] - 171s 439ms/step - loss: 0.6798 - accuracy: 0.9612 - val_loss: 1.0948 - val_accuracy: 0.8552 - lr: 0.0229\n",
            "Epoch 121/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.6801 - accuracy: 0.9601\n",
            "Epoch 00121: val_accuracy improved from 0.87810 to 0.88730, saving model to desktop/Trained_models/vgg16_cifar10\n",
            "INFO:tensorflow:Assets written to: desktop/Trained_models/vgg16_cifar10/assets\n",
            "390/390 [==============================] - 176s 451ms/step - loss: 0.6801 - accuracy: 0.9601 - val_loss: 0.9412 - val_accuracy: 0.8873 - lr: 0.0229\n",
            "Epoch 122/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.6742 - accuracy: 0.9618\n",
            "Epoch 00122: val_accuracy did not improve from 0.88730\n",
            "390/390 [==============================] - 172s 440ms/step - loss: 0.6742 - accuracy: 0.9618 - val_loss: 1.1271 - val_accuracy: 0.8519 - lr: 0.0229\n",
            "Epoch 123/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.6744 - accuracy: 0.9609\n",
            "Epoch 00123: val_accuracy did not improve from 0.88730\n",
            "390/390 [==============================] - 172s 440ms/step - loss: 0.6744 - accuracy: 0.9609 - val_loss: 1.1627 - val_accuracy: 0.8375 - lr: 0.0229\n",
            "Epoch 124/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.6748 - accuracy: 0.9597\n",
            "Epoch 00124: val_accuracy did not improve from 0.88730\n",
            "390/390 [==============================] - 171s 440ms/step - loss: 0.6748 - accuracy: 0.9597 - val_loss: 1.0141 - val_accuracy: 0.8653 - lr: 0.0229\n",
            "Epoch 125/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.6764 - accuracy: 0.9599\n",
            "Epoch 00125: val_accuracy did not improve from 0.88730\n",
            "390/390 [==============================] - 172s 440ms/step - loss: 0.6764 - accuracy: 0.9599 - val_loss: 0.9899 - val_accuracy: 0.8785 - lr: 0.0229\n",
            "Epoch 126/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.6810 - accuracy: 0.9601\n",
            "Epoch 00126: val_accuracy did not improve from 0.88730\n",
            "390/390 [==============================] - 171s 439ms/step - loss: 0.6810 - accuracy: 0.9601 - val_loss: 1.0397 - val_accuracy: 0.8614 - lr: 0.0229\n",
            "Epoch 127/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.6750 - accuracy: 0.9613\n",
            "Epoch 00127: val_accuracy did not improve from 0.88730\n",
            "390/390 [==============================] - 172s 440ms/step - loss: 0.6750 - accuracy: 0.9613 - val_loss: 1.0774 - val_accuracy: 0.8622 - lr: 0.0229\n",
            "Epoch 128/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.6710 - accuracy: 0.9620\n",
            "Epoch 00128: val_accuracy did not improve from 0.88730\n",
            "390/390 [==============================] - 171s 440ms/step - loss: 0.6710 - accuracy: 0.9620 - val_loss: 1.0130 - val_accuracy: 0.8660 - lr: 0.0229\n",
            "Epoch 129/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.6519 - accuracy: 0.9667\n",
            "Epoch 00129: val_accuracy did not improve from 0.88730\n",
            "390/390 [==============================] - 171s 440ms/step - loss: 0.6519 - accuracy: 0.9667 - val_loss: 0.9549 - val_accuracy: 0.8805 - lr: 0.0206\n",
            "Epoch 130/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.6416 - accuracy: 0.9648\n",
            "Epoch 00130: val_accuracy did not improve from 0.88730\n",
            "390/390 [==============================] - 171s 440ms/step - loss: 0.6416 - accuracy: 0.9648 - val_loss: 0.9978 - val_accuracy: 0.8684 - lr: 0.0206\n",
            "Epoch 131/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.6388 - accuracy: 0.9652\n",
            "Epoch 00131: val_accuracy did not improve from 0.88730\n",
            "390/390 [==============================] - 172s 440ms/step - loss: 0.6388 - accuracy: 0.9652 - val_loss: 0.9411 - val_accuracy: 0.8843 - lr: 0.0206\n",
            "Epoch 132/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.6360 - accuracy: 0.9645\n",
            "Epoch 00132: val_accuracy did not improve from 0.88730\n",
            "390/390 [==============================] - 172s 440ms/step - loss: 0.6360 - accuracy: 0.9645 - val_loss: 0.9899 - val_accuracy: 0.8722 - lr: 0.0206\n",
            "Epoch 133/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.6323 - accuracy: 0.9659\n",
            "Epoch 00133: val_accuracy did not improve from 0.88730\n",
            "390/390 [==============================] - 172s 440ms/step - loss: 0.6323 - accuracy: 0.9659 - val_loss: 1.0771 - val_accuracy: 0.8562 - lr: 0.0206\n",
            "Epoch 134/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.6273 - accuracy: 0.9650\n",
            "Epoch 00134: val_accuracy did not improve from 0.88730\n",
            "390/390 [==============================] - 171s 439ms/step - loss: 0.6273 - accuracy: 0.9650 - val_loss: 1.0259 - val_accuracy: 0.8631 - lr: 0.0206\n",
            "Epoch 135/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.6290 - accuracy: 0.9634\n",
            "Epoch 00135: val_accuracy did not improve from 0.88730\n",
            "390/390 [==============================] - 171s 439ms/step - loss: 0.6290 - accuracy: 0.9634 - val_loss: 1.0089 - val_accuracy: 0.8665 - lr: 0.0206\n",
            "Epoch 136/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.6353 - accuracy: 0.9630\n",
            "Epoch 00136: val_accuracy did not improve from 0.88730\n",
            "390/390 [==============================] - 171s 439ms/step - loss: 0.6353 - accuracy: 0.9630 - val_loss: 1.1112 - val_accuracy: 0.8413 - lr: 0.0206\n",
            "Epoch 137/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.6313 - accuracy: 0.9636\n",
            "Epoch 00137: val_accuracy did not improve from 0.88730\n",
            "390/390 [==============================] - 171s 439ms/step - loss: 0.6313 - accuracy: 0.9636 - val_loss: 1.1140 - val_accuracy: 0.8495 - lr: 0.0206\n",
            "Epoch 138/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.6360 - accuracy: 0.9623\n",
            "Epoch 00138: val_accuracy did not improve from 0.88730\n",
            "390/390 [==============================] - 171s 439ms/step - loss: 0.6360 - accuracy: 0.9623 - val_loss: 1.0390 - val_accuracy: 0.8568 - lr: 0.0206\n",
            "Epoch 139/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.6167 - accuracy: 0.9689\n",
            "Epoch 00139: val_accuracy improved from 0.88730 to 0.89030, saving model to desktop/Trained_models/vgg16_cifar10\n",
            "INFO:tensorflow:Assets written to: desktop/Trained_models/vgg16_cifar10/assets\n",
            "390/390 [==============================] - 175s 449ms/step - loss: 0.6167 - accuracy: 0.9689 - val_loss: 0.8973 - val_accuracy: 0.8903 - lr: 0.0185\n",
            "Epoch 140/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.6009 - accuracy: 0.9698\n",
            "Epoch 00140: val_accuracy did not improve from 0.89030\n",
            "390/390 [==============================] - 171s 440ms/step - loss: 0.6009 - accuracy: 0.9698 - val_loss: 0.9403 - val_accuracy: 0.8813 - lr: 0.0185\n",
            "Epoch 141/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.5894 - accuracy: 0.9706\n",
            "Epoch 00141: val_accuracy did not improve from 0.89030\n",
            "390/390 [==============================] - 171s 439ms/step - loss: 0.5894 - accuracy: 0.9706 - val_loss: 1.0623 - val_accuracy: 0.8564 - lr: 0.0185\n",
            "Epoch 142/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.5952 - accuracy: 0.9670\n",
            "Epoch 00142: val_accuracy did not improve from 0.89030\n",
            "390/390 [==============================] - 171s 440ms/step - loss: 0.5952 - accuracy: 0.9670 - val_loss: 0.9810 - val_accuracy: 0.8676 - lr: 0.0185\n",
            "Epoch 143/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.5975 - accuracy: 0.9666\n",
            "Epoch 00143: val_accuracy did not improve from 0.89030\n",
            "390/390 [==============================] - 172s 440ms/step - loss: 0.5975 - accuracy: 0.9666 - val_loss: 0.9683 - val_accuracy: 0.8779 - lr: 0.0185\n",
            "Epoch 144/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.5998 - accuracy: 0.9655\n",
            "Epoch 00144: val_accuracy did not improve from 0.89030\n",
            "390/390 [==============================] - 171s 440ms/step - loss: 0.5998 - accuracy: 0.9655 - val_loss: 0.9702 - val_accuracy: 0.8742 - lr: 0.0185\n",
            "Epoch 145/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.5936 - accuracy: 0.9686\n",
            "Epoch 00145: val_accuracy did not improve from 0.89030\n",
            "390/390 [==============================] - 171s 439ms/step - loss: 0.5936 - accuracy: 0.9686 - val_loss: 0.8802 - val_accuracy: 0.8871 - lr: 0.0185\n",
            "Epoch 146/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.5919 - accuracy: 0.9671\n",
            "Epoch 00146: val_accuracy did not improve from 0.89030\n",
            "390/390 [==============================] - 171s 439ms/step - loss: 0.5919 - accuracy: 0.9671 - val_loss: 0.9820 - val_accuracy: 0.8650 - lr: 0.0185\n",
            "Epoch 147/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.6066 - accuracy: 0.9628\n",
            "Epoch 00147: val_accuracy did not improve from 0.89030\n",
            "390/390 [==============================] - 171s 440ms/step - loss: 0.6066 - accuracy: 0.9628 - val_loss: 0.9511 - val_accuracy: 0.8747 - lr: 0.0185\n",
            "Epoch 148/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.5978 - accuracy: 0.9676\n",
            "Epoch 00148: val_accuracy did not improve from 0.89030\n",
            "390/390 [==============================] - 172s 440ms/step - loss: 0.5978 - accuracy: 0.9676 - val_loss: 0.9438 - val_accuracy: 0.8804 - lr: 0.0185\n",
            "Epoch 149/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.5998 - accuracy: 0.9665\n",
            "Epoch 00149: val_accuracy did not improve from 0.89030\n",
            "390/390 [==============================] - 171s 439ms/step - loss: 0.5998 - accuracy: 0.9665 - val_loss: 0.9903 - val_accuracy: 0.8665 - lr: 0.0185\n",
            "Epoch 150/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.6016 - accuracy: 0.9672\n",
            "Epoch 00150: val_accuracy did not improve from 0.89030\n",
            "390/390 [==============================] - 171s 439ms/step - loss: 0.6016 - accuracy: 0.9672 - val_loss: 0.8922 - val_accuracy: 0.8819 - lr: 0.0185\n",
            "Epoch 151/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.5915 - accuracy: 0.9693\n",
            "Epoch 00151: val_accuracy did not improve from 0.89030\n",
            "390/390 [==============================] - 171s 439ms/step - loss: 0.5915 - accuracy: 0.9693 - val_loss: 0.9292 - val_accuracy: 0.8789 - lr: 0.0185\n",
            "Epoch 152/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.5928 - accuracy: 0.9685\n",
            "Epoch 00152: val_accuracy did not improve from 0.89030\n",
            "390/390 [==============================] - 171s 439ms/step - loss: 0.5928 - accuracy: 0.9685 - val_loss: 0.8910 - val_accuracy: 0.8857 - lr: 0.0185\n",
            "Epoch 153/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.5842 - accuracy: 0.9700\n",
            "Epoch 00153: val_accuracy did not improve from 0.89030\n",
            "390/390 [==============================] - 171s 440ms/step - loss: 0.5842 - accuracy: 0.9700 - val_loss: 0.9138 - val_accuracy: 0.8837 - lr: 0.0167\n",
            "Epoch 154/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.5680 - accuracy: 0.9723\n",
            "Epoch 00154: val_accuracy did not improve from 0.89030\n",
            "390/390 [==============================] - 171s 440ms/step - loss: 0.5680 - accuracy: 0.9723 - val_loss: 0.9464 - val_accuracy: 0.8842 - lr: 0.0167\n",
            "Epoch 155/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.5621 - accuracy: 0.9721\n",
            "Epoch 00155: val_accuracy did not improve from 0.89030\n",
            "390/390 [==============================] - 171s 439ms/step - loss: 0.5621 - accuracy: 0.9721 - val_loss: 1.0088 - val_accuracy: 0.8667 - lr: 0.0167\n",
            "Epoch 156/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.5585 - accuracy: 0.9711\n",
            "Epoch 00156: val_accuracy did not improve from 0.89030\n",
            "390/390 [==============================] - 171s 438ms/step - loss: 0.5585 - accuracy: 0.9711 - val_loss: 0.8659 - val_accuracy: 0.8903 - lr: 0.0167\n",
            "Epoch 157/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.5613 - accuracy: 0.9701\n",
            "Epoch 00157: val_accuracy did not improve from 0.89030\n",
            "390/390 [==============================] - 171s 439ms/step - loss: 0.5613 - accuracy: 0.9701 - val_loss: 0.9134 - val_accuracy: 0.8795 - lr: 0.0167\n",
            "Epoch 158/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.5685 - accuracy: 0.9684\n",
            "Epoch 00158: val_accuracy did not improve from 0.89030\n",
            "390/390 [==============================] - 171s 439ms/step - loss: 0.5685 - accuracy: 0.9684 - val_loss: 0.8844 - val_accuracy: 0.8834 - lr: 0.0167\n",
            "Epoch 159/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.5579 - accuracy: 0.9713\n",
            "Epoch 00159: val_accuracy did not improve from 0.89030\n",
            "390/390 [==============================] - 172s 440ms/step - loss: 0.5579 - accuracy: 0.9713 - val_loss: 0.9557 - val_accuracy: 0.8676 - lr: 0.0167\n",
            "Epoch 160/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.5573 - accuracy: 0.9700\n",
            "Epoch 00160: val_accuracy did not improve from 0.89030\n",
            "390/390 [==============================] - 171s 439ms/step - loss: 0.5573 - accuracy: 0.9700 - val_loss: 0.9093 - val_accuracy: 0.8789 - lr: 0.0167\n",
            "Epoch 161/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.5553 - accuracy: 0.9702\n",
            "Epoch 00161: val_accuracy did not improve from 0.89030\n",
            "390/390 [==============================] - 171s 438ms/step - loss: 0.5553 - accuracy: 0.9702 - val_loss: 0.9435 - val_accuracy: 0.8685 - lr: 0.0167\n",
            "Epoch 162/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.5693 - accuracy: 0.9675\n",
            "Epoch 00162: val_accuracy did not improve from 0.89030\n",
            "390/390 [==============================] - 171s 440ms/step - loss: 0.5693 - accuracy: 0.9675 - val_loss: 0.9160 - val_accuracy: 0.8793 - lr: 0.0167\n",
            "Epoch 163/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.5545 - accuracy: 0.9718\n",
            "Epoch 00163: val_accuracy did not improve from 0.89030\n",
            "390/390 [==============================] - 172s 440ms/step - loss: 0.5545 - accuracy: 0.9718 - val_loss: 0.9481 - val_accuracy: 0.8705 - lr: 0.0167\n",
            "Epoch 164/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.5479 - accuracy: 0.9722\n",
            "Epoch 00164: val_accuracy improved from 0.89030 to 0.89270, saving model to desktop/Trained_models/vgg16_cifar10\n",
            "INFO:tensorflow:Assets written to: desktop/Trained_models/vgg16_cifar10/assets\n",
            "390/390 [==============================] - 176s 450ms/step - loss: 0.5479 - accuracy: 0.9722 - val_loss: 0.8330 - val_accuracy: 0.8927 - lr: 0.0150\n",
            "Epoch 165/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.5318 - accuracy: 0.9749\n",
            "Epoch 00165: val_accuracy did not improve from 0.89270\n",
            "390/390 [==============================] - 171s 440ms/step - loss: 0.5318 - accuracy: 0.9749 - val_loss: 0.8710 - val_accuracy: 0.8834 - lr: 0.0150\n",
            "Epoch 166/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.5218 - accuracy: 0.9749\n",
            "Epoch 00166: val_accuracy did not improve from 0.89270\n",
            "390/390 [==============================] - 172s 440ms/step - loss: 0.5218 - accuracy: 0.9749 - val_loss: 0.8701 - val_accuracy: 0.8873 - lr: 0.0150\n",
            "Epoch 167/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.5240 - accuracy: 0.9725\n",
            "Epoch 00167: val_accuracy did not improve from 0.89270\n",
            "390/390 [==============================] - 172s 440ms/step - loss: 0.5240 - accuracy: 0.9725 - val_loss: 0.9389 - val_accuracy: 0.8683 - lr: 0.0150\n",
            "Epoch 168/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.5263 - accuracy: 0.9719\n",
            "Epoch 00168: val_accuracy did not improve from 0.89270\n",
            "390/390 [==============================] - 171s 439ms/step - loss: 0.5263 - accuracy: 0.9719 - val_loss: 0.8543 - val_accuracy: 0.8861 - lr: 0.0150\n",
            "Epoch 169/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.5260 - accuracy: 0.9713\n",
            "Epoch 00169: val_accuracy did not improve from 0.89270\n",
            "390/390 [==============================] - 171s 439ms/step - loss: 0.5260 - accuracy: 0.9713 - val_loss: 0.8545 - val_accuracy: 0.8813 - lr: 0.0150\n",
            "Epoch 170/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.5290 - accuracy: 0.9708\n",
            "Epoch 00170: val_accuracy did not improve from 0.89270\n",
            "390/390 [==============================] - 172s 440ms/step - loss: 0.5290 - accuracy: 0.9708 - val_loss: 1.0230 - val_accuracy: 0.8557 - lr: 0.0150\n",
            "Epoch 171/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.5292 - accuracy: 0.9719\n",
            "Epoch 00171: val_accuracy did not improve from 0.89270\n",
            "390/390 [==============================] - 171s 439ms/step - loss: 0.5292 - accuracy: 0.9719 - val_loss: 0.9195 - val_accuracy: 0.8754 - lr: 0.0150\n",
            "Epoch 172/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.5129 - accuracy: 0.9764\n",
            "Epoch 00172: val_accuracy improved from 0.89270 to 0.89610, saving model to desktop/Trained_models/vgg16_cifar10\n",
            "INFO:tensorflow:Assets written to: desktop/Trained_models/vgg16_cifar10/assets\n",
            "390/390 [==============================] - 176s 451ms/step - loss: 0.5129 - accuracy: 0.9764 - val_loss: 0.8418 - val_accuracy: 0.8961 - lr: 0.0135\n",
            "Epoch 173/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.5044 - accuracy: 0.9765\n",
            "Epoch 00173: val_accuracy did not improve from 0.89610\n",
            "390/390 [==============================] - 172s 440ms/step - loss: 0.5044 - accuracy: 0.9765 - val_loss: 0.9056 - val_accuracy: 0.8745 - lr: 0.0135\n",
            "Epoch 174/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.4885 - accuracy: 0.9779\n",
            "Epoch 00174: val_accuracy did not improve from 0.89610\n",
            "390/390 [==============================] - 171s 439ms/step - loss: 0.4885 - accuracy: 0.9779 - val_loss: 0.8672 - val_accuracy: 0.8820 - lr: 0.0135\n",
            "Epoch 175/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.4915 - accuracy: 0.9755\n",
            "Epoch 00175: val_accuracy did not improve from 0.89610\n",
            "390/390 [==============================] - 171s 439ms/step - loss: 0.4915 - accuracy: 0.9755 - val_loss: 0.8346 - val_accuracy: 0.8848 - lr: 0.0135\n",
            "Epoch 176/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.4863 - accuracy: 0.9759\n",
            "Epoch 00176: val_accuracy did not improve from 0.89610\n",
            "390/390 [==============================] - 171s 439ms/step - loss: 0.4863 - accuracy: 0.9759 - val_loss: 0.8733 - val_accuracy: 0.8845 - lr: 0.0135\n",
            "Epoch 177/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.4859 - accuracy: 0.9749\n",
            "Epoch 00177: val_accuracy did not improve from 0.89610\n",
            "390/390 [==============================] - 171s 439ms/step - loss: 0.4859 - accuracy: 0.9749 - val_loss: 0.8619 - val_accuracy: 0.8802 - lr: 0.0135\n",
            "Epoch 178/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.4855 - accuracy: 0.9750\n",
            "Epoch 00178: val_accuracy did not improve from 0.89610\n",
            "390/390 [==============================] - 171s 439ms/step - loss: 0.4855 - accuracy: 0.9750 - val_loss: 0.8726 - val_accuracy: 0.8819 - lr: 0.0135\n",
            "Epoch 179/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.4734 - accuracy: 0.9781\n",
            "Epoch 00179: val_accuracy did not improve from 0.89610\n",
            "390/390 [==============================] - 171s 439ms/step - loss: 0.4734 - accuracy: 0.9781 - val_loss: 0.8221 - val_accuracy: 0.8941 - lr: 0.0122\n",
            "Epoch 180/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.4685 - accuracy: 0.9771\n",
            "Epoch 00180: val_accuracy improved from 0.89610 to 0.89830, saving model to desktop/Trained_models/vgg16_cifar10\n",
            "INFO:tensorflow:Assets written to: desktop/Trained_models/vgg16_cifar10/assets\n",
            "390/390 [==============================] - 176s 450ms/step - loss: 0.4685 - accuracy: 0.9771 - val_loss: 0.7786 - val_accuracy: 0.8983 - lr: 0.0122\n",
            "Epoch 181/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.4642 - accuracy: 0.9773\n",
            "Epoch 00181: val_accuracy did not improve from 0.89830\n",
            "390/390 [==============================] - 171s 440ms/step - loss: 0.4642 - accuracy: 0.9773 - val_loss: 0.7980 - val_accuracy: 0.8934 - lr: 0.0122\n",
            "Epoch 182/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.4603 - accuracy: 0.9776\n",
            "Epoch 00182: val_accuracy did not improve from 0.89830\n",
            "390/390 [==============================] - 171s 439ms/step - loss: 0.4603 - accuracy: 0.9776 - val_loss: 0.8042 - val_accuracy: 0.8899 - lr: 0.0122\n",
            "Epoch 183/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.4504 - accuracy: 0.9792\n",
            "Epoch 00183: val_accuracy did not improve from 0.89830\n",
            "390/390 [==============================] - 171s 439ms/step - loss: 0.4504 - accuracy: 0.9792 - val_loss: 0.8530 - val_accuracy: 0.8816 - lr: 0.0122\n",
            "Epoch 184/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.4539 - accuracy: 0.9771\n",
            "Epoch 00184: val_accuracy did not improve from 0.89830\n",
            "390/390 [==============================] - 171s 439ms/step - loss: 0.4539 - accuracy: 0.9771 - val_loss: 0.8509 - val_accuracy: 0.8839 - lr: 0.0122\n",
            "Epoch 185/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.4526 - accuracy: 0.9771\n",
            "Epoch 00185: val_accuracy did not improve from 0.89830\n",
            "390/390 [==============================] - 171s 440ms/step - loss: 0.4526 - accuracy: 0.9771 - val_loss: 0.8477 - val_accuracy: 0.8777 - lr: 0.0122\n",
            "Epoch 186/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.4517 - accuracy: 0.9767\n",
            "Epoch 00186: val_accuracy did not improve from 0.89830\n",
            "390/390 [==============================] - 171s 439ms/step - loss: 0.4517 - accuracy: 0.9767 - val_loss: 0.8403 - val_accuracy: 0.8822 - lr: 0.0122\n",
            "Epoch 187/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.4506 - accuracy: 0.9770\n",
            "Epoch 00187: val_accuracy did not improve from 0.89830\n",
            "390/390 [==============================] - 171s 439ms/step - loss: 0.4506 - accuracy: 0.9770 - val_loss: 0.7766 - val_accuracy: 0.8912 - lr: 0.0122\n",
            "Epoch 188/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.4522 - accuracy: 0.9767\n",
            "Epoch 00188: val_accuracy did not improve from 0.89830\n",
            "390/390 [==============================] - 171s 440ms/step - loss: 0.4522 - accuracy: 0.9767 - val_loss: 0.8301 - val_accuracy: 0.8876 - lr: 0.0122\n",
            "Epoch 189/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.4514 - accuracy: 0.9764\n",
            "Epoch 00189: val_accuracy did not improve from 0.89830\n",
            "390/390 [==============================] - 171s 439ms/step - loss: 0.4514 - accuracy: 0.9764 - val_loss: 0.7869 - val_accuracy: 0.8881 - lr: 0.0122\n",
            "Epoch 190/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.4525 - accuracy: 0.9771\n",
            "Epoch 00190: val_accuracy did not improve from 0.89830\n",
            "390/390 [==============================] - 171s 439ms/step - loss: 0.4525 - accuracy: 0.9771 - val_loss: 0.8614 - val_accuracy: 0.8711 - lr: 0.0122\n",
            "Epoch 191/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.4497 - accuracy: 0.9769\n",
            "Epoch 00191: val_accuracy did not improve from 0.89830\n",
            "390/390 [==============================] - 171s 438ms/step - loss: 0.4497 - accuracy: 0.9769 - val_loss: 0.7714 - val_accuracy: 0.8901 - lr: 0.0122\n",
            "Epoch 192/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.4466 - accuracy: 0.9779\n",
            "Epoch 00192: val_accuracy did not improve from 0.89830\n",
            "390/390 [==============================] - 171s 438ms/step - loss: 0.4466 - accuracy: 0.9779 - val_loss: 0.8313 - val_accuracy: 0.8821 - lr: 0.0122\n",
            "Epoch 193/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.4523 - accuracy: 0.9753\n",
            "Epoch 00193: val_accuracy did not improve from 0.89830\n",
            "390/390 [==============================] - 171s 440ms/step - loss: 0.4523 - accuracy: 0.9753 - val_loss: 0.8779 - val_accuracy: 0.8698 - lr: 0.0122\n",
            "Epoch 194/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.4521 - accuracy: 0.9759\n",
            "Epoch 00194: val_accuracy did not improve from 0.89830\n",
            "390/390 [==============================] - 171s 439ms/step - loss: 0.4521 - accuracy: 0.9759 - val_loss: 0.8046 - val_accuracy: 0.8829 - lr: 0.0122\n",
            "Epoch 195/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.4471 - accuracy: 0.9776\n",
            "Epoch 00195: val_accuracy did not improve from 0.89830\n",
            "390/390 [==============================] - 171s 439ms/step - loss: 0.4471 - accuracy: 0.9776 - val_loss: 0.9177 - val_accuracy: 0.8679 - lr: 0.0122\n",
            "Epoch 196/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.4568 - accuracy: 0.9753\n",
            "Epoch 00196: val_accuracy did not improve from 0.89830\n",
            "390/390 [==============================] - 171s 439ms/step - loss: 0.4568 - accuracy: 0.9753 - val_loss: 0.8489 - val_accuracy: 0.8769 - lr: 0.0122\n",
            "Epoch 197/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.4556 - accuracy: 0.9759\n",
            "Epoch 00197: val_accuracy did not improve from 0.89830\n",
            "390/390 [==============================] - 171s 439ms/step - loss: 0.4556 - accuracy: 0.9759 - val_loss: 0.9329 - val_accuracy: 0.8642 - lr: 0.0122\n",
            "Epoch 198/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.4601 - accuracy: 0.9750\n",
            "Epoch 00198: val_accuracy did not improve from 0.89830\n",
            "390/390 [==============================] - 171s 439ms/step - loss: 0.4601 - accuracy: 0.9750 - val_loss: 0.8155 - val_accuracy: 0.8788 - lr: 0.0122\n",
            "Epoch 199/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.4433 - accuracy: 0.9796\n",
            "Epoch 00199: val_accuracy did not improve from 0.89830\n",
            "390/390 [==============================] - 171s 440ms/step - loss: 0.4433 - accuracy: 0.9796 - val_loss: 0.8209 - val_accuracy: 0.8828 - lr: 0.0109\n",
            "Epoch 200/200\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.4312 - accuracy: 0.9815\n",
            "Epoch 00200: val_accuracy did not improve from 0.89830\n",
            "390/390 [==============================] - 171s 438ms/step - loss: 0.4312 - accuracy: 0.9815 - val_loss: 0.8205 - val_accuracy: 0.8871 - lr: 0.0109\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Visualize history\n",
        "# Plot history: Loss\n",
        "plt.figure(figsize = (10,8))\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.plot(history.history['loss'],label='Train loss')\n",
        "plt.title('Validation loss history vgg16')\n",
        "plt.ylabel('Loss value')\n",
        "plt.xlabel('No. epoch')\n",
        "plt.show()\n",
        "\n",
        "# Plot history: Accuracy\n",
        "plt.figure(figsize = (10,8))\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.plot(history.history['accuracy'],label='Train accuracy')\n",
        "plt.title('Validation accuracy history vgg16')\n",
        "plt.ylabel('Accuracy value (%)')\n",
        "plt.xlabel('No. epoch')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Dg8MSQzh6Tbl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f529bceb-e06a-470d-aeeb-e77cb8fd7677"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x576 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x576 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras,os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential,Model\n",
        "from keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D , Dropout\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from keras.initializers import glorot_uniform\n",
        "from keras import applications\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50 , preprocess_input\n",
        "from keras import regularizers\n",
        "from absl import app, flags\n",
        "\n",
        "# Install bleeding edge version of cleverhans\n",
        "!pip install git+https://github.com/tensorflow/cleverhans.git#egg=cleverhans\n",
        "from cleverhans.tf2.attacks.projected_gradient_descent import projected_gradient_descent\n",
        "from cleverhans.tf2.attacks.fast_gradient_method import fast_gradient_method"
      ],
      "metadata": {
        "id": "JcQE-CkzRIUw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4d72a27-1fae-48e7-ad72-7f86f623d4f8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: cleverhans from git+https://github.com/tensorflow/cleverhans.git#egg=cleverhans in ./.local/lib/python3.6/site-packages (4.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from cleverhans) (1.19.1)\n",
            "Requirement already satisfied: tensorflow-probability in ./.local/lib/python3.6/site-packages (from cleverhans) (0.14.1)\n",
            "Requirement already satisfied: joblib in ./.local/lib/python3.6/site-packages (from cleverhans) (1.1.0)\n",
            "Requirement already satisfied: easydict in ./.local/lib/python3.6/site-packages (from cleverhans) (1.9)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from cleverhans) (1.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cleverhans) (1.15.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from cleverhans) (3.3.1)\n",
            "Requirement already satisfied: nose in ./.local/lib/python3.6/site-packages (from cleverhans) (1.3.7)\n",
            "Requirement already satisfied: mnist in ./.local/lib/python3.6/site-packages (from cleverhans) (0.2.2)\n",
            "Requirement already satisfied: pycodestyle in ./.local/lib/python3.6/site-packages (from cleverhans) (2.8.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from cleverhans) (0.10.0)\n",
            "Requirement already satisfied: dm-tree in ./.local/lib/python3.6/site-packages (from tensorflow-probability->cleverhans) (0.1.6)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in ./.local/lib/python3.6/site-packages (from tensorflow-probability->cleverhans) (2.0.0)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability->cleverhans) (0.3.3)\n",
            "Requirement already satisfied: decorator in ./.local/lib/python3.6/site-packages (from tensorflow-probability->cleverhans) (4.4.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.6/dist-packages (from matplotlib->cleverhans) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->cleverhans) (2.8.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.6/dist-packages (from matplotlib->cleverhans) (7.2.0)\n",
            "Requirement already satisfied: certifi>=2020.06.20 in /usr/local/lib/python3.6/dist-packages (from matplotlib->cleverhans) (2020.6.20)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->cleverhans) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->cleverhans) (0.10.0)\n",
            "\u001b[33mWARNING: You are using pip version 20.2.2; however, version 21.3.1 is available.\n",
            "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def make_prediction(model , image , true_value):\n",
        "  true_label_index = -1\n",
        "  for i in range(len(true_value)):\n",
        "\n",
        "    if(true_value[i]==1):\n",
        "      true_label_index = i\n",
        "      break\n",
        "\n",
        "  prediction = model.predict(image)[0]\n",
        "  probability = float('-inf')\n",
        "  predicted_label_index = -1\n",
        "\n",
        "  for i in range(len(prediction)):\n",
        "\n",
        "    if(prediction[i]>probability):\n",
        "      probability = prediction[i]\n",
        "      predicted_label_index = i\n",
        "\n",
        "  if(true_label_index!=predicted_label_index):\n",
        "    return 1\n",
        "  return 0"
      ],
      "metadata": {
        "id": "SjTOAz-uQ1rg"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.load_model(model_path)"
      ],
      "metadata": {
        "id": "ovkUDKagU3Xp"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cifar-100\n"
      ],
      "metadata": {
        "id": "mkuzknVeqYet"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Yjwl08XIqYD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import img_to_array\n",
        "\n",
        "epsilon_end = 0.8\n",
        "#logits_model = tf.keras.Model(model.input,model.layers[-1].output)\n",
        "total_images = 200\n",
        "\n",
        "result={}\n",
        "epsilon = 0.1\n",
        "while(epsilon <= epsilon_end):\n",
        "\n",
        "  fgsm_counter = 0\n",
        "  pgd_counter = 0\n",
        "  print(\"Epsilon value - \",epsilon)\n",
        "  print()\n",
        "  for image_index in range(total_images):\n",
        "  \n",
        "  \n",
        "    image = x_test[image_index]\n",
        "    image = img_to_array(image)\n",
        "    image = image.reshape(1, 32, 32, 3)\n",
        "    true_value = y_test[image_index]\n",
        "    original_prediction = make_prediction(model,image,true_value)\n",
        "    fgsm_sample = fast_gradient_method(model, image, epsilon, np.inf, targeted=False)\n",
        "    pgd_sample = projected_gradient_descent(model, image, epsilon, 0.01, 40, np.inf)\n",
        "    print(epsilon,' ',image_index)\n",
        "    fgsm_prediction = make_prediction(model , fgsm_sample , true_value)\n",
        "    pgd_prediction = make_prediction(model , pgd_sample , true_value)\n",
        "    pgd_counter+=pgd_prediction\n",
        "    fgsm_counter+=fgsm_prediction\n",
        "    print(pgd_counter,' ',fgsm_counter)\n",
        "  result[epsilon] = (fgsm_counter , pgd_counter)\n",
        "  epsilon+=0.1\n"
      ],
      "metadata": {
        "id": "C1C-6c5hRQoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for key in result.keys():\n",
        "  print(key,' ',result[key])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNcSFot-RrLm",
        "outputId": "47bdda66-4f6c-4902-d983-1d1a2208cb14"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.1   (71, 71)\n",
            "0.2   (68, 66)\n",
            "0.30000000000000004   (70, 71)\n",
            "0.4   (72, 75)\n",
            "0.5   (80, 75)\n",
            "0.6   (89, 75)\n",
            "0.7   (98, 75)\n",
            "0.7999999999999999   (102, 75)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "cifar-10\n"
      ],
      "metadata": {
        "id": "1v4t73iyqf0T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.load_model(model_path)"
      ],
      "metadata": {
        "id": "M3F4f71wqnYu"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import img_to_array\n",
        "\n",
        "epsilon_end = 0.8\n",
        "#logits_model = tf.keras.Model(model.input,model.layers[-1].output)\n",
        "total_images = 200\n",
        "\n",
        "result_cifar10={}\n",
        "epsilon = 0.1\n",
        "while(epsilon <= epsilon_end):\n",
        "\n",
        "  fgsm_counter = 0\n",
        "  pgd_counter = 0\n",
        "  print(\"Epsilon value - \",epsilon)\n",
        "  print()\n",
        "  for image_index in range(total_images):\n",
        "  \n",
        "  \n",
        "    image = x_test[image_index]\n",
        "    image = img_to_array(image)\n",
        "    image = image.reshape(1, 32, 32, 3)\n",
        "    true_value = y_test[image_index]\n",
        "    original_prediction = make_prediction(model,image,true_value)\n",
        "    fgsm_sample = fast_gradient_method(model, image, epsilon, np.inf, targeted=False)\n",
        "    pgd_sample = projected_gradient_descent(model, image, epsilon, 0.01, 40, np.inf)\n",
        "    print(epsilon,' ',image_index)\n",
        "    fgsm_prediction = make_prediction(model , fgsm_sample , true_value)\n",
        "    pgd_prediction = make_prediction(model , pgd_sample , true_value)\n",
        "    pgd_counter+=pgd_prediction\n",
        "    fgsm_counter+=fgsm_prediction\n",
        "    print(pgd_counter,' ',fgsm_counter)\n",
        "  result_cifar10[epsilon] = (fgsm_counter , pgd_counter)\n",
        "  epsilon+=0.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ki0fW5YfdaSJ",
        "outputId": "9d71c785-75d3-4875-bc01-47a09c478fbc"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epsilon value -  0.1\n",
            "\n",
            "0.1   0\n",
            "0   0\n",
            "0.1   1\n",
            "0   0\n",
            "0.1   2\n",
            "0   0\n",
            "0.1   3\n",
            "0   0\n",
            "0.1   4\n",
            "0   0\n",
            "0.1   5\n",
            "0   0\n",
            "0.1   6\n",
            "0   0\n",
            "0.1   7\n",
            "0   0\n",
            "0.1   8\n",
            "0   0\n",
            "0.1   9\n",
            "0   0\n",
            "0.1   10\n",
            "0   0\n",
            "0.1   11\n",
            "0   0\n",
            "0.1   12\n",
            "0   0\n",
            "0.1   13\n",
            "0   0\n",
            "0.1   14\n",
            "0   0\n",
            "0.1   15\n",
            "0   0\n",
            "0.1   16\n",
            "0   0\n",
            "0.1   17\n",
            "0   0\n",
            "0.1   18\n",
            "0   0\n",
            "0.1   19\n",
            "0   0\n",
            "0.1   20\n",
            "0   0\n",
            "0.1   21\n",
            "0   0\n",
            "0.1   22\n",
            "1   1\n",
            "0.1   23\n",
            "1   1\n",
            "0.1   24\n",
            "2   2\n",
            "0.1   25\n",
            "2   2\n",
            "0.1   26\n",
            "2   2\n",
            "0.1   27\n",
            "2   2\n",
            "0.1   28\n",
            "2   2\n",
            "0.1   29\n",
            "2   2\n",
            "0.1   30\n",
            "2   2\n",
            "0.1   31\n",
            "2   2\n",
            "0.1   32\n",
            "2   2\n",
            "0.1   33\n",
            "2   2\n",
            "0.1   34\n",
            "2   2\n",
            "0.1   35\n",
            "2   2\n",
            "0.1   36\n",
            "2   2\n",
            "0.1   37\n",
            "3   3\n",
            "0.1   38\n",
            "3   3\n",
            "0.1   39\n",
            "3   3\n",
            "0.1   40\n",
            "3   3\n",
            "0.1   41\n",
            "3   3\n",
            "0.1   42\n",
            "3   3\n",
            "0.1   43\n",
            "3   3\n",
            "0.1   44\n",
            "3   3\n",
            "0.1   45\n",
            "3   3\n",
            "0.1   46\n",
            "3   3\n",
            "0.1   47\n",
            "4   4\n",
            "0.1   48\n",
            "4   4\n",
            "0.1   49\n",
            "4   4\n",
            "0.1   50\n",
            "4   4\n",
            "0.1   51\n",
            "4   4\n",
            "0.1   52\n",
            "5   5\n",
            "0.1   53\n",
            "5   5\n",
            "0.1   54\n",
            "5   5\n",
            "0.1   55\n",
            "5   5\n",
            "0.1   56\n",
            "5   5\n",
            "0.1   57\n",
            "6   6\n",
            "0.1   58\n",
            "7   7\n",
            "0.1   59\n",
            "8   8\n",
            "0.1   60\n",
            "8   8\n",
            "0.1   61\n",
            "9   9\n",
            "0.1   62\n",
            "9   9\n",
            "0.1   63\n",
            "9   9\n",
            "0.1   64\n",
            "9   9\n",
            "0.1   65\n",
            "9   9\n",
            "0.1   66\n",
            "9   9\n",
            "0.1   67\n",
            "9   9\n",
            "0.1   68\n",
            "10   10\n",
            "0.1   69\n",
            "10   10\n",
            "0.1   70\n",
            "10   10\n",
            "0.1   71\n",
            "10   10\n",
            "0.1   72\n",
            "10   10\n",
            "0.1   73\n",
            "10   10\n",
            "0.1   74\n",
            "10   10\n",
            "0.1   75\n",
            "10   10\n",
            "0.1   76\n",
            "10   10\n",
            "0.1   77\n",
            "10   10\n",
            "0.1   78\n",
            "11   11\n",
            "0.1   79\n",
            "11   11\n",
            "0.1   80\n",
            "11   11\n",
            "0.1   81\n",
            "11   11\n",
            "0.1   82\n",
            "11   11\n",
            "0.1   83\n",
            "11   11\n",
            "0.1   84\n",
            "11   11\n",
            "0.1   85\n",
            "12   12\n",
            "0.1   86\n",
            "13   13\n",
            "0.1   87\n",
            "13   13\n",
            "0.1   88\n",
            "13   13\n",
            "0.1   89\n",
            "13   13\n",
            "0.1   90\n",
            "13   13\n",
            "0.1   91\n",
            "13   13\n",
            "0.1   92\n",
            "13   13\n",
            "0.1   93\n",
            "13   13\n",
            "0.1   94\n",
            "13   13\n",
            "0.1   95\n",
            "13   13\n",
            "0.1   96\n",
            "13   13\n",
            "0.1   97\n",
            "13   13\n",
            "0.1   98\n",
            "13   13\n",
            "0.1   99\n",
            "13   13\n",
            "0.1   100\n",
            "13   13\n",
            "0.1   101\n",
            "13   13\n",
            "0.1   102\n",
            "13   13\n",
            "0.1   103\n",
            "14   14\n",
            "0.1   104\n",
            "14   14\n",
            "0.1   105\n",
            "14   14\n",
            "0.1   106\n",
            "15   15\n",
            "0.1   107\n",
            "15   15\n",
            "0.1   108\n",
            "15   15\n",
            "0.1   109\n",
            "15   15\n",
            "0.1   110\n",
            "15   15\n",
            "0.1   111\n",
            "15   15\n",
            "0.1   112\n",
            "15   15\n",
            "0.1   113\n",
            "15   15\n",
            "0.1   114\n",
            "15   15\n",
            "0.1   115\n",
            "15   15\n",
            "0.1   116\n",
            "15   15\n",
            "0.1   117\n",
            "15   15\n",
            "0.1   118\n",
            "15   15\n",
            "0.1   119\n",
            "15   15\n",
            "0.1   120\n",
            "15   15\n",
            "0.1   121\n",
            "16   16\n",
            "0.1   122\n",
            "16   16\n",
            "0.1   123\n",
            "16   16\n",
            "0.1   124\n",
            "16   16\n",
            "0.1   125\n",
            "16   16\n",
            "0.1   126\n",
            "16   16\n",
            "0.1   127\n",
            "16   16\n",
            "0.1   128\n",
            "17   17\n",
            "0.1   129\n",
            "17   17\n",
            "0.1   130\n",
            "17   17\n",
            "0.1   131\n",
            "17   17\n",
            "0.1   132\n",
            "17   17\n",
            "0.1   133\n",
            "17   17\n",
            "0.1   134\n",
            "17   17\n",
            "0.1   135\n",
            "17   17\n",
            "0.1   136\n",
            "17   17\n",
            "0.1   137\n",
            "17   17\n",
            "0.1   138\n",
            "17   17\n",
            "0.1   139\n",
            "18   18\n",
            "0.1   140\n",
            "18   18\n",
            "0.1   141\n",
            "18   18\n",
            "0.1   142\n",
            "18   18\n",
            "0.1   143\n",
            "18   18\n",
            "0.1   144\n",
            "18   18\n",
            "0.1   145\n",
            "19   19\n",
            "0.1   146\n",
            "19   19\n",
            "0.1   147\n",
            "20   20\n",
            "0.1   148\n",
            "20   20\n",
            "0.1   149\n",
            "20   20\n",
            "0.1   150\n",
            "21   21\n",
            "0.1   151\n",
            "21   21\n",
            "0.1   152\n",
            "21   21\n",
            "0.1   153\n",
            "21   21\n",
            "0.1   154\n",
            "21   21\n",
            "0.1   155\n",
            "21   21\n",
            "0.1   156\n",
            "21   21\n",
            "0.1   157\n",
            "21   21\n",
            "0.1   158\n",
            "22   22\n",
            "0.1   159\n",
            "22   22\n",
            "0.1   160\n",
            "22   22\n",
            "0.1   161\n",
            "22   22\n",
            "0.1   162\n",
            "22   22\n",
            "0.1   163\n",
            "22   22\n",
            "0.1   164\n",
            "23   23\n",
            "0.1   165\n",
            "24   24\n",
            "0.1   166\n",
            "24   24\n",
            "0.1   167\n",
            "24   24\n",
            "0.1   168\n",
            "24   24\n",
            "0.1   169\n",
            "25   25\n",
            "0.1   170\n",
            "25   25\n",
            "0.1   171\n",
            "26   26\n",
            "0.1   172\n",
            "26   26\n",
            "0.1   173\n",
            "26   26\n",
            "0.1   174\n",
            "26   26\n",
            "0.1   175\n",
            "26   26\n",
            "0.1   176\n",
            "26   26\n",
            "0.1   177\n",
            "26   26\n",
            "0.1   178\n",
            "26   26\n",
            "0.1   179\n",
            "26   26\n",
            "0.1   180\n",
            "26   26\n",
            "0.1   181\n",
            "26   26\n",
            "0.1   182\n",
            "26   26\n",
            "0.1   183\n",
            "26   26\n",
            "0.1   184\n",
            "26   26\n",
            "0.1   185\n",
            "26   26\n",
            "0.1   186\n",
            "26   26\n",
            "0.1   187\n",
            "27   27\n",
            "0.1   188\n",
            "28   28\n",
            "0.1   189\n",
            "28   28\n",
            "0.1   190\n",
            "28   28\n",
            "0.1   191\n",
            "28   28\n",
            "0.1   192\n",
            "28   28\n",
            "0.1   193\n",
            "28   28\n",
            "0.1   194\n",
            "28   28\n",
            "0.1   195\n",
            "28   28\n",
            "0.1   196\n",
            "28   28\n",
            "0.1   197\n",
            "28   28\n",
            "0.1   198\n",
            "28   28\n",
            "0.1   199\n",
            "28   28\n",
            "Epsilon value -  0.2\n",
            "\n",
            "0.2   0\n",
            "0   0\n",
            "0.2   1\n",
            "0   0\n",
            "0.2   2\n",
            "0   0\n",
            "0.2   3\n",
            "0   0\n",
            "0.2   4\n",
            "0   0\n",
            "0.2   5\n",
            "0   0\n",
            "0.2   6\n",
            "0   0\n",
            "0.2   7\n",
            "0   0\n",
            "0.2   8\n",
            "0   0\n",
            "0.2   9\n",
            "0   0\n",
            "0.2   10\n",
            "0   0\n",
            "0.2   11\n",
            "0   0\n",
            "0.2   12\n",
            "0   0\n",
            "0.2   13\n",
            "0   0\n",
            "0.2   14\n",
            "0   0\n",
            "0.2   15\n",
            "1   1\n",
            "0.2   16\n",
            "1   1\n",
            "0.2   17\n",
            "1   1\n",
            "0.2   18\n",
            "1   1\n",
            "0.2   19\n",
            "1   1\n",
            "0.2   20\n",
            "1   1\n",
            "0.2   21\n",
            "2   2\n",
            "0.2   22\n",
            "3   3\n",
            "0.2   23\n",
            "3   3\n",
            "0.2   24\n",
            "3   3\n",
            "0.2   25\n",
            "3   3\n",
            "0.2   26\n",
            "3   3\n",
            "0.2   27\n",
            "3   3\n",
            "0.2   28\n",
            "3   3\n",
            "0.2   29\n",
            "3   3\n",
            "0.2   30\n",
            "3   3\n",
            "0.2   31\n",
            "3   3\n",
            "0.2   32\n",
            "4   4\n",
            "0.2   33\n",
            "4   4\n",
            "0.2   34\n",
            "4   4\n",
            "0.2   35\n",
            "4   4\n",
            "0.2   36\n",
            "4   4\n",
            "0.2   37\n",
            "5   5\n",
            "0.2   38\n",
            "5   5\n",
            "0.2   39\n",
            "5   5\n",
            "0.2   40\n",
            "5   5\n",
            "0.2   41\n",
            "5   5\n",
            "0.2   42\n",
            "5   5\n",
            "0.2   43\n",
            "5   5\n",
            "0.2   44\n",
            "5   5\n",
            "0.2   45\n",
            "5   5\n",
            "0.2   46\n",
            "5   5\n",
            "0.2   47\n",
            "6   6\n",
            "0.2   48\n",
            "6   6\n",
            "0.2   49\n",
            "6   6\n",
            "0.2   50\n",
            "6   6\n",
            "0.2   51\n",
            "6   6\n",
            "0.2   52\n",
            "7   7\n",
            "0.2   53\n",
            "7   7\n",
            "0.2   54\n",
            "7   7\n",
            "0.2   55\n",
            "7   7\n",
            "0.2   56\n",
            "7   7\n",
            "0.2   57\n",
            "8   8\n",
            "0.2   58\n",
            "9   9\n",
            "0.2   59\n",
            "9   9\n",
            "0.2   60\n",
            "9   9\n",
            "0.2   61\n",
            "10   10\n",
            "0.2   62\n",
            "10   10\n",
            "0.2   63\n",
            "11   11\n",
            "0.2   64\n",
            "11   11\n",
            "0.2   65\n",
            "11   11\n",
            "0.2   66\n",
            "11   11\n",
            "0.2   67\n",
            "11   11\n",
            "0.2   68\n",
            "12   12\n",
            "0.2   69\n",
            "12   12\n",
            "0.2   70\n",
            "12   12\n",
            "0.2   71\n",
            "12   12\n",
            "0.2   72\n",
            "12   12\n",
            "0.2   73\n",
            "12   12\n",
            "0.2   74\n",
            "12   12\n",
            "0.2   75\n",
            "12   12\n",
            "0.2   76\n",
            "12   12\n",
            "0.2   77\n",
            "12   12\n",
            "0.2   78\n",
            "13   13\n",
            "0.2   79\n",
            "13   13\n",
            "0.2   80\n",
            "13   13\n",
            "0.2   81\n",
            "13   13\n",
            "0.2   82\n",
            "13   13\n",
            "0.2   83\n",
            "13   13\n",
            "0.2   84\n",
            "13   13\n",
            "0.2   85\n",
            "14   14\n",
            "0.2   86\n",
            "15   15\n",
            "0.2   87\n",
            "16   16\n",
            "0.2   88\n",
            "16   16\n",
            "0.2   89\n",
            "16   16\n",
            "0.2   90\n",
            "16   16\n",
            "0.2   91\n",
            "16   16\n",
            "0.2   92\n",
            "16   16\n",
            "0.2   93\n",
            "16   16\n",
            "0.2   94\n",
            "16   16\n",
            "0.2   95\n",
            "16   16\n",
            "0.2   96\n",
            "16   16\n",
            "0.2   97\n",
            "16   16\n",
            "0.2   98\n",
            "16   16\n",
            "0.2   99\n",
            "16   16\n",
            "0.2   100\n",
            "16   16\n",
            "0.2   101\n",
            "16   16\n",
            "0.2   102\n",
            "16   16\n",
            "0.2   103\n",
            "17   17\n",
            "0.2   104\n",
            "17   17\n",
            "0.2   105\n",
            "17   17\n",
            "0.2   106\n",
            "18   18\n",
            "0.2   107\n",
            "18   18\n",
            "0.2   108\n",
            "18   18\n",
            "0.2   109\n",
            "18   18\n",
            "0.2   110\n",
            "18   18\n",
            "0.2   111\n",
            "18   18\n",
            "0.2   112\n",
            "18   18\n",
            "0.2   113\n",
            "18   18\n",
            "0.2   114\n",
            "18   18\n",
            "0.2   115\n",
            "18   18\n",
            "0.2   116\n",
            "18   18\n",
            "0.2   117\n",
            "18   18\n",
            "0.2   118\n",
            "19   18\n",
            "0.2   119\n",
            "20   19\n",
            "0.2   120\n",
            "20   19\n",
            "0.2   121\n",
            "21   20\n",
            "0.2   122\n",
            "21   20\n",
            "0.2   123\n",
            "21   20\n",
            "0.2   124\n",
            "21   20\n",
            "0.2   125\n",
            "21   20\n",
            "0.2   126\n",
            "21   20\n",
            "0.2   127\n",
            "21   20\n",
            "0.2   128\n",
            "22   21\n",
            "0.2   129\n",
            "22   21\n",
            "0.2   130\n",
            "22   21\n",
            "0.2   131\n",
            "22   21\n",
            "0.2   132\n",
            "22   21\n",
            "0.2   133\n",
            "22   21\n",
            "0.2   134\n",
            "22   21\n",
            "0.2   135\n",
            "22   21\n",
            "0.2   136\n",
            "22   21\n",
            "0.2   137\n",
            "22   21\n",
            "0.2   138\n",
            "22   21\n",
            "0.2   139\n",
            "23   22\n",
            "0.2   140\n",
            "23   22\n",
            "0.2   141\n",
            "23   22\n",
            "0.2   142\n",
            "23   22\n",
            "0.2   143\n",
            "23   22\n",
            "0.2   144\n",
            "23   22\n",
            "0.2   145\n",
            "23   22\n",
            "0.2   146\n",
            "23   22\n",
            "0.2   147\n",
            "23   22\n",
            "0.2   148\n",
            "23   22\n",
            "0.2   149\n",
            "23   22\n",
            "0.2   150\n",
            "24   23\n",
            "0.2   151\n",
            "25   23\n",
            "0.2   152\n",
            "25   23\n",
            "0.2   153\n",
            "25   23\n",
            "0.2   154\n",
            "25   23\n",
            "0.2   155\n",
            "25   23\n",
            "0.2   156\n",
            "25   23\n",
            "0.2   157\n",
            "25   23\n",
            "0.2   158\n",
            "26   24\n",
            "0.2   159\n",
            "26   24\n",
            "0.2   160\n",
            "26   24\n",
            "0.2   161\n",
            "26   24\n",
            "0.2   162\n",
            "26   24\n",
            "0.2   163\n",
            "26   24\n",
            "0.2   164\n",
            "27   25\n",
            "0.2   165\n",
            "28   26\n",
            "0.2   166\n",
            "28   26\n",
            "0.2   167\n",
            "28   26\n",
            "0.2   168\n",
            "28   26\n",
            "0.2   169\n",
            "29   27\n",
            "0.2   170\n",
            "29   27\n",
            "0.2   171\n",
            "30   28\n",
            "0.2   172\n",
            "30   28\n",
            "0.2   173\n",
            "30   28\n",
            "0.2   174\n",
            "30   28\n",
            "0.2   175\n",
            "30   28\n",
            "0.2   176\n",
            "30   28\n",
            "0.2   177\n",
            "30   28\n",
            "0.2   178\n",
            "31   29\n",
            "0.2   179\n",
            "31   29\n",
            "0.2   180\n",
            "31   29\n",
            "0.2   181\n",
            "31   29\n",
            "0.2   182\n",
            "31   29\n",
            "0.2   183\n",
            "31   29\n",
            "0.2   184\n",
            "31   29\n",
            "0.2   185\n",
            "31   29\n",
            "0.2   186\n",
            "31   29\n",
            "0.2   187\n",
            "32   30\n",
            "0.2   188\n",
            "33   31\n",
            "0.2   189\n",
            "33   31\n",
            "0.2   190\n",
            "33   31\n",
            "0.2   191\n",
            "33   31\n",
            "0.2   192\n",
            "33   31\n",
            "0.2   193\n",
            "33   31\n",
            "0.2   194\n",
            "33   31\n",
            "0.2   195\n",
            "33   31\n",
            "0.2   196\n",
            "33   31\n",
            "0.2   197\n",
            "33   31\n",
            "0.2   198\n",
            "33   31\n",
            "0.2   199\n",
            "33   31\n",
            "Epsilon value -  0.30000000000000004\n",
            "\n",
            "0.30000000000000004   0\n",
            "0   0\n",
            "0.30000000000000004   1\n",
            "0   0\n",
            "0.30000000000000004   2\n",
            "0   0\n",
            "0.30000000000000004   3\n",
            "0   0\n",
            "0.30000000000000004   4\n",
            "0   0\n",
            "0.30000000000000004   5\n",
            "0   0\n",
            "0.30000000000000004   6\n",
            "0   0\n",
            "0.30000000000000004   7\n",
            "0   0\n",
            "0.30000000000000004   8\n",
            "0   0\n",
            "0.30000000000000004   9\n",
            "0   0\n",
            "0.30000000000000004   10\n",
            "0   0\n",
            "0.30000000000000004   11\n",
            "0   0\n",
            "0.30000000000000004   12\n",
            "0   0\n",
            "0.30000000000000004   13\n",
            "0   0\n",
            "0.30000000000000004   14\n",
            "0   0\n",
            "0.30000000000000004   15\n",
            "1   1\n",
            "0.30000000000000004   16\n",
            "1   1\n",
            "0.30000000000000004   17\n",
            "1   1\n",
            "0.30000000000000004   18\n",
            "1   1\n",
            "0.30000000000000004   19\n",
            "1   1\n",
            "0.30000000000000004   20\n",
            "1   1\n",
            "0.30000000000000004   21\n",
            "2   2\n",
            "0.30000000000000004   22\n",
            "3   3\n",
            "0.30000000000000004   23\n",
            "3   3\n",
            "0.30000000000000004   24\n",
            "3   3\n",
            "0.30000000000000004   25\n",
            "3   3\n",
            "0.30000000000000004   26\n",
            "3   3\n",
            "0.30000000000000004   27\n",
            "3   3\n",
            "0.30000000000000004   28\n",
            "3   3\n",
            "0.30000000000000004   29\n",
            "3   3\n",
            "0.30000000000000004   30\n",
            "3   3\n",
            "0.30000000000000004   31\n",
            "3   3\n",
            "0.30000000000000004   32\n",
            "4   4\n",
            "0.30000000000000004   33\n",
            "4   4\n",
            "0.30000000000000004   34\n",
            "4   4\n",
            "0.30000000000000004   35\n",
            "5   5\n",
            "0.30000000000000004   36\n",
            "5   5\n",
            "0.30000000000000004   37\n",
            "6   6\n",
            "0.30000000000000004   38\n",
            "6   6\n",
            "0.30000000000000004   39\n",
            "6   6\n",
            "0.30000000000000004   40\n",
            "6   6\n",
            "0.30000000000000004   41\n",
            "6   6\n",
            "0.30000000000000004   42\n",
            "6   6\n",
            "0.30000000000000004   43\n",
            "6   6\n",
            "0.30000000000000004   44\n",
            "6   6\n",
            "0.30000000000000004   45\n",
            "6   6\n",
            "0.30000000000000004   46\n",
            "6   6\n",
            "0.30000000000000004   47\n",
            "7   7\n",
            "0.30000000000000004   48\n",
            "7   7\n",
            "0.30000000000000004   49\n",
            "7   7\n",
            "0.30000000000000004   50\n",
            "7   7\n",
            "0.30000000000000004   51\n",
            "7   7\n",
            "0.30000000000000004   52\n",
            "7   7\n",
            "0.30000000000000004   53\n",
            "7   7\n",
            "0.30000000000000004   54\n",
            "7   7\n",
            "0.30000000000000004   55\n",
            "7   7\n",
            "0.30000000000000004   56\n",
            "7   7\n",
            "0.30000000000000004   57\n",
            "8   8\n",
            "0.30000000000000004   58\n",
            "9   9\n",
            "0.30000000000000004   59\n",
            "9   9\n",
            "0.30000000000000004   60\n",
            "9   9\n",
            "0.30000000000000004   61\n",
            "10   10\n",
            "0.30000000000000004   62\n",
            "10   10\n",
            "0.30000000000000004   63\n",
            "11   11\n",
            "0.30000000000000004   64\n",
            "11   11\n",
            "0.30000000000000004   65\n",
            "11   11\n",
            "0.30000000000000004   66\n",
            "11   11\n",
            "0.30000000000000004   67\n",
            "11   11\n",
            "0.30000000000000004   68\n",
            "12   12\n",
            "0.30000000000000004   69\n",
            "13   12\n",
            "0.30000000000000004   70\n",
            "14   12\n",
            "0.30000000000000004   71\n",
            "14   12\n",
            "0.30000000000000004   72\n",
            "14   12\n",
            "0.30000000000000004   73\n",
            "14   12\n",
            "0.30000000000000004   74\n",
            "14   12\n",
            "0.30000000000000004   75\n",
            "14   12\n",
            "0.30000000000000004   76\n",
            "14   12\n",
            "0.30000000000000004   77\n",
            "14   12\n",
            "0.30000000000000004   78\n",
            "15   13\n",
            "0.30000000000000004   79\n",
            "15   13\n",
            "0.30000000000000004   80\n",
            "15   13\n",
            "0.30000000000000004   81\n",
            "15   13\n",
            "0.30000000000000004   82\n",
            "15   13\n",
            "0.30000000000000004   83\n",
            "15   13\n",
            "0.30000000000000004   84\n",
            "15   13\n",
            "0.30000000000000004   85\n",
            "15   13\n",
            "0.30000000000000004   86\n",
            "16   14\n",
            "0.30000000000000004   87\n",
            "17   15\n",
            "0.30000000000000004   88\n",
            "17   15\n",
            "0.30000000000000004   89\n",
            "17   15\n",
            "0.30000000000000004   90\n",
            "17   15\n",
            "0.30000000000000004   91\n",
            "17   15\n",
            "0.30000000000000004   92\n",
            "17   15\n",
            "0.30000000000000004   93\n",
            "17   15\n",
            "0.30000000000000004   94\n",
            "17   15\n",
            "0.30000000000000004   95\n",
            "17   15\n",
            "0.30000000000000004   96\n",
            "17   15\n",
            "0.30000000000000004   97\n",
            "17   15\n",
            "0.30000000000000004   98\n",
            "17   15\n",
            "0.30000000000000004   99\n",
            "17   15\n",
            "0.30000000000000004   100\n",
            "17   15\n",
            "0.30000000000000004   101\n",
            "17   15\n",
            "0.30000000000000004   102\n",
            "17   15\n",
            "0.30000000000000004   103\n",
            "17   16\n",
            "0.30000000000000004   104\n",
            "17   16\n",
            "0.30000000000000004   105\n",
            "17   16\n",
            "0.30000000000000004   106\n",
            "18   17\n",
            "0.30000000000000004   107\n",
            "18   17\n",
            "0.30000000000000004   108\n",
            "18   17\n",
            "0.30000000000000004   109\n",
            "18   17\n",
            "0.30000000000000004   110\n",
            "18   17\n",
            "0.30000000000000004   111\n",
            "18   17\n",
            "0.30000000000000004   112\n",
            "18   17\n",
            "0.30000000000000004   113\n",
            "18   17\n",
            "0.30000000000000004   114\n",
            "18   17\n",
            "0.30000000000000004   115\n",
            "18   17\n",
            "0.30000000000000004   116\n",
            "18   17\n",
            "0.30000000000000004   117\n",
            "18   17\n",
            "0.30000000000000004   118\n",
            "19   18\n",
            "0.30000000000000004   119\n",
            "20   19\n",
            "0.30000000000000004   120\n",
            "20   19\n",
            "0.30000000000000004   121\n",
            "21   20\n",
            "0.30000000000000004   122\n",
            "21   20\n",
            "0.30000000000000004   123\n",
            "21   20\n",
            "0.30000000000000004   124\n",
            "21   20\n",
            "0.30000000000000004   125\n",
            "22   21\n",
            "0.30000000000000004   126\n",
            "22   21\n",
            "0.30000000000000004   127\n",
            "22   21\n",
            "0.30000000000000004   128\n",
            "23   22\n",
            "0.30000000000000004   129\n",
            "23   22\n",
            "0.30000000000000004   130\n",
            "23   22\n",
            "0.30000000000000004   131\n",
            "23   22\n",
            "0.30000000000000004   132\n",
            "23   22\n",
            "0.30000000000000004   133\n",
            "23   22\n",
            "0.30000000000000004   134\n",
            "23   22\n",
            "0.30000000000000004   135\n",
            "24   23\n",
            "0.30000000000000004   136\n",
            "24   23\n",
            "0.30000000000000004   137\n",
            "24   23\n",
            "0.30000000000000004   138\n",
            "24   23\n",
            "0.30000000000000004   139\n",
            "24   23\n",
            "0.30000000000000004   140\n",
            "24   23\n",
            "0.30000000000000004   141\n",
            "24   23\n",
            "0.30000000000000004   142\n",
            "24   23\n",
            "0.30000000000000004   143\n",
            "24   23\n",
            "0.30000000000000004   144\n",
            "24   23\n",
            "0.30000000000000004   145\n",
            "24   23\n",
            "0.30000000000000004   146\n",
            "24   23\n",
            "0.30000000000000004   147\n",
            "24   23\n",
            "0.30000000000000004   148\n",
            "24   23\n",
            "0.30000000000000004   149\n",
            "24   23\n",
            "0.30000000000000004   150\n",
            "25   24\n",
            "0.30000000000000004   151\n",
            "26   25\n",
            "0.30000000000000004   152\n",
            "26   25\n",
            "0.30000000000000004   153\n",
            "26   25\n",
            "0.30000000000000004   154\n",
            "26   25\n",
            "0.30000000000000004   155\n",
            "26   25\n",
            "0.30000000000000004   156\n",
            "26   25\n",
            "0.30000000000000004   157\n",
            "26   25\n",
            "0.30000000000000004   158\n",
            "27   26\n",
            "0.30000000000000004   159\n",
            "27   26\n",
            "0.30000000000000004   160\n",
            "27   26\n",
            "0.30000000000000004   161\n",
            "27   26\n",
            "0.30000000000000004   162\n",
            "27   26\n",
            "0.30000000000000004   163\n",
            "27   26\n",
            "0.30000000000000004   164\n",
            "28   27\n",
            "0.30000000000000004   165\n",
            "29   28\n",
            "0.30000000000000004   166\n",
            "29   28\n",
            "0.30000000000000004   167\n",
            "29   28\n",
            "0.30000000000000004   168\n",
            "29   28\n",
            "0.30000000000000004   169\n",
            "30   29\n",
            "0.30000000000000004   170\n",
            "31   29\n",
            "0.30000000000000004   171\n",
            "32   30\n",
            "0.30000000000000004   172\n",
            "32   30\n",
            "0.30000000000000004   173\n",
            "32   30\n",
            "0.30000000000000004   174\n",
            "32   30\n",
            "0.30000000000000004   175\n",
            "32   30\n",
            "0.30000000000000004   176\n",
            "32   30\n",
            "0.30000000000000004   177\n",
            "32   30\n",
            "0.30000000000000004   178\n",
            "33   31\n",
            "0.30000000000000004   179\n",
            "33   31\n",
            "0.30000000000000004   180\n",
            "33   31\n",
            "0.30000000000000004   181\n",
            "33   31\n",
            "0.30000000000000004   182\n",
            "33   31\n",
            "0.30000000000000004   183\n",
            "33   31\n",
            "0.30000000000000004   184\n",
            "33   31\n",
            "0.30000000000000004   185\n",
            "33   31\n",
            "0.30000000000000004   186\n",
            "33   31\n",
            "0.30000000000000004   187\n",
            "34   32\n",
            "0.30000000000000004   188\n",
            "35   33\n",
            "0.30000000000000004   189\n",
            "35   33\n",
            "0.30000000000000004   190\n",
            "35   33\n",
            "0.30000000000000004   191\n",
            "35   33\n",
            "0.30000000000000004   192\n",
            "35   33\n",
            "0.30000000000000004   193\n",
            "35   33\n",
            "0.30000000000000004   194\n",
            "35   33\n",
            "0.30000000000000004   195\n",
            "36   33\n",
            "0.30000000000000004   196\n",
            "36   33\n",
            "0.30000000000000004   197\n",
            "36   33\n",
            "0.30000000000000004   198\n",
            "36   33\n",
            "0.30000000000000004   199\n",
            "36   33\n",
            "Epsilon value -  0.4\n",
            "\n",
            "0.4   0\n",
            "0   0\n",
            "0.4   1\n",
            "0   0\n",
            "0.4   2\n",
            "0   0\n",
            "0.4   3\n",
            "0   0\n",
            "0.4   4\n",
            "0   0\n",
            "0.4   5\n",
            "0   0\n",
            "0.4   6\n",
            "0   0\n",
            "0.4   7\n",
            "0   0\n",
            "0.4   8\n",
            "0   0\n",
            "0.4   9\n",
            "0   0\n",
            "0.4   10\n",
            "0   0\n",
            "0.4   11\n",
            "0   0\n",
            "0.4   12\n",
            "0   0\n",
            "0.4   13\n",
            "0   0\n",
            "0.4   14\n",
            "0   0\n",
            "0.4   15\n",
            "1   1\n",
            "0.4   16\n",
            "1   1\n",
            "0.4   17\n",
            "1   1\n",
            "0.4   18\n",
            "1   1\n",
            "0.4   19\n",
            "1   1\n",
            "0.4   20\n",
            "1   1\n",
            "0.4   21\n",
            "2   2\n",
            "0.4   22\n",
            "3   3\n",
            "0.4   23\n",
            "3   3\n",
            "0.4   24\n",
            "3   3\n",
            "0.4   25\n",
            "3   3\n",
            "0.4   26\n",
            "3   3\n",
            "0.4   27\n",
            "3   3\n",
            "0.4   28\n",
            "3   3\n",
            "0.4   29\n",
            "3   3\n",
            "0.4   30\n",
            "3   3\n",
            "0.4   31\n",
            "3   3\n",
            "0.4   32\n",
            "4   4\n",
            "0.4   33\n",
            "4   4\n",
            "0.4   34\n",
            "4   4\n",
            "0.4   35\n",
            "5   5\n",
            "0.4   36\n",
            "5   5\n",
            "0.4   37\n",
            "6   6\n",
            "0.4   38\n",
            "6   6\n",
            "0.4   39\n",
            "6   6\n",
            "0.4   40\n",
            "6   6\n",
            "0.4   41\n",
            "6   6\n",
            "0.4   42\n",
            "6   6\n",
            "0.4   43\n",
            "6   6\n",
            "0.4   44\n",
            "6   6\n",
            "0.4   45\n",
            "6   6\n",
            "0.4   46\n",
            "6   6\n",
            "0.4   47\n",
            "7   7\n",
            "0.4   48\n",
            "7   7\n",
            "0.4   49\n",
            "8   8\n",
            "0.4   50\n",
            "8   8\n",
            "0.4   51\n",
            "8   8\n",
            "0.4   52\n",
            "8   8\n",
            "0.4   53\n",
            "8   8\n",
            "0.4   54\n",
            "8   8\n",
            "0.4   55\n",
            "8   8\n",
            "0.4   56\n",
            "8   8\n",
            "0.4   57\n",
            "9   9\n",
            "0.4   58\n",
            "10   10\n",
            "0.4   59\n",
            "10   10\n",
            "0.4   60\n",
            "10   10\n",
            "0.4   61\n",
            "11   11\n",
            "0.4   62\n",
            "11   11\n",
            "0.4   63\n",
            "12   12\n",
            "0.4   64\n",
            "12   12\n",
            "0.4   65\n",
            "12   12\n",
            "0.4   66\n",
            "12   12\n",
            "0.4   67\n",
            "12   12\n",
            "0.4   68\n",
            "13   13\n",
            "0.4   69\n",
            "14   14\n",
            "0.4   70\n",
            "15   15\n",
            "0.4   71\n",
            "15   15\n",
            "0.4   72\n",
            "15   15\n",
            "0.4   73\n",
            "15   15\n",
            "0.4   74\n",
            "15   15\n",
            "0.4   75\n",
            "15   15\n",
            "0.4   76\n",
            "15   15\n",
            "0.4   77\n",
            "15   15\n",
            "0.4   78\n",
            "16   16\n",
            "0.4   79\n",
            "16   16\n",
            "0.4   80\n",
            "16   16\n",
            "0.4   81\n",
            "16   16\n",
            "0.4   82\n",
            "16   16\n",
            "0.4   83\n",
            "16   16\n",
            "0.4   84\n",
            "16   16\n",
            "0.4   85\n",
            "16   16\n",
            "0.4   86\n",
            "17   17\n",
            "0.4   87\n",
            "18   18\n",
            "0.4   88\n",
            "18   18\n",
            "0.4   89\n",
            "18   18\n",
            "0.4   90\n",
            "18   18\n",
            "0.4   91\n",
            "19   19\n",
            "0.4   92\n",
            "19   19\n",
            "0.4   93\n",
            "19   19\n",
            "0.4   94\n",
            "19   19\n",
            "0.4   95\n",
            "19   19\n",
            "0.4   96\n",
            "19   19\n",
            "0.4   97\n",
            "19   19\n",
            "0.4   98\n",
            "19   19\n",
            "0.4   99\n",
            "19   19\n",
            "0.4   100\n",
            "19   19\n",
            "0.4   101\n",
            "20   20\n",
            "0.4   102\n",
            "20   20\n",
            "0.4   103\n",
            "20   20\n",
            "0.4   104\n",
            "20   20\n",
            "0.4   105\n",
            "20   20\n",
            "0.4   106\n",
            "21   21\n",
            "0.4   107\n",
            "21   21\n",
            "0.4   108\n",
            "21   21\n",
            "0.4   109\n",
            "21   21\n",
            "0.4   110\n",
            "21   21\n",
            "0.4   111\n",
            "21   21\n",
            "0.4   112\n",
            "21   21\n",
            "0.4   113\n",
            "21   21\n",
            "0.4   114\n",
            "21   21\n",
            "0.4   115\n",
            "21   21\n",
            "0.4   116\n",
            "21   21\n",
            "0.4   117\n",
            "21   21\n",
            "0.4   118\n",
            "22   22\n",
            "0.4   119\n",
            "23   23\n",
            "0.4   120\n",
            "23   23\n",
            "0.4   121\n",
            "24   24\n",
            "0.4   122\n",
            "24   24\n",
            "0.4   123\n",
            "24   24\n",
            "0.4   124\n",
            "24   24\n",
            "0.4   125\n",
            "25   25\n",
            "0.4   126\n",
            "25   25\n",
            "0.4   127\n",
            "26   26\n",
            "0.4   128\n",
            "27   27\n",
            "0.4   129\n",
            "27   27\n",
            "0.4   130\n",
            "27   27\n",
            "0.4   131\n",
            "27   27\n",
            "0.4   132\n",
            "27   27\n",
            "0.4   133\n",
            "27   27\n",
            "0.4   134\n",
            "27   27\n",
            "0.4   135\n",
            "28   28\n",
            "0.4   136\n",
            "28   28\n",
            "0.4   137\n",
            "28   28\n",
            "0.4   138\n",
            "28   28\n",
            "0.4   139\n",
            "28   28\n",
            "0.4   140\n",
            "28   28\n",
            "0.4   141\n",
            "28   28\n",
            "0.4   142\n",
            "28   28\n",
            "0.4   143\n",
            "28   28\n",
            "0.4   144\n",
            "28   28\n",
            "0.4   145\n",
            "28   28\n",
            "0.4   146\n",
            "28   28\n",
            "0.4   147\n",
            "28   28\n",
            "0.4   148\n",
            "28   28\n",
            "0.4   149\n",
            "28   28\n",
            "0.4   150\n",
            "29   29\n",
            "0.4   151\n",
            "30   30\n",
            "0.4   152\n",
            "30   30\n",
            "0.4   153\n",
            "30   30\n",
            "0.4   154\n",
            "30   30\n",
            "0.4   155\n",
            "30   30\n",
            "0.4   156\n",
            "30   30\n",
            "0.4   157\n",
            "30   30\n",
            "0.4   158\n",
            "31   31\n",
            "0.4   159\n",
            "31   31\n",
            "0.4   160\n",
            "31   31\n",
            "0.4   161\n",
            "31   31\n",
            "0.4   162\n",
            "31   31\n",
            "0.4   163\n",
            "31   31\n",
            "0.4   164\n",
            "32   32\n",
            "0.4   165\n",
            "33   33\n",
            "0.4   166\n",
            "33   33\n",
            "0.4   167\n",
            "33   33\n",
            "0.4   168\n",
            "33   33\n",
            "0.4   169\n",
            "34   34\n",
            "0.4   170\n",
            "35   35\n",
            "0.4   171\n",
            "36   36\n",
            "0.4   172\n",
            "36   36\n",
            "0.4   173\n",
            "36   36\n",
            "0.4   174\n",
            "36   36\n",
            "0.4   175\n",
            "36   36\n",
            "0.4   176\n",
            "36   36\n",
            "0.4   177\n",
            "36   36\n",
            "0.4   178\n",
            "37   37\n",
            "0.4   179\n",
            "37   37\n",
            "0.4   180\n",
            "37   37\n",
            "0.4   181\n",
            "37   37\n",
            "0.4   182\n",
            "37   37\n",
            "0.4   183\n",
            "37   37\n",
            "0.4   184\n",
            "37   37\n",
            "0.4   185\n",
            "37   37\n",
            "0.4   186\n",
            "37   37\n",
            "0.4   187\n",
            "38   38\n",
            "0.4   188\n",
            "39   39\n",
            "0.4   189\n",
            "39   39\n",
            "0.4   190\n",
            "39   39\n",
            "0.4   191\n",
            "39   39\n",
            "0.4   192\n",
            "39   39\n",
            "0.4   193\n",
            "39   39\n",
            "0.4   194\n",
            "39   39\n",
            "0.4   195\n",
            "40   40\n",
            "0.4   196\n",
            "40   40\n",
            "0.4   197\n",
            "40   40\n",
            "0.4   198\n",
            "40   40\n",
            "0.4   199\n",
            "40   40\n",
            "Epsilon value -  0.5\n",
            "\n",
            "0.5   0\n",
            "0   0\n",
            "0.5   1\n",
            "0   0\n",
            "0.5   2\n",
            "0   0\n",
            "0.5   3\n",
            "0   0\n",
            "0.5   4\n",
            "0   0\n",
            "0.5   5\n",
            "0   0\n",
            "0.5   6\n",
            "0   0\n",
            "0.5   7\n",
            "0   0\n",
            "0.5   8\n",
            "0   0\n",
            "0.5   9\n",
            "0   0\n",
            "0.5   10\n",
            "0   0\n",
            "0.5   11\n",
            "0   0\n",
            "0.5   12\n",
            "0   0\n",
            "0.5   13\n",
            "0   0\n",
            "0.5   14\n",
            "0   0\n",
            "0.5   15\n",
            "1   1\n",
            "0.5   16\n",
            "1   1\n",
            "0.5   17\n",
            "1   1\n",
            "0.5   18\n",
            "1   1\n",
            "0.5   19\n",
            "1   1\n",
            "0.5   20\n",
            "1   1\n",
            "0.5   21\n",
            "2   2\n",
            "0.5   22\n",
            "3   3\n",
            "0.5   23\n",
            "3   3\n",
            "0.5   24\n",
            "3   3\n",
            "0.5   25\n",
            "3   3\n",
            "0.5   26\n",
            "3   3\n",
            "0.5   27\n",
            "3   3\n",
            "0.5   28\n",
            "3   3\n",
            "0.5   29\n",
            "3   3\n",
            "0.5   30\n",
            "3   3\n",
            "0.5   31\n",
            "3   3\n",
            "0.5   32\n",
            "4   4\n",
            "0.5   33\n",
            "4   4\n",
            "0.5   34\n",
            "4   4\n",
            "0.5   35\n",
            "5   5\n",
            "0.5   36\n",
            "5   5\n",
            "0.5   37\n",
            "6   6\n",
            "0.5   38\n",
            "6   6\n",
            "0.5   39\n",
            "6   6\n",
            "0.5   40\n",
            "6   6\n",
            "0.5   41\n",
            "6   6\n",
            "0.5   42\n",
            "6   6\n",
            "0.5   43\n",
            "6   6\n",
            "0.5   44\n",
            "6   6\n",
            "0.5   45\n",
            "6   6\n",
            "0.5   46\n",
            "6   6\n",
            "0.5   47\n",
            "7   7\n",
            "0.5   48\n",
            "7   7\n",
            "0.5   49\n",
            "8   8\n",
            "0.5   50\n",
            "8   8\n",
            "0.5   51\n",
            "8   8\n",
            "0.5   52\n",
            "8   8\n",
            "0.5   53\n",
            "8   8\n",
            "0.5   54\n",
            "8   8\n",
            "0.5   55\n",
            "8   8\n",
            "0.5   56\n",
            "8   8\n",
            "0.5   57\n",
            "9   9\n",
            "0.5   58\n",
            "10   10\n",
            "0.5   59\n",
            "10   10\n",
            "0.5   60\n",
            "10   10\n",
            "0.5   61\n",
            "11   11\n",
            "0.5   62\n",
            "11   11\n",
            "0.5   63\n",
            "12   12\n",
            "0.5   64\n",
            "12   12\n",
            "0.5   65\n",
            "12   12\n",
            "0.5   66\n",
            "12   12\n",
            "0.5   67\n",
            "12   12\n",
            "0.5   68\n",
            "13   13\n",
            "0.5   69\n",
            "14   14\n",
            "0.5   70\n",
            "15   15\n",
            "0.5   71\n",
            "15   15\n",
            "0.5   72\n",
            "15   15\n",
            "0.5   73\n",
            "15   15\n",
            "0.5   74\n",
            "15   15\n",
            "0.5   75\n",
            "15   15\n",
            "0.5   76\n",
            "15   15\n",
            "0.5   77\n",
            "15   15\n",
            "0.5   78\n",
            "16   16\n",
            "0.5   79\n",
            "16   16\n",
            "0.5   80\n",
            "16   16\n",
            "0.5   81\n",
            "16   16\n",
            "0.5   82\n",
            "16   16\n",
            "0.5   83\n",
            "16   16\n",
            "0.5   84\n",
            "16   16\n",
            "0.5   85\n",
            "16   16\n",
            "0.5   86\n",
            "17   17\n",
            "0.5   87\n",
            "18   18\n",
            "0.5   88\n",
            "18   18\n",
            "0.5   89\n",
            "18   18\n",
            "0.5   90\n",
            "18   18\n",
            "0.5   91\n",
            "19   19\n",
            "0.5   92\n",
            "19   19\n",
            "0.5   93\n",
            "19   19\n",
            "0.5   94\n",
            "19   19\n",
            "0.5   95\n",
            "19   20\n",
            "0.5   96\n",
            "19   20\n",
            "0.5   97\n",
            "19   20\n",
            "0.5   98\n",
            "19   20\n",
            "0.5   99\n",
            "19   20\n",
            "0.5   100\n",
            "19   20\n",
            "0.5   101\n",
            "20   21\n",
            "0.5   102\n",
            "20   21\n",
            "0.5   103\n",
            "20   21\n",
            "0.5   104\n",
            "20   21\n",
            "0.5   105\n",
            "20   21\n",
            "0.5   106\n",
            "21   22\n",
            "0.5   107\n",
            "21   22\n",
            "0.5   108\n",
            "21   22\n",
            "0.5   109\n",
            "21   22\n",
            "0.5   110\n",
            "21   22\n",
            "0.5   111\n",
            "21   22\n",
            "0.5   112\n",
            "21   22\n",
            "0.5   113\n",
            "21   22\n",
            "0.5   114\n",
            "21   22\n",
            "0.5   115\n",
            "21   22\n",
            "0.5   116\n",
            "21   22\n",
            "0.5   117\n",
            "21   22\n",
            "0.5   118\n",
            "22   23\n",
            "0.5   119\n",
            "23   24\n",
            "0.5   120\n",
            "23   24\n",
            "0.5   121\n",
            "24   25\n",
            "0.5   122\n",
            "24   25\n",
            "0.5   123\n",
            "24   25\n",
            "0.5   124\n",
            "24   26\n",
            "0.5   125\n",
            "25   27\n",
            "0.5   126\n",
            "25   27\n",
            "0.5   127\n",
            "26   28\n",
            "0.5   128\n",
            "27   29\n",
            "0.5   129\n",
            "27   29\n",
            "0.5   130\n",
            "27   30\n",
            "0.5   131\n",
            "27   30\n",
            "0.5   132\n",
            "27   30\n",
            "0.5   133\n",
            "27   30\n",
            "0.5   134\n",
            "27   30\n",
            "0.5   135\n",
            "28   31\n",
            "0.5   136\n",
            "28   31\n",
            "0.5   137\n",
            "28   31\n",
            "0.5   138\n",
            "28   31\n",
            "0.5   139\n",
            "28   31\n",
            "0.5   140\n",
            "28   31\n",
            "0.5   141\n",
            "28   31\n",
            "0.5   142\n",
            "28   31\n",
            "0.5   143\n",
            "28   31\n",
            "0.5   144\n",
            "28   31\n",
            "0.5   145\n",
            "28   31\n",
            "0.5   146\n",
            "28   31\n",
            "0.5   147\n",
            "28   31\n",
            "0.5   148\n",
            "28   31\n",
            "0.5   149\n",
            "28   31\n",
            "0.5   150\n",
            "29   32\n",
            "0.5   151\n",
            "30   33\n",
            "0.5   152\n",
            "30   33\n",
            "0.5   153\n",
            "30   33\n",
            "0.5   154\n",
            "30   33\n",
            "0.5   155\n",
            "30   33\n",
            "0.5   156\n",
            "30   33\n",
            "0.5   157\n",
            "30   33\n",
            "0.5   158\n",
            "31   34\n",
            "0.5   159\n",
            "31   34\n",
            "0.5   160\n",
            "31   34\n",
            "0.5   161\n",
            "31   34\n",
            "0.5   162\n",
            "31   34\n",
            "0.5   163\n",
            "31   34\n",
            "0.5   164\n",
            "32   35\n",
            "0.5   165\n",
            "33   36\n",
            "0.5   166\n",
            "33   36\n",
            "0.5   167\n",
            "33   36\n",
            "0.5   168\n",
            "33   36\n",
            "0.5   169\n",
            "34   37\n",
            "0.5   170\n",
            "35   38\n",
            "0.5   171\n",
            "36   39\n",
            "0.5   172\n",
            "36   39\n",
            "0.5   173\n",
            "36   40\n",
            "0.5   174\n",
            "36   40\n",
            "0.5   175\n",
            "36   40\n",
            "0.5   176\n",
            "36   40\n",
            "0.5   177\n",
            "36   40\n",
            "0.5   178\n",
            "37   41\n",
            "0.5   179\n",
            "37   41\n",
            "0.5   180\n",
            "37   41\n",
            "0.5   181\n",
            "37   41\n",
            "0.5   182\n",
            "37   41\n",
            "0.5   183\n",
            "37   41\n",
            "0.5   184\n",
            "37   41\n",
            "0.5   185\n",
            "37   41\n",
            "0.5   186\n",
            "37   41\n",
            "0.5   187\n",
            "38   42\n",
            "0.5   188\n",
            "39   43\n",
            "0.5   189\n",
            "39   43\n",
            "0.5   190\n",
            "39   43\n",
            "0.5   191\n",
            "39   43\n",
            "0.5   192\n",
            "39   43\n",
            "0.5   193\n",
            "39   43\n",
            "0.5   194\n",
            "39   43\n",
            "0.5   195\n",
            "40   44\n",
            "0.5   196\n",
            "40   44\n",
            "0.5   197\n",
            "40   44\n",
            "0.5   198\n",
            "40   44\n",
            "0.5   199\n",
            "40   44\n",
            "Epsilon value -  0.6\n",
            "\n",
            "0.6   0\n",
            "0   0\n",
            "0.6   1\n",
            "0   0\n",
            "0.6   2\n",
            "0   0\n",
            "0.6   3\n",
            "0   0\n",
            "0.6   4\n",
            "0   0\n",
            "0.6   5\n",
            "0   0\n",
            "0.6   6\n",
            "0   0\n",
            "0.6   7\n",
            "0   0\n",
            "0.6   8\n",
            "0   0\n",
            "0.6   9\n",
            "0   0\n",
            "0.6   10\n",
            "0   0\n",
            "0.6   11\n",
            "0   0\n",
            "0.6   12\n",
            "0   0\n",
            "0.6   13\n",
            "0   0\n",
            "0.6   14\n",
            "0   0\n",
            "0.6   15\n",
            "1   1\n",
            "0.6   16\n",
            "1   1\n",
            "0.6   17\n",
            "1   1\n",
            "0.6   18\n",
            "1   1\n",
            "0.6   19\n",
            "1   1\n",
            "0.6   20\n",
            "1   1\n",
            "0.6   21\n",
            "2   2\n",
            "0.6   22\n",
            "3   3\n",
            "0.6   23\n",
            "3   3\n",
            "0.6   24\n",
            "3   3\n",
            "0.6   25\n",
            "3   3\n",
            "0.6   26\n",
            "3   4\n",
            "0.6   27\n",
            "3   4\n",
            "0.6   28\n",
            "3   4\n",
            "0.6   29\n",
            "3   4\n",
            "0.6   30\n",
            "3   4\n",
            "0.6   31\n",
            "3   4\n",
            "0.6   32\n",
            "4   5\n",
            "0.6   33\n",
            "4   5\n",
            "0.6   34\n",
            "4   5\n",
            "0.6   35\n",
            "5   6\n",
            "0.6   36\n",
            "5   6\n",
            "0.6   37\n",
            "6   7\n",
            "0.6   38\n",
            "6   7\n",
            "0.6   39\n",
            "6   7\n",
            "0.6   40\n",
            "6   7\n",
            "0.6   41\n",
            "6   7\n",
            "0.6   42\n",
            "6   7\n",
            "0.6   43\n",
            "6   7\n",
            "0.6   44\n",
            "6   7\n",
            "0.6   45\n",
            "6   7\n",
            "0.6   46\n",
            "6   7\n",
            "0.6   47\n",
            "7   8\n",
            "0.6   48\n",
            "7   8\n",
            "0.6   49\n",
            "8   9\n",
            "0.6   50\n",
            "8   9\n",
            "0.6   51\n",
            "8   9\n",
            "0.6   52\n",
            "8   9\n",
            "0.6   53\n",
            "8   9\n",
            "0.6   54\n",
            "8   9\n",
            "0.6   55\n",
            "8   9\n",
            "0.6   56\n",
            "8   9\n",
            "0.6   57\n",
            "9   10\n",
            "0.6   58\n",
            "10   11\n",
            "0.6   59\n",
            "10   11\n",
            "0.6   60\n",
            "10   11\n",
            "0.6   61\n",
            "11   12\n",
            "0.6   62\n",
            "11   12\n",
            "0.6   63\n",
            "12   13\n",
            "0.6   64\n",
            "12   13\n",
            "0.6   65\n",
            "12   13\n",
            "0.6   66\n",
            "12   13\n",
            "0.6   67\n",
            "12   13\n",
            "0.6   68\n",
            "13   14\n",
            "0.6   69\n",
            "14   15\n",
            "0.6   70\n",
            "15   16\n",
            "0.6   71\n",
            "15   16\n",
            "0.6   72\n",
            "15   16\n",
            "0.6   73\n",
            "15   16\n",
            "0.6   74\n",
            "15   16\n",
            "0.6   75\n",
            "15   16\n",
            "0.6   76\n",
            "15   16\n",
            "0.6   77\n",
            "15   16\n",
            "0.6   78\n",
            "16   16\n",
            "0.6   79\n",
            "16   16\n",
            "0.6   80\n",
            "16   16\n",
            "0.6   81\n",
            "16   16\n",
            "0.6   82\n",
            "16   16\n",
            "0.6   83\n",
            "16   16\n",
            "0.6   84\n",
            "16   16\n",
            "0.6   85\n",
            "16   16\n",
            "0.6   86\n",
            "17   17\n",
            "0.6   87\n",
            "18   18\n",
            "0.6   88\n",
            "18   19\n",
            "0.6   89\n",
            "18   19\n",
            "0.6   90\n",
            "18   19\n",
            "0.6   91\n",
            "19   20\n",
            "0.6   92\n",
            "19   20\n",
            "0.6   93\n",
            "19   20\n",
            "0.6   94\n",
            "19   20\n",
            "0.6   95\n",
            "19   21\n",
            "0.6   96\n",
            "19   21\n",
            "0.6   97\n",
            "19   21\n",
            "0.6   98\n",
            "19   21\n",
            "0.6   99\n",
            "19   21\n",
            "0.6   100\n",
            "19   21\n",
            "0.6   101\n",
            "20   22\n",
            "0.6   102\n",
            "20   22\n",
            "0.6   103\n",
            "20   22\n",
            "0.6   104\n",
            "20   22\n",
            "0.6   105\n",
            "20   22\n",
            "0.6   106\n",
            "21   23\n",
            "0.6   107\n",
            "21   23\n",
            "0.6   108\n",
            "21   24\n",
            "0.6   109\n",
            "21   24\n",
            "0.6   110\n",
            "21   25\n",
            "0.6   111\n",
            "21   25\n",
            "0.6   112\n",
            "21   25\n",
            "0.6   113\n",
            "21   25\n",
            "0.6   114\n",
            "21   25\n",
            "0.6   115\n",
            "21   26\n",
            "0.6   116\n",
            "21   26\n",
            "0.6   117\n",
            "21   26\n",
            "0.6   118\n",
            "22   27\n",
            "0.6   119\n",
            "23   28\n",
            "0.6   120\n",
            "23   28\n",
            "0.6   121\n",
            "24   29\n",
            "0.6   122\n",
            "24   29\n",
            "0.6   123\n",
            "24   29\n",
            "0.6   124\n",
            "24   30\n",
            "0.6   125\n",
            "25   31\n",
            "0.6   126\n",
            "25   31\n",
            "0.6   127\n",
            "26   32\n",
            "0.6   128\n",
            "27   33\n",
            "0.6   129\n",
            "27   33\n",
            "0.6   130\n",
            "27   34\n",
            "0.6   131\n",
            "27   34\n",
            "0.6   132\n",
            "27   34\n",
            "0.6   133\n",
            "27   34\n",
            "0.6   134\n",
            "27   34\n",
            "0.6   135\n",
            "28   35\n",
            "0.6   136\n",
            "28   35\n",
            "0.6   137\n",
            "28   35\n",
            "0.6   138\n",
            "28   35\n",
            "0.6   139\n",
            "28   35\n",
            "0.6   140\n",
            "28   35\n",
            "0.6   141\n",
            "28   35\n",
            "0.6   142\n",
            "28   35\n",
            "0.6   143\n",
            "28   36\n",
            "0.6   144\n",
            "28   36\n",
            "0.6   145\n",
            "28   36\n",
            "0.6   146\n",
            "28   36\n",
            "0.6   147\n",
            "28   36\n",
            "0.6   148\n",
            "28   36\n",
            "0.6   149\n",
            "28   36\n",
            "0.6   150\n",
            "29   37\n",
            "0.6   151\n",
            "30   38\n",
            "0.6   152\n",
            "30   38\n",
            "0.6   153\n",
            "30   38\n",
            "0.6   154\n",
            "30   38\n",
            "0.6   155\n",
            "30   38\n",
            "0.6   156\n",
            "30   38\n",
            "0.6   157\n",
            "30   38\n",
            "0.6   158\n",
            "31   39\n",
            "0.6   159\n",
            "31   39\n",
            "0.6   160\n",
            "31   39\n",
            "0.6   161\n",
            "31   39\n",
            "0.6   162\n",
            "31   39\n",
            "0.6   163\n",
            "31   39\n",
            "0.6   164\n",
            "32   40\n",
            "0.6   165\n",
            "33   41\n",
            "0.6   166\n",
            "33   41\n",
            "0.6   167\n",
            "33   42\n",
            "0.6   168\n",
            "33   42\n",
            "0.6   169\n",
            "34   43\n",
            "0.6   170\n",
            "35   44\n",
            "0.6   171\n",
            "36   44\n",
            "0.6   172\n",
            "36   44\n",
            "0.6   173\n",
            "36   45\n",
            "0.6   174\n",
            "36   45\n",
            "0.6   175\n",
            "36   45\n",
            "0.6   176\n",
            "36   45\n",
            "0.6   177\n",
            "36   45\n",
            "0.6   178\n",
            "37   46\n",
            "0.6   179\n",
            "37   46\n",
            "0.6   180\n",
            "37   46\n",
            "0.6   181\n",
            "37   46\n",
            "0.6   182\n",
            "37   46\n",
            "0.6   183\n",
            "37   46\n",
            "0.6   184\n",
            "37   46\n",
            "0.6   185\n",
            "37   46\n",
            "0.6   186\n",
            "37   46\n",
            "0.6   187\n",
            "38   47\n",
            "0.6   188\n",
            "39   48\n",
            "0.6   189\n",
            "39   48\n",
            "0.6   190\n",
            "39   48\n",
            "0.6   191\n",
            "39   48\n",
            "0.6   192\n",
            "39   49\n",
            "0.6   193\n",
            "39   49\n",
            "0.6   194\n",
            "39   49\n",
            "0.6   195\n",
            "40   50\n",
            "0.6   196\n",
            "40   50\n",
            "0.6   197\n",
            "40   50\n",
            "0.6   198\n",
            "40   50\n",
            "0.6   199\n",
            "40   50\n",
            "Epsilon value -  0.7\n",
            "\n",
            "0.7   0\n",
            "0   0\n",
            "0.7   1\n",
            "0   0\n",
            "0.7   2\n",
            "0   0\n",
            "0.7   3\n",
            "0   0\n",
            "0.7   4\n",
            "0   0\n",
            "0.7   5\n",
            "0   1\n",
            "0.7   6\n",
            "0   1\n",
            "0.7   7\n",
            "0   1\n",
            "0.7   8\n",
            "0   1\n",
            "0.7   9\n",
            "0   1\n",
            "0.7   10\n",
            "0   1\n",
            "0.7   11\n",
            "0   1\n",
            "0.7   12\n",
            "0   1\n",
            "0.7   13\n",
            "0   1\n",
            "0.7   14\n",
            "0   1\n",
            "0.7   15\n",
            "1   2\n",
            "0.7   16\n",
            "1   2\n",
            "0.7   17\n",
            "1   3\n",
            "0.7   18\n",
            "1   3\n",
            "0.7   19\n",
            "1   3\n",
            "0.7   20\n",
            "1   4\n",
            "0.7   21\n",
            "2   5\n",
            "0.7   22\n",
            "3   6\n",
            "0.7   23\n",
            "3   6\n",
            "0.7   24\n",
            "3   6\n",
            "0.7   25\n",
            "3   7\n",
            "0.7   26\n",
            "3   8\n",
            "0.7   27\n",
            "3   8\n",
            "0.7   28\n",
            "3   8\n",
            "0.7   29\n",
            "3   8\n",
            "0.7   30\n",
            "3   8\n",
            "0.7   31\n",
            "3   8\n",
            "0.7   32\n",
            "4   9\n",
            "0.7   33\n",
            "4   9\n",
            "0.7   34\n",
            "4   10\n",
            "0.7   35\n",
            "5   11\n",
            "0.7   36\n",
            "5   11\n",
            "0.7   37\n",
            "6   12\n",
            "0.7   38\n",
            "6   12\n",
            "0.7   39\n",
            "6   12\n",
            "0.7   40\n",
            "6   12\n",
            "0.7   41\n",
            "6   12\n",
            "0.7   42\n",
            "6   13\n",
            "0.7   43\n",
            "6   13\n",
            "0.7   44\n",
            "6   13\n",
            "0.7   45\n",
            "6   13\n",
            "0.7   46\n",
            "6   13\n",
            "0.7   47\n",
            "7   14\n",
            "0.7   48\n",
            "7   14\n",
            "0.7   49\n",
            "8   15\n",
            "0.7   50\n",
            "8   15\n",
            "0.7   51\n",
            "8   15\n",
            "0.7   52\n",
            "8   15\n",
            "0.7   53\n",
            "8   15\n",
            "0.7   54\n",
            "8   15\n",
            "0.7   55\n",
            "8   15\n",
            "0.7   56\n",
            "8   15\n",
            "0.7   57\n",
            "9   16\n",
            "0.7   58\n",
            "10   17\n",
            "0.7   59\n",
            "10   17\n",
            "0.7   60\n",
            "10   17\n",
            "0.7   61\n",
            "11   18\n",
            "0.7   62\n",
            "11   18\n",
            "0.7   63\n",
            "12   19\n",
            "0.7   64\n",
            "12   19\n",
            "0.7   65\n",
            "12   19\n",
            "0.7   66\n",
            "12   19\n",
            "0.7   67\n",
            "12   19\n",
            "0.7   68\n",
            "13   20\n",
            "0.7   69\n",
            "14   21\n",
            "0.7   70\n",
            "15   22\n",
            "0.7   71\n",
            "15   22\n",
            "0.7   72\n",
            "15   22\n",
            "0.7   73\n",
            "15   22\n",
            "0.7   74\n",
            "15   23\n",
            "0.7   75\n",
            "15   23\n",
            "0.7   76\n",
            "15   23\n",
            "0.7   77\n",
            "15   23\n",
            "0.7   78\n",
            "16   23\n",
            "0.7   79\n",
            "16   23\n",
            "0.7   80\n",
            "16   23\n",
            "0.7   81\n",
            "16   24\n",
            "0.7   82\n",
            "16   24\n",
            "0.7   83\n",
            "16   24\n",
            "0.7   84\n",
            "16   24\n",
            "0.7   85\n",
            "16   24\n",
            "0.7   86\n",
            "17   25\n",
            "0.7   87\n",
            "18   26\n",
            "0.7   88\n",
            "18   27\n",
            "0.7   89\n",
            "18   27\n",
            "0.7   90\n",
            "18   27\n",
            "0.7   91\n",
            "19   28\n",
            "0.7   92\n",
            "19   28\n",
            "0.7   93\n",
            "19   28\n",
            "0.7   94\n",
            "19   28\n",
            "0.7   95\n",
            "19   29\n",
            "0.7   96\n",
            "19   29\n",
            "0.7   97\n",
            "19   29\n",
            "0.7   98\n",
            "19   29\n",
            "0.7   99\n",
            "19   29\n",
            "0.7   100\n",
            "19   29\n",
            "0.7   101\n",
            "20   30\n",
            "0.7   102\n",
            "20   30\n",
            "0.7   103\n",
            "20   30\n",
            "0.7   104\n",
            "20   30\n",
            "0.7   105\n",
            "20   30\n",
            "0.7   106\n",
            "21   31\n",
            "0.7   107\n",
            "21   31\n",
            "0.7   108\n",
            "21   32\n",
            "0.7   109\n",
            "21   32\n",
            "0.7   110\n",
            "21   33\n",
            "0.7   111\n",
            "21   33\n",
            "0.7   112\n",
            "21   33\n",
            "0.7   113\n",
            "21   33\n",
            "0.7   114\n",
            "21   33\n",
            "0.7   115\n",
            "21   34\n",
            "0.7   116\n",
            "21   34\n",
            "0.7   117\n",
            "21   34\n",
            "0.7   118\n",
            "22   35\n",
            "0.7   119\n",
            "23   36\n",
            "0.7   120\n",
            "23   36\n",
            "0.7   121\n",
            "24   37\n",
            "0.7   122\n",
            "24   37\n",
            "0.7   123\n",
            "24   37\n",
            "0.7   124\n",
            "24   38\n",
            "0.7   125\n",
            "25   39\n",
            "0.7   126\n",
            "25   39\n",
            "0.7   127\n",
            "26   40\n",
            "0.7   128\n",
            "27   41\n",
            "0.7   129\n",
            "27   41\n",
            "0.7   130\n",
            "27   42\n",
            "0.7   131\n",
            "27   42\n",
            "0.7   132\n",
            "27   42\n",
            "0.7   133\n",
            "27   42\n",
            "0.7   134\n",
            "27   42\n",
            "0.7   135\n",
            "28   43\n",
            "0.7   136\n",
            "28   43\n",
            "0.7   137\n",
            "28   43\n",
            "0.7   138\n",
            "28   43\n",
            "0.7   139\n",
            "28   43\n",
            "0.7   140\n",
            "28   43\n",
            "0.7   141\n",
            "28   43\n",
            "0.7   142\n",
            "28   43\n",
            "0.7   143\n",
            "28   44\n",
            "0.7   144\n",
            "28   44\n",
            "0.7   145\n",
            "28   44\n",
            "0.7   146\n",
            "28   44\n",
            "0.7   147\n",
            "28   44\n",
            "0.7   148\n",
            "28   44\n",
            "0.7   149\n",
            "28   44\n",
            "0.7   150\n",
            "29   45\n",
            "0.7   151\n",
            "30   46\n",
            "0.7   152\n",
            "30   46\n",
            "0.7   153\n",
            "30   46\n",
            "0.7   154\n",
            "30   46\n",
            "0.7   155\n",
            "30   47\n",
            "0.7   156\n",
            "30   47\n",
            "0.7   157\n",
            "30   47\n",
            "0.7   158\n",
            "31   48\n",
            "0.7   159\n",
            "31   48\n",
            "0.7   160\n",
            "31   48\n",
            "0.7   161\n",
            "31   48\n",
            "0.7   162\n",
            "31   48\n",
            "0.7   163\n",
            "31   48\n",
            "0.7   164\n",
            "32   49\n",
            "0.7   165\n",
            "33   49\n",
            "0.7   166\n",
            "33   50\n",
            "0.7   167\n",
            "33   51\n",
            "0.7   168\n",
            "33   51\n",
            "0.7   169\n",
            "34   52\n",
            "0.7   170\n",
            "35   53\n",
            "0.7   171\n",
            "36   53\n",
            "0.7   172\n",
            "36   53\n",
            "0.7   173\n",
            "36   54\n",
            "0.7   174\n",
            "36   54\n",
            "0.7   175\n",
            "36   54\n",
            "0.7   176\n",
            "36   54\n",
            "0.7   177\n",
            "36   54\n",
            "0.7   178\n",
            "37   55\n",
            "0.7   179\n",
            "37   55\n",
            "0.7   180\n",
            "37   55\n",
            "0.7   181\n",
            "37   55\n",
            "0.7   182\n",
            "37   55\n",
            "0.7   183\n",
            "37   56\n",
            "0.7   184\n",
            "37   56\n",
            "0.7   185\n",
            "37   56\n",
            "0.7   186\n",
            "37   56\n",
            "0.7   187\n",
            "38   57\n",
            "0.7   188\n",
            "39   58\n",
            "0.7   189\n",
            "39   58\n",
            "0.7   190\n",
            "39   59\n",
            "0.7   191\n",
            "39   59\n",
            "0.7   192\n",
            "39   60\n",
            "0.7   193\n",
            "39   60\n",
            "0.7   194\n",
            "39   60\n",
            "0.7   195\n",
            "40   61\n",
            "0.7   196\n",
            "40   61\n",
            "0.7   197\n",
            "40   61\n",
            "0.7   198\n",
            "40   61\n",
            "0.7   199\n",
            "40   61\n",
            "Epsilon value -  0.7999999999999999\n",
            "\n",
            "0.7999999999999999   0\n",
            "0   0\n",
            "0.7999999999999999   1\n",
            "0   0\n",
            "0.7999999999999999   2\n",
            "0   0\n",
            "0.7999999999999999   3\n",
            "0   0\n",
            "0.7999999999999999   4\n",
            "0   0\n",
            "0.7999999999999999   5\n",
            "0   1\n",
            "0.7999999999999999   6\n",
            "0   1\n",
            "0.7999999999999999   7\n",
            "0   1\n",
            "0.7999999999999999   8\n",
            "0   1\n",
            "0.7999999999999999   9\n",
            "0   1\n",
            "0.7999999999999999   10\n",
            "0   1\n",
            "0.7999999999999999   11\n",
            "0   1\n",
            "0.7999999999999999   12\n",
            "0   1\n",
            "0.7999999999999999   13\n",
            "0   1\n",
            "0.7999999999999999   14\n",
            "0   1\n",
            "0.7999999999999999   15\n",
            "1   2\n",
            "0.7999999999999999   16\n",
            "1   2\n",
            "0.7999999999999999   17\n",
            "1   3\n",
            "0.7999999999999999   18\n",
            "1   3\n",
            "0.7999999999999999   19\n",
            "1   3\n",
            "0.7999999999999999   20\n",
            "1   4\n",
            "0.7999999999999999   21\n",
            "2   5\n",
            "0.7999999999999999   22\n",
            "3   6\n",
            "0.7999999999999999   23\n",
            "3   6\n",
            "0.7999999999999999   24\n",
            "3   6\n",
            "0.7999999999999999   25\n",
            "3   7\n",
            "0.7999999999999999   26\n",
            "3   8\n",
            "0.7999999999999999   27\n",
            "3   8\n",
            "0.7999999999999999   28\n",
            "3   8\n",
            "0.7999999999999999   29\n",
            "3   8\n",
            "0.7999999999999999   30\n",
            "3   9\n",
            "0.7999999999999999   31\n",
            "3   9\n",
            "0.7999999999999999   32\n",
            "4   10\n",
            "0.7999999999999999   33\n",
            "4   11\n",
            "0.7999999999999999   34\n",
            "4   12\n",
            "0.7999999999999999   35\n",
            "5   13\n",
            "0.7999999999999999   36\n",
            "5   13\n",
            "0.7999999999999999   37\n",
            "6   14\n",
            "0.7999999999999999   38\n",
            "6   14\n",
            "0.7999999999999999   39\n",
            "6   14\n",
            "0.7999999999999999   40\n",
            "6   14\n",
            "0.7999999999999999   41\n",
            "6   14\n",
            "0.7999999999999999   42\n",
            "6   15\n",
            "0.7999999999999999   43\n",
            "6   15\n",
            "0.7999999999999999   44\n",
            "6   15\n",
            "0.7999999999999999   45\n",
            "6   15\n",
            "0.7999999999999999   46\n",
            "6   15\n",
            "0.7999999999999999   47\n",
            "7   16\n",
            "0.7999999999999999   48\n",
            "7   16\n",
            "0.7999999999999999   49\n",
            "8   17\n",
            "0.7999999999999999   50\n",
            "8   17\n",
            "0.7999999999999999   51\n",
            "8   17\n",
            "0.7999999999999999   52\n",
            "8   17\n",
            "0.7999999999999999   53\n",
            "8   17\n",
            "0.7999999999999999   54\n",
            "8   17\n",
            "0.7999999999999999   55\n",
            "8   17\n",
            "0.7999999999999999   56\n",
            "8   17\n",
            "0.7999999999999999   57\n",
            "9   18\n",
            "0.7999999999999999   58\n",
            "10   19\n",
            "0.7999999999999999   59\n",
            "10   19\n",
            "0.7999999999999999   60\n",
            "10   19\n",
            "0.7999999999999999   61\n",
            "11   20\n",
            "0.7999999999999999   62\n",
            "11   20\n",
            "0.7999999999999999   63\n",
            "12   21\n",
            "0.7999999999999999   64\n",
            "12   21\n",
            "0.7999999999999999   65\n",
            "12   22\n",
            "0.7999999999999999   66\n",
            "12   22\n",
            "0.7999999999999999   67\n",
            "12   22\n",
            "0.7999999999999999   68\n",
            "13   23\n",
            "0.7999999999999999   69\n",
            "14   24\n",
            "0.7999999999999999   70\n",
            "15   25\n",
            "0.7999999999999999   71\n",
            "15   25\n",
            "0.7999999999999999   72\n",
            "15   25\n",
            "0.7999999999999999   73\n",
            "15   25\n",
            "0.7999999999999999   74\n",
            "15   26\n",
            "0.7999999999999999   75\n",
            "15   26\n",
            "0.7999999999999999   76\n",
            "15   26\n",
            "0.7999999999999999   77\n",
            "15   26\n",
            "0.7999999999999999   78\n",
            "16   26\n",
            "0.7999999999999999   79\n",
            "16   26\n",
            "0.7999999999999999   80\n",
            "16   26\n",
            "0.7999999999999999   81\n",
            "16   27\n",
            "0.7999999999999999   82\n",
            "16   27\n",
            "0.7999999999999999   83\n",
            "16   27\n",
            "0.7999999999999999   84\n",
            "16   27\n",
            "0.7999999999999999   85\n",
            "16   27\n",
            "0.7999999999999999   86\n",
            "17   28\n",
            "0.7999999999999999   87\n",
            "18   29\n",
            "0.7999999999999999   88\n",
            "18   30\n",
            "0.7999999999999999   89\n",
            "18   31\n",
            "0.7999999999999999   90\n",
            "18   31\n",
            "0.7999999999999999   91\n",
            "19   32\n",
            "0.7999999999999999   92\n",
            "19   32\n",
            "0.7999999999999999   93\n",
            "19   32\n",
            "0.7999999999999999   94\n",
            "19   32\n",
            "0.7999999999999999   95\n",
            "19   33\n",
            "0.7999999999999999   96\n",
            "19   33\n",
            "0.7999999999999999   97\n",
            "19   33\n",
            "0.7999999999999999   98\n",
            "19   33\n",
            "0.7999999999999999   99\n",
            "19   33\n",
            "0.7999999999999999   100\n",
            "19   33\n",
            "0.7999999999999999   101\n",
            "20   34\n",
            "0.7999999999999999   102\n",
            "20   34\n",
            "0.7999999999999999   103\n",
            "20   34\n",
            "0.7999999999999999   104\n",
            "20   34\n",
            "0.7999999999999999   105\n",
            "20   34\n",
            "0.7999999999999999   106\n",
            "21   35\n",
            "0.7999999999999999   107\n",
            "21   35\n",
            "0.7999999999999999   108\n",
            "21   36\n",
            "0.7999999999999999   109\n",
            "21   36\n",
            "0.7999999999999999   110\n",
            "21   37\n",
            "0.7999999999999999   111\n",
            "21   37\n",
            "0.7999999999999999   112\n",
            "21   37\n",
            "0.7999999999999999   113\n",
            "21   37\n",
            "0.7999999999999999   114\n",
            "21   37\n",
            "0.7999999999999999   115\n",
            "21   38\n",
            "0.7999999999999999   116\n",
            "21   38\n",
            "0.7999999999999999   117\n",
            "21   38\n",
            "0.7999999999999999   118\n",
            "22   39\n",
            "0.7999999999999999   119\n",
            "23   40\n",
            "0.7999999999999999   120\n",
            "23   40\n",
            "0.7999999999999999   121\n",
            "24   41\n",
            "0.7999999999999999   122\n",
            "24   41\n",
            "0.7999999999999999   123\n",
            "24   41\n",
            "0.7999999999999999   124\n",
            "24   42\n",
            "0.7999999999999999   125\n",
            "25   43\n",
            "0.7999999999999999   126\n",
            "25   43\n",
            "0.7999999999999999   127\n",
            "26   44\n",
            "0.7999999999999999   128\n",
            "27   45\n",
            "0.7999999999999999   129\n",
            "27   45\n",
            "0.7999999999999999   130\n",
            "27   46\n",
            "0.7999999999999999   131\n",
            "27   46\n",
            "0.7999999999999999   132\n",
            "27   46\n",
            "0.7999999999999999   133\n",
            "27   46\n",
            "0.7999999999999999   134\n",
            "27   46\n",
            "0.7999999999999999   135\n",
            "28   47\n",
            "0.7999999999999999   136\n",
            "28   47\n",
            "0.7999999999999999   137\n",
            "28   47\n",
            "0.7999999999999999   138\n",
            "28   47\n",
            "0.7999999999999999   139\n",
            "28   47\n",
            "0.7999999999999999   140\n",
            "28   48\n",
            "0.7999999999999999   141\n",
            "28   48\n",
            "0.7999999999999999   142\n",
            "28   48\n",
            "0.7999999999999999   143\n",
            "28   49\n",
            "0.7999999999999999   144\n",
            "28   49\n",
            "0.7999999999999999   145\n",
            "28   50\n",
            "0.7999999999999999   146\n",
            "28   50\n",
            "0.7999999999999999   147\n",
            "28   50\n",
            "0.7999999999999999   148\n",
            "28   50\n",
            "0.7999999999999999   149\n",
            "28   50\n",
            "0.7999999999999999   150\n",
            "29   51\n",
            "0.7999999999999999   151\n",
            "30   52\n",
            "0.7999999999999999   152\n",
            "30   52\n",
            "0.7999999999999999   153\n",
            "30   52\n",
            "0.7999999999999999   154\n",
            "30   52\n",
            "0.7999999999999999   155\n",
            "30   53\n",
            "0.7999999999999999   156\n",
            "30   53\n",
            "0.7999999999999999   157\n",
            "30   53\n",
            "0.7999999999999999   158\n",
            "31   54\n",
            "0.7999999999999999   159\n",
            "31   54\n",
            "0.7999999999999999   160\n",
            "31   54\n",
            "0.7999999999999999   161\n",
            "31   54\n",
            "0.7999999999999999   162\n",
            "31   54\n",
            "0.7999999999999999   163\n",
            "31   54\n",
            "0.7999999999999999   164\n",
            "32   55\n",
            "0.7999999999999999   165\n",
            "33   55\n",
            "0.7999999999999999   166\n",
            "33   56\n",
            "0.7999999999999999   167\n",
            "33   57\n",
            "0.7999999999999999   168\n",
            "33   57\n",
            "0.7999999999999999   169\n",
            "34   58\n",
            "0.7999999999999999   170\n",
            "35   59\n",
            "0.7999999999999999   171\n",
            "36   59\n",
            "0.7999999999999999   172\n",
            "36   60\n",
            "0.7999999999999999   173\n",
            "36   61\n",
            "0.7999999999999999   174\n",
            "36   61\n",
            "0.7999999999999999   175\n",
            "36   61\n",
            "0.7999999999999999   176\n",
            "36   61\n",
            "0.7999999999999999   177\n",
            "36   61\n",
            "0.7999999999999999   178\n",
            "37   62\n",
            "0.7999999999999999   179\n",
            "37   62\n",
            "0.7999999999999999   180\n",
            "37   62\n",
            "0.7999999999999999   181\n",
            "37   62\n",
            "0.7999999999999999   182\n",
            "37   62\n",
            "0.7999999999999999   183\n",
            "37   63\n",
            "0.7999999999999999   184\n",
            "37   64\n",
            "0.7999999999999999   185\n",
            "37   64\n",
            "0.7999999999999999   186\n",
            "37   64\n",
            "0.7999999999999999   187\n",
            "38   65\n",
            "0.7999999999999999   188\n",
            "39   66\n",
            "0.7999999999999999   189\n",
            "39   66\n",
            "0.7999999999999999   190\n",
            "39   67\n",
            "0.7999999999999999   191\n",
            "39   67\n",
            "0.7999999999999999   192\n",
            "39   68\n",
            "0.7999999999999999   193\n",
            "39   68\n",
            "0.7999999999999999   194\n",
            "39   68\n",
            "0.7999999999999999   195\n",
            "40   69\n",
            "0.7999999999999999   196\n",
            "40   69\n",
            "0.7999999999999999   197\n",
            "40   69\n",
            "0.7999999999999999   198\n",
            "40   69\n",
            "0.7999999999999999   199\n",
            "40   69\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for key in result_cifar10.keys():\n",
        "  print(key,' ',result_cifar10[key])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xZClhbNq1Fi",
        "outputId": "2a9e3c23-1a80-4c41-ee6a-8585f76b8260"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.1   (28, 28)\n",
            "0.2   (31, 33)\n",
            "0.30000000000000004   (33, 36)\n",
            "0.4   (40, 40)\n",
            "0.5   (44, 40)\n",
            "0.6   (50, 40)\n",
            "0.7   (61, 40)\n",
            "0.7999999999999999   (69, 40)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6czkrtYCQ_zz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}