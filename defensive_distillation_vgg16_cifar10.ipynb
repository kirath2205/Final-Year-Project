{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "defensive_distillation_vgg16_cifar10.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPRTxblozAw+6EbeC9Kq+Se",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kirath2205/Final-Year-Project/blob/main/defensive_distillation_vgg16_cifar10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "c9Vpz-objd13"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Input, UpSampling2D, Flatten, BatchNormalization, Dense, Dropout, GlobalAveragePooling2D , UpSampling3D , Lambda , Conv2D ,MaxPooling2D\n",
        "from tensorflow.keras import optimizers\n",
        "from keras.datasets import cifar100,cifar10,fashion_mnist\n",
        "import tensorflow as tf\n",
        "from keras.utils import np_utils\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from skimage.transform import resize\n",
        "from keras import applications\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50 , preprocess_input , decode_predictions\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras import regularizers\n",
        "tf.keras.backend.clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "device_lib.list_local_devices()\n",
        "tf.test.gpu_device_name()\n"
      ],
      "metadata": {
        "id": "FNOkdguAen-8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6411716d-5e62-4294-b0b7-59d48cce1f81"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Distiller(keras.Model):\n",
        "    def __init__(self, student, teacher):\n",
        "        super(Distiller, self).__init__()\n",
        "        self.teacher = teacher\n",
        "        self.student = student\n",
        "\n",
        "    def compile(\n",
        "        self,\n",
        "        optimizer,\n",
        "        metrics,\n",
        "        student_loss_fn,\n",
        "        distillation_loss_fn,\n",
        "        alpha=0.2,\n",
        "        temperature=20,\n",
        "    ):\n",
        "        \"\"\" Configure the distiller.\n",
        "\n",
        "        Args:\n",
        "            optimizer: Keras optimizer for the student weights\n",
        "            metrics: Keras metrics for evaluation\n",
        "            student_loss_fn: Loss function of difference between student\n",
        "                predictions and ground-truth\n",
        "            distillation_loss_fn: Loss function of difference between soft\n",
        "                student predictions and soft teacher predictions\n",
        "            alpha: weight to student_loss_fn and 1-alpha to distillation_loss_fn\n",
        "            temperature: Temperature for softening probability distributions.\n",
        "                Larger temperature gives softer distributions.\n",
        "        \"\"\"\n",
        "        super(Distiller, self).compile(optimizer=optimizer, metrics=metrics)\n",
        "        self.student_loss_fn = student_loss_fn\n",
        "        self.distillation_loss_fn = distillation_loss_fn\n",
        "        self.alpha = alpha\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def train_step(self, data):\n",
        "        # Unpack data\n",
        "        x, y = data\n",
        "\n",
        "        # Forward pass of teacher\n",
        "        teacher_predictions = self.teacher(x, training=False)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Forward pass of student\n",
        "            student_predictions = self.student(x, training=True)\n",
        "\n",
        "            # Compute losses\n",
        "            student_loss = self.student_loss_fn(y, student_predictions)\n",
        "            distillation_loss = self.distillation_loss_fn(\n",
        "                tf.nn.softmax(teacher_predictions / self.temperature, axis=1),\n",
        "                tf.nn.softmax(student_predictions / self.temperature, axis=1),\n",
        "            )\n",
        "            loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n",
        "\n",
        "        # Compute gradients\n",
        "        trainable_vars = self.student.trainable_variables\n",
        "        gradients = tape.gradient(loss, trainable_vars)\n",
        "\n",
        "        # Update weights\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "\n",
        "        # Update the metrics configured in `compile()`.\n",
        "        self.compiled_metrics.update_state(y, student_predictions)\n",
        "\n",
        "        # Return a dict of performance\n",
        "        results = {m.name: m.result() for m in self.metrics}\n",
        "        results.update(\n",
        "            {\"student_loss\": student_loss, \"distillation_loss\": distillation_loss}\n",
        "        )\n",
        "        return results\n",
        "\n",
        "    def test_step(self, data):\n",
        "        # Unpack the data\n",
        "        x, y = data\n",
        "\n",
        "        # Compute predictions\n",
        "        y_prediction = self.student(x, training=False)\n",
        "\n",
        "        # Calculate the loss\n",
        "        student_loss = self.student_loss_fn(y, y_prediction)\n",
        "\n",
        "        # Update the metrics.\n",
        "        self.compiled_metrics.update_state(y, y_prediction)\n",
        "\n",
        "        # Return a dict of performance\n",
        "        results = {m.name: m.result() for m in self.metrics}\n",
        "        results.update({\"student_loss\": student_loss})\n",
        "        return results\n",
        "\n",
        "    def call(self, inputs, *args, **kwargs):\n",
        "       return self.student(inputs)"
      ],
      "metadata": {
        "id": "3vClqBLTkEoT"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_random_eraser(p=0.5, s_l=0.02, s_h=0.4, r_1=0.3, r_2=1/0.3, v_l=0, v_h=255, pixel_level=False):\n",
        "    def eraser(input_img):\n",
        "        img_h, img_w, img_c = input_img.shape\n",
        "        p_1 = np.random.rand()\n",
        "\n",
        "        if p_1 > p:\n",
        "            return input_img\n",
        "\n",
        "        while True:\n",
        "            s = np.random.uniform(s_l, s_h) * img_h * img_w\n",
        "            r = np.random.uniform(r_1, r_2)\n",
        "            w = int(np.sqrt(s / r))\n",
        "            h = int(np.sqrt(s * r))\n",
        "            left = np.random.randint(0, img_w)\n",
        "            top = np.random.randint(0, img_h)\n",
        "\n",
        "            if left + w <= img_w and top + h <= img_h:\n",
        "                break\n",
        "\n",
        "        if pixel_level:\n",
        "            c = np.random.uniform(v_l, v_h, (h, w, img_c))\n",
        "        else:\n",
        "            c = np.random.uniform(v_l, v_h)\n",
        "\n",
        "        input_img[top:top + h, left:left + w, :] = c\n",
        "\n",
        "        return input_img\n",
        "\n",
        "    return eraser"
      ],
      "metadata": {
        "id": "_4ac45bEkIq9"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def select_dataset(index=1): #1 for cifar10 , 2 for cifar100 , 3 for fashion mnist\n",
        "  if(index == 1):\n",
        "    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "    channel = 3\n",
        "    num_classes = 10\n",
        "  if(index == 2):\n",
        "    (x_train, y_train), (x_test, y_test) = cifar100.load_data()\n",
        "    num_classes = 100\n",
        "    channel = 3\n",
        "  if(index == 3):\n",
        "    (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "    x_train = x_train.reshape((60000, 28, 28, 1))\n",
        "    x_test =  x_test.reshape((10000, 28, 28, 1))\n",
        "    num_classes = 10\n",
        "    y_train = np_utils.to_categorical(y_train, num_classes)\n",
        "    y_test = np_utils.to_categorical(y_test, num_classes)\n",
        "    channel = 1\n",
        "    return (x_train , y_train , x_test , y_test , num_classes , channel)\n",
        "\n",
        "  #Pre-process the data\n",
        "  x_train = preprocess_input(x_train)\n",
        "  x_test = preprocess_input(x_test)\n",
        "  datagen = ImageDataGenerator(preprocessing_function=get_random_eraser(v_l=0, v_h=1, pixel_level=True))\n",
        "  datagen.fit(x_train)\n",
        "  y_train = np_utils.to_categorical(y_train, num_classes)\n",
        "  y_test = np_utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "  return (x_train , y_train , x_test , y_test , num_classes , channel , datagen)"
      ],
      "metadata": {
        "id": "Ds4lOH1qmZpq"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = 1\n",
        "x_train , y_train , x_test , y_test , num_classes ,channel , datagen = select_dataset(index)\n",
        "model_name = 'Desktop/Trained_models/defensive_distillation_keras_vgg16_cifar10'\n",
        "model_path = 'Desktop/Trained_models/defensive_distillation_keras_vgg16_cifar10.h5'"
      ],
      "metadata": {
        "id": "8TbyWALmnNDC"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = 2\n",
        "x_train , y_train , x_test , y_test , num_classes ,channel , datagen = select_dataset(index)\n",
        "model_name = 'Desktop/Trained_models/defensive_distillation_keras_vgg16_cifar100'\n",
        "model_path = 'Desktop/Trained_models/defensive_distillation_keras_vgg16_cifar100.h5'"
      ],
      "metadata": {
        "id": "2lRB-7LZncsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp = 200\n",
        "weight_decay = 0.0005\n",
        "if(index==1 or index==2):\n",
        "  teacher = keras.Sequential(\n",
        "      [\n",
        "        layers.UpSampling2D((5,5)),\n",
        "        layers.Conv2D(64,3,padding=\"same\",kernel_regularizer=regularizers.l2(weight_decay),input_shape = (160,160,3)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.ReLU(),\n",
        "        layers.Conv2D(64,3,padding=\"same\",kernel_regularizer=regularizers.l2(weight_decay)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.ReLU(),\n",
        "        layers.MaxPooling2D((2,2)),\n",
        "        layers.Dropout(0.25),\n",
        "        layers.Conv2D(128,3,padding=\"same\",kernel_regularizer=regularizers.l2(weight_decay)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.ReLU(),\n",
        "        layers.Conv2D(128,3,padding=\"same\",kernel_regularizer=regularizers.l2(weight_decay)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.ReLU(),\n",
        "        layers.MaxPooling2D((2,2)),\n",
        "        layers.Dropout(0.25),\n",
        "        layers.Conv2D(256,3,padding=\"same\",kernel_regularizer=regularizers.l2(weight_decay)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.ReLU(),\n",
        "        layers.Conv2D(256,3,padding=\"same\",kernel_regularizer=regularizers.l2(weight_decay)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.ReLU(),\n",
        "        layers.Conv2D(256,3,padding=\"same\",kernel_regularizer=regularizers.l2(weight_decay)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.ReLU(),\n",
        "        layers.MaxPooling2D((2,2)),\n",
        "        layers.Dropout(0.25),\n",
        "        layers.Conv2D(512,3,padding=\"same\",kernel_regularizer=regularizers.l2(weight_decay)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.ReLU(),\n",
        "        layers.Conv2D(512,3,padding=\"same\",kernel_regularizer=regularizers.l2(weight_decay)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.ReLU(),\n",
        "        layers.Conv2D(512,3,padding=\"same\",kernel_regularizer=regularizers.l2(weight_decay)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.ReLU(),\n",
        "        layers.MaxPooling2D((2,2)),\n",
        "        layers.Dropout(0.25),\n",
        "        layers.Conv2D(512,3,padding=\"same\",kernel_regularizer=regularizers.l2(weight_decay)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.ReLU(),\n",
        "        layers.Conv2D(512,3,padding=\"same\",kernel_regularizer=regularizers.l2(weight_decay)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.ReLU(),\n",
        "        layers.Conv2D(512,3,padding=\"same\",kernel_regularizer=regularizers.l2(weight_decay)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.ReLU(),\n",
        "        layers.MaxPooling2D((2,2)),\n",
        "        layers.Dropout(0.25),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(512,kernel_regularizer=regularizers.l2(weight_decay)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.ReLU(),\n",
        "        layers.Dropout(0.25),\n",
        "        layers.Dense(num_classes),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Lambda((lambda x: x / temp)),\n",
        "      ],\n",
        "      name=\"teacher\",\n",
        "  )\n",
        "\n",
        "# Create the student\n",
        "  student = keras.Sequential(\n",
        "      [\n",
        "        layers.UpSampling2D((5,5)),\n",
        "        layers.Conv2D(64,3,padding=\"same\",kernel_regularizer=regularizers.l2(weight_decay),input_shape = (160,160,3)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.ReLU(),\n",
        "        layers.Conv2D(64,3,padding=\"same\",kernel_regularizer=regularizers.l2(weight_decay)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.ReLU(),\n",
        "        layers.MaxPooling2D((2,2)),\n",
        "        layers.Dropout(0.25),\n",
        "        layers.Conv2D(128,3,padding=\"same\",kernel_regularizer=regularizers.l2(weight_decay)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.ReLU(),\n",
        "        layers.Conv2D(128,3,padding=\"same\",kernel_regularizer=regularizers.l2(weight_decay)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.ReLU(),\n",
        "        layers.MaxPooling2D((2,2)),\n",
        "        layers.Dropout(0.25),\n",
        "        layers.Conv2D(256,3,padding=\"same\",kernel_regularizer=regularizers.l2(weight_decay)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.ReLU(),\n",
        "        layers.Conv2D(256,3,padding=\"same\",kernel_regularizer=regularizers.l2(weight_decay)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.ReLU(),\n",
        "        layers.Conv2D(256,3,padding=\"same\",kernel_regularizer=regularizers.l2(weight_decay)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.ReLU(),\n",
        "        layers.MaxPooling2D((2,2)),\n",
        "        layers.Dropout(0.25),\n",
        "        layers.Conv2D(512,3,padding=\"same\",kernel_regularizer=regularizers.l2(weight_decay)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.ReLU(),\n",
        "        layers.Conv2D(512,3,padding=\"same\",kernel_regularizer=regularizers.l2(weight_decay)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.ReLU(),\n",
        "        layers.Conv2D(512,3,padding=\"same\",kernel_regularizer=regularizers.l2(weight_decay)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.ReLU(),\n",
        "        layers.MaxPooling2D((2,2)),\n",
        "        layers.Dropout(0.25),\n",
        "        layers.Conv2D(512,3,padding=\"same\",kernel_regularizer=regularizers.l2(weight_decay)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.ReLU(),\n",
        "        layers.Conv2D(512,3,padding=\"same\",kernel_regularizer=regularizers.l2(weight_decay)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.ReLU(),\n",
        "        layers.Conv2D(512,3,padding=\"same\",kernel_regularizer=regularizers.l2(weight_decay)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.ReLU(),\n",
        "        layers.MaxPooling2D((2,2)),\n",
        "        layers.Dropout(0.25),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(512,kernel_regularizer=regularizers.l2(weight_decay)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.ReLU(),\n",
        "        layers.Dropout(0.25),\n",
        "        layers.Dense(num_classes),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Lambda((lambda x: x / temp)),\n",
        "      ],\n",
        "      name=\"student\",\n",
        "  )\n"
      ],
      "metadata": {
        "id": "cMXAsAZXnp9y"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 125\n",
        "learning_rate = 0.1\n",
        "lr_decay = 1e-6\n",
        "lr_drop = 20\n",
        "callbacks = [ \n",
        "    tf.keras.callbacks.ModelCheckpoint(model_path, monitor='val_categorical_accuracy', verbose=1, save_best_only=True),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau( factor = 0.7, patience = 5, min_lr = 0.000001, verbose = 1,monitor='val_loss' )\n",
        "  ]\n",
        "batch_size = 128\n",
        "teacher.compile(\n",
        "    optimizer=keras.optimizers.SGD(lr=learning_rate, decay=lr_decay, momentum=0.9, nesterov=True),\n",
        "    loss=keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[keras.metrics.CategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "# Train and evaluate teacher on data.\n",
        "history_teacher = teacher.fit_generator(datagen.flow(x_train, y_train,\n",
        "                                  batch_size=batch_size),\n",
        "                                  steps_per_epoch=x_train.shape[0] // batch_size,\n",
        "                                  epochs=epochs,\n",
        "                                  validation_data=(x_test, y_test),callbacks=callbacks)\n",
        "teacher.evaluate(x_test, y_test)"
      ],
      "metadata": {
        "id": "pQUorMgFnyxf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0da2833-8b15-428c-a17c-9e8813c98096"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 4.0067 - categorical_accuracy: 0.3207\n",
            "Epoch 00001: val_categorical_accuracy improved from -inf to 0.10020, saving model to Desktop/Trained_models/defensive_distillation_keras_vgg16_cifar10.h5\n",
            "390/390 [==============================] - 167s 428ms/step - loss: 4.0067 - categorical_accuracy: 0.3207 - val_loss: 3.4266 - val_categorical_accuracy: 0.1002 - lr: 0.1000\n",
            "Epoch 2/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 3.0729 - categorical_accuracy: 0.4636\n",
            "Epoch 00002: val_categorical_accuracy improved from 0.10020 to 0.10660, saving model to Desktop/Trained_models/defensive_distillation_keras_vgg16_cifar10.h5\n",
            "390/390 [==============================] - 169s 434ms/step - loss: 3.0729 - categorical_accuracy: 0.4636 - val_loss: 2.8148 - val_categorical_accuracy: 0.1066 - lr: 0.1000\n",
            "Epoch 3/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 2.6448 - categorical_accuracy: 0.5661\n",
            "Epoch 00003: val_categorical_accuracy improved from 0.10660 to 0.14600, saving model to Desktop/Trained_models/defensive_distillation_keras_vgg16_cifar10.h5\n",
            "390/390 [==============================] - 167s 429ms/step - loss: 2.6448 - categorical_accuracy: 0.5661 - val_loss: 2.5347 - val_categorical_accuracy: 0.1460 - lr: 0.1000\n",
            "Epoch 4/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 2.4466 - categorical_accuracy: 0.6547\n",
            "Epoch 00004: val_categorical_accuracy improved from 0.14600 to 0.16940, saving model to Desktop/Trained_models/defensive_distillation_keras_vgg16_cifar10.h5\n",
            "390/390 [==============================] - 167s 428ms/step - loss: 2.4466 - categorical_accuracy: 0.6547 - val_loss: 2.4047 - val_categorical_accuracy: 0.1694 - lr: 0.1000\n",
            "Epoch 5/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 2.3531 - categorical_accuracy: 0.7235\n",
            "Epoch 00005: val_categorical_accuracy improved from 0.16940 to 0.55750, saving model to Desktop/Trained_models/defensive_distillation_keras_vgg16_cifar10.h5\n",
            "390/390 [==============================] - 167s 428ms/step - loss: 2.3531 - categorical_accuracy: 0.7235 - val_loss: 2.3327 - val_categorical_accuracy: 0.5575 - lr: 0.1000\n",
            "Epoch 6/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 2.3078 - categorical_accuracy: 0.7561\n",
            "Epoch 00006: val_categorical_accuracy improved from 0.55750 to 0.75940, saving model to Desktop/Trained_models/defensive_distillation_keras_vgg16_cifar10.h5\n",
            "390/390 [==============================] - 167s 428ms/step - loss: 2.3078 - categorical_accuracy: 0.7561 - val_loss: 2.2935 - val_categorical_accuracy: 0.7594 - lr: 0.1000\n",
            "Epoch 7/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 2.2844 - categorical_accuracy: 0.7769\n",
            "Epoch 00007: val_categorical_accuracy did not improve from 0.75940\n",
            "390/390 [==============================] - 166s 427ms/step - loss: 2.2844 - categorical_accuracy: 0.7769 - val_loss: 2.2802 - val_categorical_accuracy: 0.7155 - lr: 0.1000\n",
            "Epoch 8/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 2.2711 - categorical_accuracy: 0.7896\n",
            "Epoch 00008: val_categorical_accuracy did not improve from 0.75940\n",
            "390/390 [==============================] - 166s 427ms/step - loss: 2.2711 - categorical_accuracy: 0.7896 - val_loss: 2.2744 - val_categorical_accuracy: 0.6321 - lr: 0.1000\n",
            "Epoch 9/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 2.2622 - categorical_accuracy: 0.7979\n",
            "Epoch 00009: val_categorical_accuracy did not improve from 0.75940\n",
            "390/390 [==============================] - 166s 427ms/step - loss: 2.2622 - categorical_accuracy: 0.7979 - val_loss: 2.2639 - val_categorical_accuracy: 0.7109 - lr: 0.1000\n",
            "Epoch 10/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 2.2555 - categorical_accuracy: 0.8040\n",
            "Epoch 00010: val_categorical_accuracy did not improve from 0.75940\n",
            "390/390 [==============================] - 166s 427ms/step - loss: 2.2555 - categorical_accuracy: 0.8040 - val_loss: 2.2792 - val_categorical_accuracy: 0.5393 - lr: 0.1000\n",
            "Epoch 11/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 2.2497 - categorical_accuracy: 0.8119\n",
            "Epoch 00011: val_categorical_accuracy did not improve from 0.75940\n",
            "390/390 [==============================] - 166s 427ms/step - loss: 2.2497 - categorical_accuracy: 0.8119 - val_loss: 2.2615 - val_categorical_accuracy: 0.6253 - lr: 0.1000\n",
            "Epoch 12/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 2.2441 - categorical_accuracy: 0.8159\n",
            "Epoch 00012: val_categorical_accuracy did not improve from 0.75940\n",
            "390/390 [==============================] - 166s 427ms/step - loss: 2.2441 - categorical_accuracy: 0.8159 - val_loss: 2.2727 - val_categorical_accuracy: 0.4724 - lr: 0.1000\n",
            "Epoch 13/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 2.2388 - categorical_accuracy: 0.8203\n",
            "Epoch 00013: val_categorical_accuracy did not improve from 0.75940\n",
            "390/390 [==============================] - 166s 427ms/step - loss: 2.2388 - categorical_accuracy: 0.8203 - val_loss: 2.2485 - val_categorical_accuracy: 0.7174 - lr: 0.1000\n",
            "Epoch 14/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 2.2337 - categorical_accuracy: 0.8222\n",
            "Epoch 00014: val_categorical_accuracy did not improve from 0.75940\n",
            "390/390 [==============================] - 166s 427ms/step - loss: 2.2337 - categorical_accuracy: 0.8222 - val_loss: 2.2476 - val_categorical_accuracy: 0.6755 - lr: 0.1000\n",
            "Epoch 15/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 2.2285 - categorical_accuracy: 0.8258\n",
            "Epoch 00015: val_categorical_accuracy did not improve from 0.75940\n",
            "390/390 [==============================] - 166s 427ms/step - loss: 2.2285 - categorical_accuracy: 0.8258 - val_loss: 2.2380 - val_categorical_accuracy: 0.7425 - lr: 0.1000\n",
            "Epoch 16/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 2.2235 - categorical_accuracy: 0.8271\n",
            "Epoch 00016: val_categorical_accuracy did not improve from 0.75940\n",
            "390/390 [==============================] - 166s 427ms/step - loss: 2.2235 - categorical_accuracy: 0.8271 - val_loss: 2.2500 - val_categorical_accuracy: 0.5763 - lr: 0.1000\n",
            "Epoch 17/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 2.2181 - categorical_accuracy: 0.8329\n",
            "Epoch 00017: val_categorical_accuracy did not improve from 0.75940\n",
            "390/390 [==============================] - 166s 427ms/step - loss: 2.2181 - categorical_accuracy: 0.8329 - val_loss: 2.2339 - val_categorical_accuracy: 0.7076 - lr: 0.1000\n",
            "Epoch 18/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 2.2129 - categorical_accuracy: 0.8337\n",
            "Epoch 00018: val_categorical_accuracy did not improve from 0.75940\n",
            "390/390 [==============================] - 166s 427ms/step - loss: 2.2129 - categorical_accuracy: 0.8337 - val_loss: 2.2263 - val_categorical_accuracy: 0.7154 - lr: 0.1000\n",
            "Epoch 19/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 2.2077 - categorical_accuracy: 0.8368\n",
            "Epoch 00019: val_categorical_accuracy did not improve from 0.75940\n",
            "390/390 [==============================] - 166s 427ms/step - loss: 2.2077 - categorical_accuracy: 0.8368 - val_loss: 2.2311 - val_categorical_accuracy: 0.6699 - lr: 0.1000\n",
            "Epoch 20/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 2.2028 - categorical_accuracy: 0.8347\n",
            "Epoch 00020: val_categorical_accuracy did not improve from 0.75940\n",
            "390/390 [==============================] - 166s 427ms/step - loss: 2.2028 - categorical_accuracy: 0.8347 - val_loss: 2.2140 - val_categorical_accuracy: 0.7320 - lr: 0.1000\n",
            "Epoch 21/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 2.1974 - categorical_accuracy: 0.8388\n",
            "Epoch 00021: val_categorical_accuracy did not improve from 0.75940\n",
            "390/390 [==============================] - 166s 427ms/step - loss: 2.1974 - categorical_accuracy: 0.8388 - val_loss: 2.2128 - val_categorical_accuracy: 0.7195 - lr: 0.1000\n",
            "Epoch 22/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 2.1923 - categorical_accuracy: 0.8408\n",
            "Epoch 00022: val_categorical_accuracy did not improve from 0.75940\n",
            "390/390 [==============================] - 167s 427ms/step - loss: 2.1923 - categorical_accuracy: 0.8408 - val_loss: 2.2263 - val_categorical_accuracy: 0.6084 - lr: 0.1000\n",
            "Epoch 23/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 2.1876 - categorical_accuracy: 0.8403\n",
            "Epoch 00023: val_categorical_accuracy did not improve from 0.75940\n",
            "390/390 [==============================] - 166s 427ms/step - loss: 2.1876 - categorical_accuracy: 0.8403 - val_loss: 2.2424 - val_categorical_accuracy: 0.5100 - lr: 0.1000\n",
            "Epoch 24/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 2.1825 - categorical_accuracy: 0.8412\n",
            "Epoch 00024: val_categorical_accuracy did not improve from 0.75940\n",
            "390/390 [==============================] - 166s 427ms/step - loss: 2.1825 - categorical_accuracy: 0.8412 - val_loss: 2.1961 - val_categorical_accuracy: 0.7529 - lr: 0.1000\n",
            "Epoch 25/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 2.1774 - categorical_accuracy: 0.8421\n",
            "Epoch 00025: val_categorical_accuracy did not improve from 0.75940\n",
            "390/390 [==============================] - 166s 426ms/step - loss: 2.1774 - categorical_accuracy: 0.8421 - val_loss: 2.2172 - val_categorical_accuracy: 0.6707 - lr: 0.1000\n",
            "Epoch 26/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 2.1722 - categorical_accuracy: 0.8446\n",
            "Epoch 00026: val_categorical_accuracy improved from 0.75940 to 0.77940, saving model to Desktop/Trained_models/defensive_distillation_keras_vgg16_cifar10.h5\n",
            "390/390 [==============================] - 167s 428ms/step - loss: 2.1722 - categorical_accuracy: 0.8446 - val_loss: 2.1865 - val_categorical_accuracy: 0.7794 - lr: 0.1000\n",
            "Epoch 27/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 2.1668 - categorical_accuracy: 0.8461\n",
            "Epoch 00027: val_categorical_accuracy did not improve from 0.77940\n",
            "390/390 [==============================] - 166s 427ms/step - loss: 2.1668 - categorical_accuracy: 0.8461 - val_loss: 2.2210 - val_categorical_accuracy: 0.5711 - lr: 0.1000\n",
            "Epoch 28/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 2.1621 - categorical_accuracy: 0.8458\n",
            "Epoch 00028: val_categorical_accuracy did not improve from 0.77940\n",
            "390/390 [==============================] - 167s 427ms/step - loss: 2.1621 - categorical_accuracy: 0.8458 - val_loss: 2.1872 - val_categorical_accuracy: 0.7449 - lr: 0.1000\n",
            "Epoch 29/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 2.1569 - categorical_accuracy: 0.8468\n",
            "Epoch 00029: val_categorical_accuracy did not improve from 0.77940\n",
            "390/390 [==============================] - 166s 427ms/step - loss: 2.1569 - categorical_accuracy: 0.8468 - val_loss: 2.2031 - val_categorical_accuracy: 0.6522 - lr: 0.1000\n",
            "Epoch 30/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 2.1520 - categorical_accuracy: 0.8457\n",
            "Epoch 00030: val_categorical_accuracy did not improve from 0.77940\n",
            "390/390 [==============================] - 166s 426ms/step - loss: 2.1520 - categorical_accuracy: 0.8457 - val_loss: 2.1934 - val_categorical_accuracy: 0.6857 - lr: 0.1000\n",
            "Epoch 31/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 2.1472 - categorical_accuracy: 0.8450\n",
            "Epoch 00031: val_categorical_accuracy did not improve from 0.77940\n",
            "\n",
            "Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.07000000104308128.\n",
            "390/390 [==============================] - 166s 426ms/step - loss: 2.1472 - categorical_accuracy: 0.8450 - val_loss: 2.1956 - val_categorical_accuracy: 0.6355 - lr: 0.1000\n",
            "Epoch 32/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 2.1365 - categorical_accuracy: 0.8718\n",
            "Epoch 00032: val_categorical_accuracy did not improve from 0.77940\n",
            "390/390 [==============================] - 166s 427ms/step - loss: 2.1365 - categorical_accuracy: 0.8718 - val_loss: 2.1642 - val_categorical_accuracy: 0.7715 - lr: 0.0700\n",
            "Epoch 33/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 2.1312 - categorical_accuracy: 0.8717\n",
            "Epoch 00033: val_categorical_accuracy did not improve from 0.77940\n",
            "390/390 [==============================] - 166s 427ms/step - loss: 2.1312 - categorical_accuracy: 0.8717 - val_loss: 2.1708 - val_categorical_accuracy: 0.7114 - lr: 0.0700\n",
            "Epoch 34/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 2.1269 - categorical_accuracy: 0.8733\n",
            "Epoch 00034: val_categorical_accuracy did not improve from 0.77940\n",
            "390/390 [==============================] - 167s 427ms/step - loss: 2.1269 - categorical_accuracy: 0.8733 - val_loss: 2.1638 - val_categorical_accuracy: 0.6531 - lr: 0.0700\n",
            "Epoch 35/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 2.1232 - categorical_accuracy: 0.8723\n",
            "Epoch 00035: val_categorical_accuracy did not improve from 0.77940\n",
            "390/390 [==============================] - 166s 427ms/step - loss: 2.1232 - categorical_accuracy: 0.8723 - val_loss: 2.1678 - val_categorical_accuracy: 0.6853 - lr: 0.0700\n",
            "Epoch 36/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 2.1194 - categorical_accuracy: 0.8732\n",
            "Epoch 00036: val_categorical_accuracy did not improve from 0.77940\n",
            "390/390 [==============================] - 167s 427ms/step - loss: 2.1194 - categorical_accuracy: 0.8732 - val_loss: 2.1520 - val_categorical_accuracy: 0.7325 - lr: 0.0700\n",
            "Epoch 37/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 2.1156 - categorical_accuracy: 0.8755\n",
            "Epoch 00037: val_categorical_accuracy did not improve from 0.77940\n",
            "390/390 [==============================] - 166s 427ms/step - loss: 2.1156 - categorical_accuracy: 0.8755 - val_loss: 2.1553 - val_categorical_accuracy: 0.7544 - lr: 0.0700\n",
            "Epoch 38/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 2.1122 - categorical_accuracy: 0.8741\n",
            "Epoch 00038: val_categorical_accuracy improved from 0.77940 to 0.80020, saving model to Desktop/Trained_models/defensive_distillation_keras_vgg16_cifar10.h5\n",
            "390/390 [==============================] - 167s 428ms/step - loss: 2.1122 - categorical_accuracy: 0.8741 - val_loss: 2.1251 - val_categorical_accuracy: 0.8002 - lr: 0.0700\n",
            "Epoch 39/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 2.1080 - categorical_accuracy: 0.8772\n",
            "Epoch 00039: val_categorical_accuracy did not improve from 0.80020\n",
            "390/390 [==============================] - 166s 427ms/step - loss: 2.1080 - categorical_accuracy: 0.8772 - val_loss: 2.1467 - val_categorical_accuracy: 0.6974 - lr: 0.0700\n",
            "Epoch 40/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 2.1046 - categorical_accuracy: 0.8758\n",
            "Epoch 00040: val_categorical_accuracy improved from 0.80020 to 0.80640, saving model to Desktop/Trained_models/defensive_distillation_keras_vgg16_cifar10.h5\n",
            "390/390 [==============================] - 167s 428ms/step - loss: 2.1046 - categorical_accuracy: 0.8758 - val_loss: 2.1340 - val_categorical_accuracy: 0.8064 - lr: 0.0700\n",
            "Epoch 41/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 2.1002 - categorical_accuracy: 0.8801\n",
            "Epoch 00041: val_categorical_accuracy improved from 0.80640 to 0.82700, saving model to Desktop/Trained_models/defensive_distillation_keras_vgg16_cifar10.h5\n",
            "390/390 [==============================] - 167s 428ms/step - loss: 2.1002 - categorical_accuracy: 0.8801 - val_loss: 2.1164 - val_categorical_accuracy: 0.8270 - lr: 0.0700\n",
            "Epoch 42/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 2.0968 - categorical_accuracy: 0.8775\n",
            "Epoch 00042: val_categorical_accuracy did not improve from 0.82700\n",
            "390/390 [==============================] - 166s 427ms/step - loss: 2.0968 - categorical_accuracy: 0.8775 - val_loss: 2.1373 - val_categorical_accuracy: 0.7736 - lr: 0.0700\n",
            "Epoch 43/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 2.0924 - categorical_accuracy: 0.8818\n",
            "Epoch 00043: val_categorical_accuracy did not improve from 0.82700\n",
            "390/390 [==============================] - 166s 426ms/step - loss: 2.0924 - categorical_accuracy: 0.8818 - val_loss: 2.1501 - val_categorical_accuracy: 0.7246 - lr: 0.0700\n",
            "Epoch 44/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 2.0894 - categorical_accuracy: 0.8792\n",
            "Epoch 00044: val_categorical_accuracy did not improve from 0.82700\n",
            "390/390 [==============================] - 166s 427ms/step - loss: 2.0894 - categorical_accuracy: 0.8792 - val_loss: 2.1158 - val_categorical_accuracy: 0.8083 - lr: 0.0700\n",
            "Epoch 45/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 2.0855 - categorical_accuracy: 0.8809\n",
            "Epoch 00045: val_categorical_accuracy did not improve from 0.82700\n",
            "390/390 [==============================] - 166s 427ms/step - loss: 2.0855 - categorical_accuracy: 0.8809 - val_loss: 2.1229 - val_categorical_accuracy: 0.7711 - lr: 0.0700\n",
            "Epoch 46/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 2.0815 - categorical_accuracy: 0.8826\n",
            "Epoch 00046: val_categorical_accuracy did not improve from 0.82700\n",
            "390/390 [==============================] - 166s 426ms/step - loss: 2.0815 - categorical_accuracy: 0.8826 - val_loss: 2.1149 - val_categorical_accuracy: 0.7741 - lr: 0.0700\n",
            "Epoch 47/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 2.0787 - categorical_accuracy: 0.8800\n",
            "Epoch 00047: val_categorical_accuracy did not improve from 0.82700\n",
            "390/390 [==============================] - 166s 426ms/step - loss: 2.0787 - categorical_accuracy: 0.8800 - val_loss: 2.1251 - val_categorical_accuracy: 0.7561 - lr: 0.0700\n",
            "Epoch 48/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 2.0742 - categorical_accuracy: 0.8823\n",
            "Epoch 00048: val_categorical_accuracy did not improve from 0.82700\n",
            "390/390 [==============================] - 167s 427ms/step - loss: 2.0742 - categorical_accuracy: 0.8823 - val_loss: 2.0943 - val_categorical_accuracy: 0.8140 - lr: 0.0700\n",
            "Epoch 49/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 2.0704 - categorical_accuracy: 0.8843\n",
            "Epoch 00049: val_categorical_accuracy improved from 0.82700 to 0.82910, saving model to Desktop/Trained_models/defensive_distillation_keras_vgg16_cifar10.h5\n",
            "390/390 [==============================] - 167s 428ms/step - loss: 2.0704 - categorical_accuracy: 0.8843 - val_loss: 2.0803 - val_categorical_accuracy: 0.8291 - lr: 0.0700\n",
            "Epoch 50/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 2.0671 - categorical_accuracy: 0.8824\n",
            "Epoch 00050: val_categorical_accuracy did not improve from 0.82910\n",
            "390/390 [==============================] - 167s 427ms/step - loss: 2.0671 - categorical_accuracy: 0.8824 - val_loss: 2.0974 - val_categorical_accuracy: 0.8042 - lr: 0.0700\n",
            "Epoch 51/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 2.0629 - categorical_accuracy: 0.8850\n",
            "Epoch 00051: val_categorical_accuracy did not improve from 0.82910\n",
            "390/390 [==============================] - 166s 427ms/step - loss: 2.0629 - categorical_accuracy: 0.8850 - val_loss: 2.1142 - val_categorical_accuracy: 0.7720 - lr: 0.0700\n",
            "Epoch 52/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 2.0597 - categorical_accuracy: 0.8827\n",
            "Epoch 00052: val_categorical_accuracy did not improve from 0.82910\n",
            "390/390 [==============================] - 166s 427ms/step - loss: 2.0597 - categorical_accuracy: 0.8827 - val_loss: 2.0877 - val_categorical_accuracy: 0.7922 - lr: 0.0700\n",
            "Epoch 53/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 2.0558 - categorical_accuracy: 0.8866\n",
            "Epoch 00053: val_categorical_accuracy did not improve from 0.82910\n",
            "390/390 [==============================] - 166s 427ms/step - loss: 2.0558 - categorical_accuracy: 0.8866 - val_loss: 2.1112 - val_categorical_accuracy: 0.7422 - lr: 0.0700\n",
            "Epoch 54/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 2.0521 - categorical_accuracy: 0.8846\n",
            "Epoch 00054: val_categorical_accuracy did not improve from 0.82910\n",
            "\n",
            "Epoch 00054: ReduceLROnPlateau reducing learning rate to 0.04900000020861625.\n",
            "390/390 [==============================] - 166s 426ms/step - loss: 2.0521 - categorical_accuracy: 0.8846 - val_loss: 2.1175 - val_categorical_accuracy: 0.7301 - lr: 0.0700\n",
            "Epoch 55/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 2.0405 - categorical_accuracy: 0.9078\n",
            "Epoch 00055: val_categorical_accuracy did not improve from 0.82910\n",
            "390/390 [==============================] - 166s 427ms/step - loss: 2.0405 - categorical_accuracy: 0.9078 - val_loss: 2.0800 - val_categorical_accuracy: 0.8081 - lr: 0.0490\n",
            "Epoch 56/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 2.0345 - categorical_accuracy: 0.9103\n",
            "Epoch 00056: val_categorical_accuracy improved from 0.82910 to 0.85440, saving model to Desktop/Trained_models/defensive_distillation_keras_vgg16_cifar10.h5\n",
            "390/390 [==============================] - 167s 428ms/step - loss: 2.0345 - categorical_accuracy: 0.9103 - val_loss: 2.0551 - val_categorical_accuracy: 0.8544 - lr: 0.0490\n",
            "Epoch 57/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 2.0318 - categorical_accuracy: 0.9068\n",
            "Epoch 00057: val_categorical_accuracy did not improve from 0.85440\n",
            "390/390 [==============================] - 166s 427ms/step - loss: 2.0318 - categorical_accuracy: 0.9068 - val_loss: 2.0709 - val_categorical_accuracy: 0.7911 - lr: 0.0490\n",
            "Epoch 58/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 2.0286 - categorical_accuracy: 0.9091\n",
            "Epoch 00058: val_categorical_accuracy did not improve from 0.85440\n",
            "390/390 [==============================] - 167s 427ms/step - loss: 2.0286 - categorical_accuracy: 0.9091 - val_loss: 2.0902 - val_categorical_accuracy: 0.7622 - lr: 0.0490\n",
            "Epoch 59/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 2.0266 - categorical_accuracy: 0.9063\n",
            "Epoch 00059: val_categorical_accuracy did not improve from 0.85440\n",
            "390/390 [==============================] - 167s 427ms/step - loss: 2.0266 - categorical_accuracy: 0.9063 - val_loss: 2.0484 - val_categorical_accuracy: 0.8410 - lr: 0.0490\n",
            "Epoch 60/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 2.0234 - categorical_accuracy: 0.9085\n",
            "Epoch 00060: val_categorical_accuracy did not improve from 0.85440\n",
            "390/390 [==============================] - 167s 427ms/step - loss: 2.0234 - categorical_accuracy: 0.9085 - val_loss: 2.0789 - val_categorical_accuracy: 0.7788 - lr: 0.0490\n",
            "Epoch 61/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 2.0208 - categorical_accuracy: 0.9076\n",
            "Epoch 00061: val_categorical_accuracy did not improve from 0.85440\n",
            "390/390 [==============================] - 166s 427ms/step - loss: 2.0208 - categorical_accuracy: 0.9076 - val_loss: 2.0584 - val_categorical_accuracy: 0.8136 - lr: 0.0490\n",
            "Epoch 62/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 2.0179 - categorical_accuracy: 0.9098\n",
            "Epoch 00062: val_categorical_accuracy did not improve from 0.85440\n",
            "390/390 [==============================] - 166s 427ms/step - loss: 2.0179 - categorical_accuracy: 0.9098 - val_loss: 2.0444 - val_categorical_accuracy: 0.8275 - lr: 0.0490\n",
            "Epoch 63/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 2.0154 - categorical_accuracy: 0.9104\n",
            "Epoch 00063: val_categorical_accuracy did not improve from 0.85440\n",
            "390/390 [==============================] - 167s 427ms/step - loss: 2.0154 - categorical_accuracy: 0.9104 - val_loss: 2.0705 - val_categorical_accuracy: 0.7746 - lr: 0.0490\n",
            "Epoch 64/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 2.0126 - categorical_accuracy: 0.9119\n",
            "Epoch 00064: val_categorical_accuracy did not improve from 0.85440\n",
            "390/390 [==============================] - 166s 427ms/step - loss: 2.0126 - categorical_accuracy: 0.9119 - val_loss: 2.0469 - val_categorical_accuracy: 0.8478 - lr: 0.0490\n",
            "Epoch 65/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 2.0101 - categorical_accuracy: 0.9108\n",
            "Epoch 00065: val_categorical_accuracy did not improve from 0.85440\n",
            "390/390 [==============================] - 167s 427ms/step - loss: 2.0101 - categorical_accuracy: 0.9108 - val_loss: 2.0365 - val_categorical_accuracy: 0.8462 - lr: 0.0490\n",
            "Epoch 66/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 2.0074 - categorical_accuracy: 0.9104\n",
            "Epoch 00066: val_categorical_accuracy did not improve from 0.85440\n",
            "390/390 [==============================] - 167s 427ms/step - loss: 2.0074 - categorical_accuracy: 0.9104 - val_loss: 2.0422 - val_categorical_accuracy: 0.8365 - lr: 0.0490\n",
            "Epoch 67/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 2.0047 - categorical_accuracy: 0.9111\n",
            "Epoch 00067: val_categorical_accuracy did not improve from 0.85440\n",
            "390/390 [==============================] - 166s 427ms/step - loss: 2.0047 - categorical_accuracy: 0.9111 - val_loss: 2.0425 - val_categorical_accuracy: 0.8195 - lr: 0.0490\n",
            "Epoch 68/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 2.0024 - categorical_accuracy: 0.9108\n",
            "Epoch 00068: val_categorical_accuracy did not improve from 0.85440\n",
            "390/390 [==============================] - 166s 427ms/step - loss: 2.0024 - categorical_accuracy: 0.9108 - val_loss: 2.0527 - val_categorical_accuracy: 0.7908 - lr: 0.0490\n",
            "Epoch 69/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.9994 - categorical_accuracy: 0.9141\n",
            "Epoch 00069: val_categorical_accuracy did not improve from 0.85440\n",
            "390/390 [==============================] - 166s 427ms/step - loss: 1.9994 - categorical_accuracy: 0.9141 - val_loss: 2.0692 - val_categorical_accuracy: 0.7616 - lr: 0.0490\n",
            "Epoch 70/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.9961 - categorical_accuracy: 0.9153\n",
            "Epoch 00070: val_categorical_accuracy did not improve from 0.85440\n",
            "\n",
            "Epoch 00070: ReduceLROnPlateau reducing learning rate to 0.03429999910295009.\n",
            "390/390 [==============================] - 166s 427ms/step - loss: 1.9961 - categorical_accuracy: 0.9153 - val_loss: 2.0468 - val_categorical_accuracy: 0.8077 - lr: 0.0490\n",
            "Epoch 71/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.9854 - categorical_accuracy: 0.9351\n",
            "Epoch 00071: val_categorical_accuracy improved from 0.85440 to 0.87230, saving model to Desktop/Trained_models/defensive_distillation_keras_vgg16_cifar10.h5\n",
            "390/390 [==============================] - 167s 428ms/step - loss: 1.9854 - categorical_accuracy: 0.9351 - val_loss: 2.0155 - val_categorical_accuracy: 0.8723 - lr: 0.0343\n",
            "Epoch 72/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.9795 - categorical_accuracy: 0.9380\n",
            "Epoch 00072: val_categorical_accuracy did not improve from 0.87230\n",
            "390/390 [==============================] - 166s 427ms/step - loss: 1.9795 - categorical_accuracy: 0.9380 - val_loss: 2.0124 - val_categorical_accuracy: 0.8640 - lr: 0.0343\n",
            "Epoch 73/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.9772 - categorical_accuracy: 0.9356\n",
            "Epoch 00073: val_categorical_accuracy did not improve from 0.87230\n",
            "390/390 [==============================] - 166s 427ms/step - loss: 1.9772 - categorical_accuracy: 0.9356 - val_loss: 2.0004 - val_categorical_accuracy: 0.8519 - lr: 0.0343\n",
            "Epoch 74/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.9753 - categorical_accuracy: 0.9342\n",
            "Epoch 00074: val_categorical_accuracy did not improve from 0.87230\n",
            "390/390 [==============================] - 167s 427ms/step - loss: 1.9753 - categorical_accuracy: 0.9342 - val_loss: 1.9981 - val_categorical_accuracy: 0.8603 - lr: 0.0343\n",
            "Epoch 75/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.9735 - categorical_accuracy: 0.9328\n",
            "Epoch 00075: val_categorical_accuracy did not improve from 0.87230\n",
            "390/390 [==============================] - 166s 427ms/step - loss: 1.9735 - categorical_accuracy: 0.9328 - val_loss: 2.0141 - val_categorical_accuracy: 0.8482 - lr: 0.0343\n",
            "Epoch 76/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.9713 - categorical_accuracy: 0.9343\n",
            "Epoch 00076: val_categorical_accuracy did not improve from 0.87230\n",
            "390/390 [==============================] - 166s 427ms/step - loss: 1.9713 - categorical_accuracy: 0.9343 - val_loss: 2.0165 - val_categorical_accuracy: 0.8225 - lr: 0.0343\n",
            "Epoch 77/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.9689 - categorical_accuracy: 0.9352\n",
            "Epoch 00077: val_categorical_accuracy improved from 0.87230 to 0.88410, saving model to Desktop/Trained_models/defensive_distillation_keras_vgg16_cifar10.h5\n",
            "390/390 [==============================] - 167s 428ms/step - loss: 1.9689 - categorical_accuracy: 0.9352 - val_loss: 1.9952 - val_categorical_accuracy: 0.8841 - lr: 0.0343\n",
            "Epoch 78/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.9667 - categorical_accuracy: 0.9356\n",
            "Epoch 00078: val_categorical_accuracy did not improve from 0.88410\n",
            "390/390 [==============================] - 166s 426ms/step - loss: 1.9667 - categorical_accuracy: 0.9356 - val_loss: 2.0031 - val_categorical_accuracy: 0.8413 - lr: 0.0343\n",
            "Epoch 79/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.9651 - categorical_accuracy: 0.9348\n",
            "Epoch 00079: val_categorical_accuracy did not improve from 0.88410\n",
            "390/390 [==============================] - 166s 427ms/step - loss: 1.9651 - categorical_accuracy: 0.9348 - val_loss: 1.9992 - val_categorical_accuracy: 0.8475 - lr: 0.0343\n",
            "Epoch 80/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.9626 - categorical_accuracy: 0.9359\n",
            "Epoch 00080: val_categorical_accuracy did not improve from 0.88410\n",
            "390/390 [==============================] - 166s 427ms/step - loss: 1.9626 - categorical_accuracy: 0.9359 - val_loss: 2.0168 - val_categorical_accuracy: 0.8239 - lr: 0.0343\n",
            "Epoch 81/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.9615 - categorical_accuracy: 0.9353\n",
            "Epoch 00081: val_categorical_accuracy did not improve from 0.88410\n",
            "390/390 [==============================] - 166s 427ms/step - loss: 1.9615 - categorical_accuracy: 0.9353 - val_loss: 2.0303 - val_categorical_accuracy: 0.8134 - lr: 0.0343\n",
            "Epoch 82/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.9587 - categorical_accuracy: 0.9369\n",
            "Epoch 00082: val_categorical_accuracy did not improve from 0.88410\n",
            "390/390 [==============================] - 166s 427ms/step - loss: 1.9587 - categorical_accuracy: 0.9369 - val_loss: 1.9894 - val_categorical_accuracy: 0.8508 - lr: 0.0343\n",
            "Epoch 83/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.9573 - categorical_accuracy: 0.9366\n",
            "Epoch 00083: val_categorical_accuracy did not improve from 0.88410\n",
            "390/390 [==============================] - 166s 427ms/step - loss: 1.9573 - categorical_accuracy: 0.9366 - val_loss: 2.0130 - val_categorical_accuracy: 0.8357 - lr: 0.0343\n",
            "Epoch 84/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.9553 - categorical_accuracy: 0.9359\n",
            "Epoch 00084: val_categorical_accuracy did not improve from 0.88410\n",
            "390/390 [==============================] - 166s 427ms/step - loss: 1.9553 - categorical_accuracy: 0.9359 - val_loss: 2.0196 - val_categorical_accuracy: 0.7954 - lr: 0.0343\n",
            "Epoch 85/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.9533 - categorical_accuracy: 0.9371\n",
            "Epoch 00085: val_categorical_accuracy did not improve from 0.88410\n",
            "390/390 [==============================] - 166s 427ms/step - loss: 1.9533 - categorical_accuracy: 0.9371 - val_loss: 1.9950 - val_categorical_accuracy: 0.8448 - lr: 0.0343\n",
            "Epoch 86/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.9505 - categorical_accuracy: 0.9382\n",
            "Epoch 00086: val_categorical_accuracy did not improve from 0.88410\n",
            "390/390 [==============================] - 166s 427ms/step - loss: 1.9505 - categorical_accuracy: 0.9382 - val_loss: 1.9897 - val_categorical_accuracy: 0.8462 - lr: 0.0343\n",
            "Epoch 87/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.9482 - categorical_accuracy: 0.9399\n",
            "Epoch 00087: val_categorical_accuracy did not improve from 0.88410\n",
            "390/390 [==============================] - 166s 427ms/step - loss: 1.9482 - categorical_accuracy: 0.9399 - val_loss: 1.9878 - val_categorical_accuracy: 0.8482 - lr: 0.0343\n",
            "Epoch 88/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.9467 - categorical_accuracy: 0.9381\n",
            "Epoch 00088: val_categorical_accuracy did not improve from 0.88410\n",
            "390/390 [==============================] - 167s 427ms/step - loss: 1.9467 - categorical_accuracy: 0.9381 - val_loss: 2.0254 - val_categorical_accuracy: 0.7938 - lr: 0.0343\n",
            "Epoch 89/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.9453 - categorical_accuracy: 0.9379\n",
            "Epoch 00089: val_categorical_accuracy did not improve from 0.88410\n",
            "390/390 [==============================] - 166s 427ms/step - loss: 1.9453 - categorical_accuracy: 0.9379 - val_loss: 1.9893 - val_categorical_accuracy: 0.8601 - lr: 0.0343\n",
            "Epoch 90/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.9438 - categorical_accuracy: 0.9364\n",
            "Epoch 00090: val_categorical_accuracy did not improve from 0.88410\n",
            "390/390 [==============================] - 166s 427ms/step - loss: 1.9438 - categorical_accuracy: 0.9364 - val_loss: 2.0186 - val_categorical_accuracy: 0.7798 - lr: 0.0343\n",
            "Epoch 91/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.9411 - categorical_accuracy: 0.9393\n",
            "Epoch 00091: val_categorical_accuracy did not improve from 0.88410\n",
            "390/390 [==============================] - 167s 427ms/step - loss: 1.9411 - categorical_accuracy: 0.9393 - val_loss: 1.9809 - val_categorical_accuracy: 0.8636 - lr: 0.0343\n",
            "Epoch 92/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.9388 - categorical_accuracy: 0.9396\n",
            "Epoch 00092: val_categorical_accuracy did not improve from 0.88410\n",
            "390/390 [==============================] - 166s 427ms/step - loss: 1.9388 - categorical_accuracy: 0.9396 - val_loss: 1.9722 - val_categorical_accuracy: 0.8742 - lr: 0.0343\n",
            "Epoch 93/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.9370 - categorical_accuracy: 0.9398\n",
            "Epoch 00093: val_categorical_accuracy did not improve from 0.88410\n",
            "390/390 [==============================] - 166s 427ms/step - loss: 1.9370 - categorical_accuracy: 0.9398 - val_loss: 1.9826 - val_categorical_accuracy: 0.8462 - lr: 0.0343\n",
            "Epoch 94/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.9357 - categorical_accuracy: 0.9384\n",
            "Epoch 00094: val_categorical_accuracy did not improve from 0.88410\n",
            "390/390 [==============================] - 166s 427ms/step - loss: 1.9357 - categorical_accuracy: 0.9384 - val_loss: 1.9967 - val_categorical_accuracy: 0.8329 - lr: 0.0343\n",
            "Epoch 95/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.9334 - categorical_accuracy: 0.9385\n",
            "Epoch 00095: val_categorical_accuracy did not improve from 0.88410\n",
            "390/390 [==============================] - 166s 427ms/step - loss: 1.9334 - categorical_accuracy: 0.9385 - val_loss: 1.9686 - val_categorical_accuracy: 0.8619 - lr: 0.0343\n",
            "Epoch 96/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.9315 - categorical_accuracy: 0.9396\n",
            "Epoch 00096: val_categorical_accuracy did not improve from 0.88410\n",
            "390/390 [==============================] - 166s 427ms/step - loss: 1.9315 - categorical_accuracy: 0.9396 - val_loss: 2.0367 - val_categorical_accuracy: 0.7221 - lr: 0.0343\n",
            "Epoch 97/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.9298 - categorical_accuracy: 0.9387\n",
            "Epoch 00097: val_categorical_accuracy did not improve from 0.88410\n",
            "390/390 [==============================] - 166s 427ms/step - loss: 1.9298 - categorical_accuracy: 0.9387 - val_loss: 1.9720 - val_categorical_accuracy: 0.8620 - lr: 0.0343\n",
            "Epoch 98/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.9281 - categorical_accuracy: 0.9377\n",
            "Epoch 00098: val_categorical_accuracy did not improve from 0.88410\n",
            "390/390 [==============================] - 166s 427ms/step - loss: 1.9281 - categorical_accuracy: 0.9377 - val_loss: 1.9917 - val_categorical_accuracy: 0.7994 - lr: 0.0343\n",
            "Epoch 99/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.9257 - categorical_accuracy: 0.9399\n",
            "Epoch 00099: val_categorical_accuracy did not improve from 0.88410\n",
            "390/390 [==============================] - 166s 427ms/step - loss: 1.9257 - categorical_accuracy: 0.9399 - val_loss: 1.9717 - val_categorical_accuracy: 0.8420 - lr: 0.0343\n",
            "Epoch 100/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.9233 - categorical_accuracy: 0.9406\n",
            "Epoch 00100: val_categorical_accuracy did not improve from 0.88410\n",
            "\n",
            "Epoch 00100: ReduceLROnPlateau reducing learning rate to 0.024009999632835385.\n",
            "390/390 [==============================] - 166s 427ms/step - loss: 1.9233 - categorical_accuracy: 0.9406 - val_loss: 2.0003 - val_categorical_accuracy: 0.8239 - lr: 0.0343\n",
            "Epoch 101/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.9132 - categorical_accuracy: 0.9591\n",
            "Epoch 00101: val_categorical_accuracy improved from 0.88410 to 0.89230, saving model to Desktop/Trained_models/defensive_distillation_keras_vgg16_cifar10.h5\n",
            "390/390 [==============================] - 167s 428ms/step - loss: 1.9132 - categorical_accuracy: 0.9591 - val_loss: 1.9467 - val_categorical_accuracy: 0.8923 - lr: 0.0240\n",
            "Epoch 102/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.9074 - categorical_accuracy: 0.9624\n",
            "Epoch 00102: val_categorical_accuracy did not improve from 0.89230\n",
            "390/390 [==============================] - 167s 427ms/step - loss: 1.9074 - categorical_accuracy: 0.9624 - val_loss: 1.9599 - val_categorical_accuracy: 0.8651 - lr: 0.0240\n",
            "Epoch 103/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.9046 - categorical_accuracy: 0.9610\n",
            "Epoch 00103: val_categorical_accuracy did not improve from 0.89230\n",
            "390/390 [==============================] - 167s 427ms/step - loss: 1.9046 - categorical_accuracy: 0.9610 - val_loss: 1.9683 - val_categorical_accuracy: 0.8508 - lr: 0.0240\n",
            "Epoch 104/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.9043 - categorical_accuracy: 0.9554\n",
            "Epoch 00104: val_categorical_accuracy did not improve from 0.89230\n",
            "390/390 [==============================] - 167s 427ms/step - loss: 1.9043 - categorical_accuracy: 0.9554 - val_loss: 1.9264 - val_categorical_accuracy: 0.8892 - lr: 0.0240\n",
            "Epoch 105/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.9016 - categorical_accuracy: 0.9582\n",
            "Epoch 00105: val_categorical_accuracy did not improve from 0.89230\n",
            "390/390 [==============================] - 167s 427ms/step - loss: 1.9016 - categorical_accuracy: 0.9582 - val_loss: 1.9399 - val_categorical_accuracy: 0.8724 - lr: 0.0240\n",
            "Epoch 106/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.9002 - categorical_accuracy: 0.9576\n",
            "Epoch 00106: val_categorical_accuracy did not improve from 0.89230\n",
            "390/390 [==============================] - 167s 427ms/step - loss: 1.9002 - categorical_accuracy: 0.9576 - val_loss: 1.9370 - val_categorical_accuracy: 0.8874 - lr: 0.0240\n",
            "Epoch 107/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.8986 - categorical_accuracy: 0.9573\n",
            "Epoch 00107: val_categorical_accuracy did not improve from 0.89230\n",
            "390/390 [==============================] - 166s 427ms/step - loss: 1.8986 - categorical_accuracy: 0.9573 - val_loss: 1.9684 - val_categorical_accuracy: 0.8287 - lr: 0.0240\n",
            "Epoch 108/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.8981 - categorical_accuracy: 0.9554\n",
            "Epoch 00108: val_categorical_accuracy did not improve from 0.89230\n",
            "390/390 [==============================] - 166s 427ms/step - loss: 1.8981 - categorical_accuracy: 0.9554 - val_loss: 1.9540 - val_categorical_accuracy: 0.8571 - lr: 0.0240\n",
            "Epoch 109/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.8960 - categorical_accuracy: 0.9565\n",
            "Epoch 00109: val_categorical_accuracy did not improve from 0.89230\n",
            "\n",
            "Epoch 00109: ReduceLROnPlateau reducing learning rate to 0.01680699922144413.\n",
            "390/390 [==============================] - 167s 427ms/step - loss: 1.8960 - categorical_accuracy: 0.9565 - val_loss: 1.9589 - val_categorical_accuracy: 0.8344 - lr: 0.0240\n",
            "Epoch 110/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.8892 - categorical_accuracy: 0.9682\n",
            "Epoch 00110: val_categorical_accuracy improved from 0.89230 to 0.89340, saving model to Desktop/Trained_models/defensive_distillation_keras_vgg16_cifar10.h5\n",
            "390/390 [==============================] - 167s 429ms/step - loss: 1.8892 - categorical_accuracy: 0.9682 - val_loss: 1.9282 - val_categorical_accuracy: 0.8934 - lr: 0.0168\n",
            "Epoch 111/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.8843 - categorical_accuracy: 0.9727\n",
            "Epoch 00111: val_categorical_accuracy did not improve from 0.89340\n",
            "390/390 [==============================] - 167s 427ms/step - loss: 1.8843 - categorical_accuracy: 0.9727 - val_loss: 1.9264 - val_categorical_accuracy: 0.8902 - lr: 0.0168\n",
            "Epoch 112/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.8820 - categorical_accuracy: 0.9724\n",
            "Epoch 00112: val_categorical_accuracy improved from 0.89340 to 0.89760, saving model to Desktop/Trained_models/defensive_distillation_keras_vgg16_cifar10.h5\n",
            "390/390 [==============================] - 167s 429ms/step - loss: 1.8820 - categorical_accuracy: 0.9724 - val_loss: 1.9187 - val_categorical_accuracy: 0.8976 - lr: 0.0168\n",
            "Epoch 113/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.8808 - categorical_accuracy: 0.9702\n",
            "Epoch 00113: val_categorical_accuracy did not improve from 0.89760\n",
            "390/390 [==============================] - 166s 427ms/step - loss: 1.8808 - categorical_accuracy: 0.9702 - val_loss: 1.9201 - val_categorical_accuracy: 0.8868 - lr: 0.0168\n",
            "Epoch 114/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.8800 - categorical_accuracy: 0.9692\n",
            "Epoch 00114: val_categorical_accuracy improved from 0.89760 to 0.89980, saving model to Desktop/Trained_models/defensive_distillation_keras_vgg16_cifar10.h5\n",
            "390/390 [==============================] - 167s 428ms/step - loss: 1.8800 - categorical_accuracy: 0.9692 - val_loss: 1.9278 - val_categorical_accuracy: 0.8998 - lr: 0.0168\n",
            "Epoch 115/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.8786 - categorical_accuracy: 0.9680\n",
            "Epoch 00115: val_categorical_accuracy did not improve from 0.89980\n",
            "390/390 [==============================] - 167s 427ms/step - loss: 1.8786 - categorical_accuracy: 0.9680 - val_loss: 1.9249 - val_categorical_accuracy: 0.8909 - lr: 0.0168\n",
            "Epoch 116/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.8777 - categorical_accuracy: 0.9682\n",
            "Epoch 00116: val_categorical_accuracy did not improve from 0.89980\n",
            "390/390 [==============================] - 166s 427ms/step - loss: 1.8777 - categorical_accuracy: 0.9682 - val_loss: 1.9142 - val_categorical_accuracy: 0.8887 - lr: 0.0168\n",
            "Epoch 117/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.8754 - categorical_accuracy: 0.9687\n",
            "Epoch 00117: val_categorical_accuracy did not improve from 0.89980\n",
            "390/390 [==============================] - 167s 427ms/step - loss: 1.8754 - categorical_accuracy: 0.9687 - val_loss: 1.9428 - val_categorical_accuracy: 0.8537 - lr: 0.0168\n",
            "Epoch 118/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.8749 - categorical_accuracy: 0.9694\n",
            "Epoch 00118: val_categorical_accuracy did not improve from 0.89980\n",
            "390/390 [==============================] - 167s 427ms/step - loss: 1.8749 - categorical_accuracy: 0.9694 - val_loss: 1.9392 - val_categorical_accuracy: 0.8307 - lr: 0.0168\n",
            "Epoch 119/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.8748 - categorical_accuracy: 0.9670\n",
            "Epoch 00119: val_categorical_accuracy did not improve from 0.89980\n",
            "390/390 [==============================] - 166s 427ms/step - loss: 1.8748 - categorical_accuracy: 0.9670 - val_loss: 1.9495 - val_categorical_accuracy: 0.8307 - lr: 0.0168\n",
            "Epoch 120/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.8734 - categorical_accuracy: 0.9674\n",
            "Epoch 00120: val_categorical_accuracy did not improve from 0.89980\n",
            "390/390 [==============================] - 167s 427ms/step - loss: 1.8734 - categorical_accuracy: 0.9674 - val_loss: 1.9394 - val_categorical_accuracy: 0.8652 - lr: 0.0168\n",
            "Epoch 121/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.8718 - categorical_accuracy: 0.9676\n",
            "Epoch 00121: val_categorical_accuracy did not improve from 0.89980\n",
            "\n",
            "Epoch 00121: ReduceLROnPlateau reducing learning rate to 0.01176489945501089.\n",
            "390/390 [==============================] - 166s 427ms/step - loss: 1.8718 - categorical_accuracy: 0.9676 - val_loss: 1.9209 - val_categorical_accuracy: 0.8880 - lr: 0.0168\n",
            "Epoch 122/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.8660 - categorical_accuracy: 0.9782\n",
            "Epoch 00122: val_categorical_accuracy improved from 0.89980 to 0.91570, saving model to Desktop/Trained_models/defensive_distillation_keras_vgg16_cifar10.h5\n",
            "390/390 [==============================] - 167s 428ms/step - loss: 1.8660 - categorical_accuracy: 0.9782 - val_loss: 1.8961 - val_categorical_accuracy: 0.9157 - lr: 0.0118\n",
            "Epoch 123/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.8626 - categorical_accuracy: 0.9814\n",
            "Epoch 00123: val_categorical_accuracy improved from 0.91570 to 0.91810, saving model to Desktop/Trained_models/defensive_distillation_keras_vgg16_cifar10.h5\n",
            "390/390 [==============================] - 167s 429ms/step - loss: 1.8626 - categorical_accuracy: 0.9814 - val_loss: 1.8993 - val_categorical_accuracy: 0.9181 - lr: 0.0118\n",
            "Epoch 124/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.8611 - categorical_accuracy: 0.9805\n",
            "Epoch 00124: val_categorical_accuracy did not improve from 0.91810\n",
            "390/390 [==============================] - 167s 427ms/step - loss: 1.8611 - categorical_accuracy: 0.9805 - val_loss: 1.8992 - val_categorical_accuracy: 0.9052 - lr: 0.0118\n",
            "Epoch 125/125\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.8600 - categorical_accuracy: 0.9788\n",
            "Epoch 00125: val_categorical_accuracy did not improve from 0.91810\n",
            "390/390 [==============================] - 167s 427ms/step - loss: 1.8600 - categorical_accuracy: 0.9788 - val_loss: 1.8992 - val_categorical_accuracy: 0.9065 - lr: 0.0118\n",
            "313/313 [==============================] - 10s 31ms/step - loss: 1.8992 - categorical_accuracy: 0.9065\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.8992424011230469, 0.906499981880188]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize = (9,6))\n",
        "plt.plot(history_teacher.history['val_loss'],label='Test loss')\n",
        "plt.plot(history_teacher.history['loss'],label='Train loss')\n",
        "plt.title('Loss curve for resnet model')\n",
        "plt.ylabel('Loss value')\n",
        "plt.xlabel('No. epoch')\n",
        "plt.show()\n",
        "\n",
        "# Plot history: Accuracy\n",
        "plt.figure(figsize = (9,6))\n",
        "plt.plot(history_teacher.history['val_categorical_accuracy'],label = 'Test accuracy')\n",
        "plt.plot(history_teacher.history['categorical_accuracy'],label = 'Train accuracy')\n",
        "plt.title('Accuracy curve for resnet model')\n",
        "plt.ylabel('Accuracy value (%)')\n",
        "plt.xlabel('No. epoch')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "2L3ki1WWzaSw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 791
        },
        "outputId": "bb42b84f-464b-4014-dadc-79367b51227c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 648x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 648x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "teacher = keras.models.load_model(model_path)"
      ],
      "metadata": {
        "id": "l8deT9rZTP1r"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.backend.clear_session()"
      ],
      "metadata": {
        "id": "HnjhI6SennWm"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "distiller = Distiller(student=student, teacher=teacher)\n",
        "distiller.compile(\n",
        "    optimizer=keras.optimizers.SGD(lr=learning_rate, decay=lr_decay, momentum=0.9, nesterov=True),\n",
        "    metrics=[keras.metrics.CategoricalAccuracy()],\n",
        "    student_loss_fn=keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "    distillation_loss_fn=keras.losses.KLDivergence(),\n",
        "    alpha=0.2,\n",
        "    temperature=200,\n",
        ")\n",
        "callbacks = [ \n",
        "    tf.keras.callbacks.ModelCheckpoint('Desktop/Trained_models/student_keras_vgg16_cifar10.h5', monitor='val_categorical_accuracy', verbose=1, save_best_only=True),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau( factor = 0.7, patience = 5, min_lr = 0.000001, verbose = 1, monitor='val_student_loss'), #patience = 7 and 20 for cifar-100 , patience = 5 and 10 for cifar-10\n",
        "    #tf.keras.callbacks.EarlyStopping(monitor = 'val_accuracy' , patience = 10)\n",
        "  ]\n",
        "# Distill teacher to student\n",
        "history_distiller = distiller.fit_generator(datagen.flow(x_train, y_train,\n",
        "                                  batch_size=batch_size),\n",
        "                                  steps_per_epoch=x_train.shape[0] // batch_size,\n",
        "                                  epochs=150,\n",
        "                                  validation_data=(x_test, y_test),\n",
        "                                  callbacks = callbacks)\n",
        "\n",
        "# Evaluate student on test dataset\n",
        "distiller.evaluate(x_test, y_test)"
      ],
      "metadata": {
        "id": "-u-x9gYOoCW0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "befa12ad-c62f-45f7-ee3e-5c506707e6a6"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.2017 - student_loss: 2.3001 - distillation_loss: 3.4282e-07\n",
            "Epoch 00001: val_categorical_accuracy improved from -inf to 0.11070, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "390/390 [==============================] - 210s 538ms/step - categorical_accuracy: 0.2017 - student_loss: 2.3001 - distillation_loss: 3.4282e-07 - val_categorical_accuracy: 0.1107 - val_student_loss: 2.3028 - lr: 0.1000\n",
            "Epoch 2/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.3120 - student_loss: 2.2980 - distillation_loss: 3.3927e-07\n",
            "Epoch 00002: val_categorical_accuracy improved from 0.11070 to 0.11690, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "390/390 [==============================] - 210s 538ms/step - categorical_accuracy: 0.3120 - student_loss: 2.2980 - distillation_loss: 3.3924e-07 - val_categorical_accuracy: 0.1169 - val_student_loss: 2.3032 - lr: 0.1000\n",
            "Epoch 3/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.3577 - student_loss: 2.2971 - distillation_loss: 3.3865e-07\n",
            "Epoch 00003: val_categorical_accuracy did not improve from 0.11690\n",
            "390/390 [==============================] - 209s 537ms/step - categorical_accuracy: 0.3577 - student_loss: 2.2971 - distillation_loss: 3.3864e-07 - val_categorical_accuracy: 0.1149 - val_student_loss: 2.3036 - lr: 0.1000\n",
            "Epoch 4/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.3876 - student_loss: 2.2964 - distillation_loss: 3.3708e-07\n",
            "Epoch 00004: val_categorical_accuracy did not improve from 0.11690\n",
            "390/390 [==============================] - 209s 537ms/step - categorical_accuracy: 0.3876 - student_loss: 2.2964 - distillation_loss: 3.3712e-07 - val_categorical_accuracy: 0.1151 - val_student_loss: 2.3037 - lr: 0.1000\n",
            "Epoch 5/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.4052 - student_loss: 2.2957 - distillation_loss: 3.3578e-07\n",
            "Epoch 00005: val_categorical_accuracy did not improve from 0.11690\n",
            "390/390 [==============================] - 210s 537ms/step - categorical_accuracy: 0.4052 - student_loss: 2.2957 - distillation_loss: 3.3577e-07 - val_categorical_accuracy: 0.1164 - val_student_loss: 2.3046 - lr: 0.1000\n",
            "Epoch 6/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.4254 - student_loss: 2.2951 - distillation_loss: 3.3481e-07\n",
            "Epoch 00006: val_categorical_accuracy improved from 0.11690 to 0.12180, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.07000000104308128.\n",
            "390/390 [==============================] - 210s 538ms/step - categorical_accuracy: 0.4254 - student_loss: 2.2951 - distillation_loss: 3.3479e-07 - val_categorical_accuracy: 0.1218 - val_student_loss: 2.3037 - lr: 0.1000\n",
            "Epoch 7/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.4378 - student_loss: 2.2946 - distillation_loss: 3.3442e-07\n",
            "Epoch 00007: val_categorical_accuracy did not improve from 0.12180\n",
            "390/390 [==============================] - 209s 537ms/step - categorical_accuracy: 0.4378 - student_loss: 2.2945 - distillation_loss: 3.3439e-07 - val_categorical_accuracy: 0.1157 - val_student_loss: 2.3043 - lr: 0.0700\n",
            "Epoch 8/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.4471 - student_loss: 2.2941 - distillation_loss: 3.3416e-07\n",
            "Epoch 00008: val_categorical_accuracy improved from 0.12180 to 0.12260, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "390/390 [==============================] - 210s 538ms/step - categorical_accuracy: 0.4471 - student_loss: 2.2941 - distillation_loss: 3.3419e-07 - val_categorical_accuracy: 0.1226 - val_student_loss: 2.3035 - lr: 0.0700\n",
            "Epoch 9/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.4512 - student_loss: 2.2937 - distillation_loss: 3.3330e-07\n",
            "Epoch 00009: val_categorical_accuracy improved from 0.12260 to 0.12730, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "390/390 [==============================] - 210s 538ms/step - categorical_accuracy: 0.4512 - student_loss: 2.2937 - distillation_loss: 3.3328e-07 - val_categorical_accuracy: 0.1273 - val_student_loss: 2.3032 - lr: 0.0700\n",
            "Epoch 10/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.4621 - student_loss: 2.2933 - distillation_loss: 3.3283e-07\n",
            "Epoch 00010: val_categorical_accuracy improved from 0.12730 to 0.13680, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "390/390 [==============================] - 210s 538ms/step - categorical_accuracy: 0.4621 - student_loss: 2.2933 - distillation_loss: 3.3287e-07 - val_categorical_accuracy: 0.1368 - val_student_loss: 2.3022 - lr: 0.0700\n",
            "Epoch 11/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.4709 - student_loss: 2.2928 - distillation_loss: 3.3203e-07\n",
            "Epoch 00011: val_categorical_accuracy did not improve from 0.13680\n",
            "390/390 [==============================] - 210s 537ms/step - categorical_accuracy: 0.4709 - student_loss: 2.2928 - distillation_loss: 3.3200e-07 - val_categorical_accuracy: 0.1346 - val_student_loss: 2.3025 - lr: 0.0700\n",
            "Epoch 12/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.4791 - student_loss: 2.2924 - distillation_loss: 3.3183e-07\n",
            "Epoch 00012: val_categorical_accuracy improved from 0.13680 to 0.14960, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "390/390 [==============================] - 210s 538ms/step - categorical_accuracy: 0.4791 - student_loss: 2.2924 - distillation_loss: 3.3186e-07 - val_categorical_accuracy: 0.1496 - val_student_loss: 2.3019 - lr: 0.0700\n",
            "Epoch 13/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.4854 - student_loss: 2.2920 - distillation_loss: 3.3163e-07\n",
            "Epoch 00013: val_categorical_accuracy improved from 0.14960 to 0.15130, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "390/390 [==============================] - 210s 538ms/step - categorical_accuracy: 0.4854 - student_loss: 2.2920 - distillation_loss: 3.3168e-07 - val_categorical_accuracy: 0.1513 - val_student_loss: 2.3009 - lr: 0.0700\n",
            "Epoch 14/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.4907 - student_loss: 2.2915 - distillation_loss: 3.2985e-07\n",
            "Epoch 00014: val_categorical_accuracy improved from 0.15130 to 0.15970, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "390/390 [==============================] - 210s 538ms/step - categorical_accuracy: 0.4907 - student_loss: 2.2915 - distillation_loss: 3.2986e-07 - val_categorical_accuracy: 0.1597 - val_student_loss: 2.3003 - lr: 0.0700\n",
            "Epoch 15/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.4972 - student_loss: 2.2911 - distillation_loss: 3.3002e-07\n",
            "Epoch 00015: val_categorical_accuracy improved from 0.15970 to 0.16300, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "390/390 [==============================] - 210s 538ms/step - categorical_accuracy: 0.4972 - student_loss: 2.2911 - distillation_loss: 3.3000e-07 - val_categorical_accuracy: 0.1630 - val_student_loss: 2.3004 - lr: 0.0700\n",
            "Epoch 16/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.5076 - student_loss: 2.2906 - distillation_loss: 3.2896e-07\n",
            "Epoch 00016: val_categorical_accuracy improved from 0.16300 to 0.16910, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "390/390 [==============================] - 210s 538ms/step - categorical_accuracy: 0.5076 - student_loss: 2.2906 - distillation_loss: 3.2896e-07 - val_categorical_accuracy: 0.1691 - val_student_loss: 2.2999 - lr: 0.0700\n",
            "Epoch 17/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.5116 - student_loss: 2.2901 - distillation_loss: 3.2883e-07\n",
            "Epoch 00017: val_categorical_accuracy improved from 0.16910 to 0.17320, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "390/390 [==============================] - 210s 539ms/step - categorical_accuracy: 0.5116 - student_loss: 2.2901 - distillation_loss: 3.2883e-07 - val_categorical_accuracy: 0.1732 - val_student_loss: 2.2995 - lr: 0.0700\n",
            "Epoch 18/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.5177 - student_loss: 2.2896 - distillation_loss: 3.2798e-07\n",
            "Epoch 00018: val_categorical_accuracy improved from 0.17320 to 0.20110, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "390/390 [==============================] - 210s 538ms/step - categorical_accuracy: 0.5177 - student_loss: 2.2896 - distillation_loss: 3.2799e-07 - val_categorical_accuracy: 0.2011 - val_student_loss: 2.2977 - lr: 0.0700\n",
            "Epoch 19/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.5266 - student_loss: 2.2891 - distillation_loss: 3.2716e-07\n",
            "Epoch 00019: val_categorical_accuracy improved from 0.20110 to 0.21480, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "390/390 [==============================] - 210s 538ms/step - categorical_accuracy: 0.5266 - student_loss: 2.2891 - distillation_loss: 3.2712e-07 - val_categorical_accuracy: 0.2148 - val_student_loss: 2.2972 - lr: 0.0700\n",
            "Epoch 20/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.5301 - student_loss: 2.2886 - distillation_loss: 3.2668e-07\n",
            "Epoch 00020: val_categorical_accuracy improved from 0.21480 to 0.22300, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "390/390 [==============================] - 210s 538ms/step - categorical_accuracy: 0.5301 - student_loss: 2.2886 - distillation_loss: 3.2664e-07 - val_categorical_accuracy: 0.2230 - val_student_loss: 2.2968 - lr: 0.0700\n",
            "Epoch 21/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.5388 - student_loss: 2.2881 - distillation_loss: 3.2630e-07\n",
            "Epoch 00021: val_categorical_accuracy improved from 0.22300 to 0.23090, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "390/390 [==============================] - 210s 538ms/step - categorical_accuracy: 0.5388 - student_loss: 2.2881 - distillation_loss: 3.2631e-07 - val_categorical_accuracy: 0.2309 - val_student_loss: 2.2969 - lr: 0.0700\n",
            "Epoch 22/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.5410 - student_loss: 2.2876 - distillation_loss: 3.2517e-07\n",
            "Epoch 00022: val_categorical_accuracy improved from 0.23090 to 0.26180, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "390/390 [==============================] - 210s 538ms/step - categorical_accuracy: 0.5410 - student_loss: 2.2876 - distillation_loss: 3.2518e-07 - val_categorical_accuracy: 0.2618 - val_student_loss: 2.2948 - lr: 0.0700\n",
            "Epoch 23/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.5503 - student_loss: 2.2870 - distillation_loss: 3.2490e-07\n",
            "Epoch 00023: val_categorical_accuracy improved from 0.26180 to 0.26810, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "390/390 [==============================] - 210s 539ms/step - categorical_accuracy: 0.5503 - student_loss: 2.2870 - distillation_loss: 3.2488e-07 - val_categorical_accuracy: 0.2681 - val_student_loss: 2.2942 - lr: 0.0700\n",
            "Epoch 24/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.5539 - student_loss: 2.2865 - distillation_loss: 3.2400e-07\n",
            "Epoch 00024: val_categorical_accuracy improved from 0.26810 to 0.27340, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "390/390 [==============================] - 210s 539ms/step - categorical_accuracy: 0.5539 - student_loss: 2.2865 - distillation_loss: 3.2400e-07 - val_categorical_accuracy: 0.2734 - val_student_loss: 2.2945 - lr: 0.0700\n",
            "Epoch 25/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.5579 - student_loss: 2.2860 - distillation_loss: 3.2379e-07\n",
            "Epoch 00025: val_categorical_accuracy did not improve from 0.27340\n",
            "390/390 [==============================] - 209s 537ms/step - categorical_accuracy: 0.5579 - student_loss: 2.2860 - distillation_loss: 3.2375e-07 - val_categorical_accuracy: 0.2704 - val_student_loss: 2.2940 - lr: 0.0700\n",
            "Epoch 26/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.5681 - student_loss: 2.2854 - distillation_loss: 3.2247e-07\n",
            "Epoch 00026: val_categorical_accuracy did not improve from 0.27340\n",
            "390/390 [==============================] - 209s 537ms/step - categorical_accuracy: 0.5681 - student_loss: 2.2854 - distillation_loss: 3.2250e-07 - val_categorical_accuracy: 0.2713 - val_student_loss: 2.2947 - lr: 0.0700\n",
            "Epoch 27/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.5704 - student_loss: 2.2849 - distillation_loss: 3.2260e-07\n",
            "Epoch 00027: val_categorical_accuracy improved from 0.27340 to 0.32270, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "390/390 [==============================] - 210s 538ms/step - categorical_accuracy: 0.5704 - student_loss: 2.2849 - distillation_loss: 3.2258e-07 - val_categorical_accuracy: 0.3227 - val_student_loss: 2.2918 - lr: 0.0700\n",
            "Epoch 28/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.5768 - student_loss: 2.2843 - distillation_loss: 3.2127e-07\n",
            "Epoch 00028: val_categorical_accuracy did not improve from 0.32270\n",
            "390/390 [==============================] - 209s 537ms/step - categorical_accuracy: 0.5768 - student_loss: 2.2843 - distillation_loss: 3.2132e-07 - val_categorical_accuracy: 0.3117 - val_student_loss: 2.2919 - lr: 0.0700\n",
            "Epoch 29/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.5828 - student_loss: 2.2837 - distillation_loss: 3.2060e-07\n",
            "Epoch 00029: val_categorical_accuracy improved from 0.32270 to 0.33330, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "390/390 [==============================] - 210s 539ms/step - categorical_accuracy: 0.5828 - student_loss: 2.2837 - distillation_loss: 3.2057e-07 - val_categorical_accuracy: 0.3333 - val_student_loss: 2.2917 - lr: 0.0700\n",
            "Epoch 30/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.5908 - student_loss: 2.2831 - distillation_loss: 3.1989e-07\n",
            "Epoch 00030: val_categorical_accuracy did not improve from 0.33330\n",
            "390/390 [==============================] - 209s 537ms/step - categorical_accuracy: 0.5908 - student_loss: 2.2831 - distillation_loss: 3.1992e-07 - val_categorical_accuracy: 0.3329 - val_student_loss: 2.2900 - lr: 0.0700\n",
            "Epoch 31/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.5940 - student_loss: 2.2825 - distillation_loss: 3.1944e-07\n",
            "Epoch 00031: val_categorical_accuracy improved from 0.33330 to 0.37820, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "390/390 [==============================] - 210s 538ms/step - categorical_accuracy: 0.5940 - student_loss: 2.2825 - distillation_loss: 3.1942e-07 - val_categorical_accuracy: 0.3782 - val_student_loss: 2.2893 - lr: 0.0700\n",
            "Epoch 32/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.5993 - student_loss: 2.2819 - distillation_loss: 3.1820e-07\n",
            "Epoch 00032: val_categorical_accuracy did not improve from 0.37820\n",
            "390/390 [==============================] - 209s 536ms/step - categorical_accuracy: 0.5993 - student_loss: 2.2819 - distillation_loss: 3.1823e-07 - val_categorical_accuracy: 0.3583 - val_student_loss: 2.2896 - lr: 0.0700\n",
            "Epoch 33/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.6046 - student_loss: 2.2813 - distillation_loss: 3.1706e-07\n",
            "Epoch 00033: val_categorical_accuracy did not improve from 0.37820\n",
            "390/390 [==============================] - 209s 537ms/step - categorical_accuracy: 0.6046 - student_loss: 2.2813 - distillation_loss: 3.1705e-07 - val_categorical_accuracy: 0.3662 - val_student_loss: 2.2896 - lr: 0.0700\n",
            "Epoch 34/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.6097 - student_loss: 2.2806 - distillation_loss: 3.1625e-07\n",
            "Epoch 00034: val_categorical_accuracy did not improve from 0.37820\n",
            "390/390 [==============================] - 209s 537ms/step - categorical_accuracy: 0.6097 - student_loss: 2.2806 - distillation_loss: 3.1627e-07 - val_categorical_accuracy: 0.3266 - val_student_loss: 2.2919 - lr: 0.0700\n",
            "Epoch 35/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.6153 - student_loss: 2.2800 - distillation_loss: 3.1590e-07\n",
            "Epoch 00035: val_categorical_accuracy improved from 0.37820 to 0.42650, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "390/390 [==============================] - 210s 538ms/step - categorical_accuracy: 0.6153 - student_loss: 2.2800 - distillation_loss: 3.1591e-07 - val_categorical_accuracy: 0.4265 - val_student_loss: 2.2868 - lr: 0.0700\n",
            "Epoch 36/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.6210 - student_loss: 2.2794 - distillation_loss: 3.1509e-07\n",
            "Epoch 00036: val_categorical_accuracy did not improve from 0.42650\n",
            "390/390 [==============================] - 209s 536ms/step - categorical_accuracy: 0.6210 - student_loss: 2.2794 - distillation_loss: 3.1511e-07 - val_categorical_accuracy: 0.4236 - val_student_loss: 2.2866 - lr: 0.0700\n",
            "Epoch 37/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.6231 - student_loss: 2.2788 - distillation_loss: 3.1424e-07\n",
            "Epoch 00037: val_categorical_accuracy did not improve from 0.42650\n",
            "390/390 [==============================] - 209s 537ms/step - categorical_accuracy: 0.6231 - student_loss: 2.2788 - distillation_loss: 3.1423e-07 - val_categorical_accuracy: 0.4137 - val_student_loss: 2.2871 - lr: 0.0700\n",
            "Epoch 38/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.6306 - student_loss: 2.2781 - distillation_loss: 3.1345e-07\n",
            "Epoch 00038: val_categorical_accuracy did not improve from 0.42650\n",
            "390/390 [==============================] - 209s 537ms/step - categorical_accuracy: 0.6306 - student_loss: 2.2781 - distillation_loss: 3.1345e-07 - val_categorical_accuracy: 0.4138 - val_student_loss: 2.2860 - lr: 0.0700\n",
            "Epoch 39/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.6344 - student_loss: 2.2774 - distillation_loss: 3.1270e-07\n",
            "Epoch 00039: val_categorical_accuracy did not improve from 0.42650\n",
            "390/390 [==============================] - 209s 537ms/step - categorical_accuracy: 0.6344 - student_loss: 2.2774 - distillation_loss: 3.1270e-07 - val_categorical_accuracy: 0.4053 - val_student_loss: 2.2879 - lr: 0.0700\n",
            "Epoch 40/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.6378 - student_loss: 2.2768 - distillation_loss: 3.1176e-07\n",
            "Epoch 00040: val_categorical_accuracy improved from 0.42650 to 0.44210, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "390/390 [==============================] - 210s 538ms/step - categorical_accuracy: 0.6378 - student_loss: 2.2768 - distillation_loss: 3.1175e-07 - val_categorical_accuracy: 0.4421 - val_student_loss: 2.2855 - lr: 0.0700\n",
            "Epoch 41/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.6460 - student_loss: 2.2761 - distillation_loss: 3.1029e-07\n",
            "Epoch 00041: val_categorical_accuracy improved from 0.44210 to 0.46810, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "390/390 [==============================] - 210s 539ms/step - categorical_accuracy: 0.6460 - student_loss: 2.2761 - distillation_loss: 3.1028e-07 - val_categorical_accuracy: 0.4681 - val_student_loss: 2.2831 - lr: 0.0700\n",
            "Epoch 42/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.6500 - student_loss: 2.2754 - distillation_loss: 3.0959e-07\n",
            "Epoch 00042: val_categorical_accuracy did not improve from 0.46810\n",
            "390/390 [==============================] - 210s 537ms/step - categorical_accuracy: 0.6500 - student_loss: 2.2754 - distillation_loss: 3.0955e-07 - val_categorical_accuracy: 0.4586 - val_student_loss: 2.2833 - lr: 0.0700\n",
            "Epoch 43/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.6547 - student_loss: 2.2747 - distillation_loss: 3.0882e-07\n",
            "Epoch 00043: val_categorical_accuracy did not improve from 0.46810\n",
            "390/390 [==============================] - 209s 537ms/step - categorical_accuracy: 0.6547 - student_loss: 2.2747 - distillation_loss: 3.0881e-07 - val_categorical_accuracy: 0.4416 - val_student_loss: 2.2840 - lr: 0.0700\n",
            "Epoch 44/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.6569 - student_loss: 2.2741 - distillation_loss: 3.0808e-07\n",
            "Epoch 00044: val_categorical_accuracy did not improve from 0.46810\n",
            "390/390 [==============================] - 209s 537ms/step - categorical_accuracy: 0.6569 - student_loss: 2.2741 - distillation_loss: 3.0804e-07 - val_categorical_accuracy: 0.4453 - val_student_loss: 2.2839 - lr: 0.0700\n",
            "Epoch 45/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.6620 - student_loss: 2.2734 - distillation_loss: 3.0718e-07\n",
            "Epoch 00045: val_categorical_accuracy did not improve from 0.46810\n",
            "390/390 [==============================] - 209s 537ms/step - categorical_accuracy: 0.6620 - student_loss: 2.2734 - distillation_loss: 3.0717e-07 - val_categorical_accuracy: 0.4577 - val_student_loss: 2.2815 - lr: 0.0700\n",
            "Epoch 46/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.6656 - student_loss: 2.2727 - distillation_loss: 3.0644e-07\n",
            "Epoch 00046: val_categorical_accuracy did not improve from 0.46810\n",
            "390/390 [==============================] - 209s 537ms/step - categorical_accuracy: 0.6656 - student_loss: 2.2727 - distillation_loss: 3.0646e-07 - val_categorical_accuracy: 0.4646 - val_student_loss: 2.2799 - lr: 0.0700\n",
            "Epoch 47/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.6712 - student_loss: 2.2720 - distillation_loss: 3.0507e-07\n",
            "Epoch 00047: val_categorical_accuracy improved from 0.46810 to 0.46920, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "390/390 [==============================] - 210s 539ms/step - categorical_accuracy: 0.6712 - student_loss: 2.2720 - distillation_loss: 3.0507e-07 - val_categorical_accuracy: 0.4692 - val_student_loss: 2.2811 - lr: 0.0700\n",
            "Epoch 48/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.6734 - student_loss: 2.2713 - distillation_loss: 3.0457e-07\n",
            "Epoch 00048: val_categorical_accuracy improved from 0.46920 to 0.48900, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "390/390 [==============================] - 210s 538ms/step - categorical_accuracy: 0.6734 - student_loss: 2.2713 - distillation_loss: 3.0459e-07 - val_categorical_accuracy: 0.4890 - val_student_loss: 2.2782 - lr: 0.0700\n",
            "Epoch 49/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.6798 - student_loss: 2.2705 - distillation_loss: 3.0341e-07\n",
            "Epoch 00049: val_categorical_accuracy did not improve from 0.48900\n",
            "390/390 [==============================] - 209s 537ms/step - categorical_accuracy: 0.6798 - student_loss: 2.2705 - distillation_loss: 3.0341e-07 - val_categorical_accuracy: 0.4860 - val_student_loss: 2.2764 - lr: 0.0700\n",
            "Epoch 50/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.6822 - student_loss: 2.2699 - distillation_loss: 3.0279e-07\n",
            "Epoch 00050: val_categorical_accuracy did not improve from 0.48900\n",
            "390/390 [==============================] - 209s 537ms/step - categorical_accuracy: 0.6822 - student_loss: 2.2699 - distillation_loss: 3.0281e-07 - val_categorical_accuracy: 0.4852 - val_student_loss: 2.2775 - lr: 0.0700\n",
            "Epoch 51/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.6857 - student_loss: 2.2692 - distillation_loss: 3.0221e-07\n",
            "Epoch 00051: val_categorical_accuracy improved from 0.48900 to 0.49160, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "390/390 [==============================] - 210s 538ms/step - categorical_accuracy: 0.6857 - student_loss: 2.2692 - distillation_loss: 3.0222e-07 - val_categorical_accuracy: 0.4916 - val_student_loss: 2.2774 - lr: 0.0700\n",
            "Epoch 52/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.6901 - student_loss: 2.2684 - distillation_loss: 3.0092e-07\n",
            "Epoch 00052: val_categorical_accuracy did not improve from 0.49160\n",
            "390/390 [==============================] - 209s 537ms/step - categorical_accuracy: 0.6901 - student_loss: 2.2684 - distillation_loss: 3.0090e-07 - val_categorical_accuracy: 0.4735 - val_student_loss: 2.2763 - lr: 0.0700\n",
            "Epoch 53/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.6931 - student_loss: 2.2677 - distillation_loss: 2.9984e-07\n",
            "Epoch 00053: val_categorical_accuracy did not improve from 0.49160\n",
            "390/390 [==============================] - 210s 537ms/step - categorical_accuracy: 0.6931 - student_loss: 2.2677 - distillation_loss: 2.9985e-07 - val_categorical_accuracy: 0.4612 - val_student_loss: 2.2761 - lr: 0.0700\n",
            "Epoch 54/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.6982 - student_loss: 2.2669 - distillation_loss: 2.9960e-07\n",
            "Epoch 00054: val_categorical_accuracy did not improve from 0.49160\n",
            "390/390 [==============================] - 210s 538ms/step - categorical_accuracy: 0.6982 - student_loss: 2.2669 - distillation_loss: 2.9960e-07 - val_categorical_accuracy: 0.4799 - val_student_loss: 2.2727 - lr: 0.0700\n",
            "Epoch 55/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.7001 - student_loss: 2.2662 - distillation_loss: 2.9800e-07\n",
            "Epoch 00055: val_categorical_accuracy improved from 0.49160 to 0.53130, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "390/390 [==============================] - 210s 538ms/step - categorical_accuracy: 0.7001 - student_loss: 2.2662 - distillation_loss: 2.9801e-07 - val_categorical_accuracy: 0.5313 - val_student_loss: 2.2740 - lr: 0.0700\n",
            "Epoch 56/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.7039 - student_loss: 2.2655 - distillation_loss: 2.9745e-07\n",
            "Epoch 00056: val_categorical_accuracy did not improve from 0.53130\n",
            "390/390 [==============================] - 209s 537ms/step - categorical_accuracy: 0.7039 - student_loss: 2.2655 - distillation_loss: 2.9744e-07 - val_categorical_accuracy: 0.4738 - val_student_loss: 2.2745 - lr: 0.0700\n",
            "Epoch 57/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.7067 - student_loss: 2.2648 - distillation_loss: 2.9585e-07\n",
            "Epoch 00057: val_categorical_accuracy did not improve from 0.53130\n",
            "390/390 [==============================] - 209s 537ms/step - categorical_accuracy: 0.7067 - student_loss: 2.2648 - distillation_loss: 2.9584e-07 - val_categorical_accuracy: 0.5045 - val_student_loss: 2.2692 - lr: 0.0700\n",
            "Epoch 58/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.7082 - student_loss: 2.2641 - distillation_loss: 2.9527e-07\n",
            "Epoch 00058: val_categorical_accuracy improved from 0.53130 to 0.53270, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "390/390 [==============================] - 210s 538ms/step - categorical_accuracy: 0.7082 - student_loss: 2.2641 - distillation_loss: 2.9524e-07 - val_categorical_accuracy: 0.5327 - val_student_loss: 2.2701 - lr: 0.0700\n",
            "Epoch 59/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.7130 - student_loss: 2.2633 - distillation_loss: 2.9422e-07\n",
            "Epoch 00059: val_categorical_accuracy improved from 0.53270 to 0.53570, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "390/390 [==============================] - 210s 538ms/step - categorical_accuracy: 0.7130 - student_loss: 2.2633 - distillation_loss: 2.9421e-07 - val_categorical_accuracy: 0.5357 - val_student_loss: 2.2673 - lr: 0.0700\n",
            "Epoch 60/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.7168 - student_loss: 2.2625 - distillation_loss: 2.9399e-07\n",
            "Epoch 00060: val_categorical_accuracy did not improve from 0.53570\n",
            "390/390 [==============================] - 209s 537ms/step - categorical_accuracy: 0.7168 - student_loss: 2.2625 - distillation_loss: 2.9400e-07 - val_categorical_accuracy: 0.5237 - val_student_loss: 2.2703 - lr: 0.0700\n",
            "Epoch 61/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.7192 - student_loss: 2.2618 - distillation_loss: 2.9250e-07\n",
            "Epoch 00061: val_categorical_accuracy improved from 0.53570 to 0.53800, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "390/390 [==============================] - 210s 538ms/step - categorical_accuracy: 0.7192 - student_loss: 2.2618 - distillation_loss: 2.9248e-07 - val_categorical_accuracy: 0.5380 - val_student_loss: 2.2677 - lr: 0.0700\n",
            "Epoch 62/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.7230 - student_loss: 2.2610 - distillation_loss: 2.9125e-07\n",
            "Epoch 00062: val_categorical_accuracy did not improve from 0.53800\n",
            "390/390 [==============================] - 210s 537ms/step - categorical_accuracy: 0.7230 - student_loss: 2.2610 - distillation_loss: 2.9123e-07 - val_categorical_accuracy: 0.4874 - val_student_loss: 2.2743 - lr: 0.0700\n",
            "Epoch 63/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.7264 - student_loss: 2.2602 - distillation_loss: 2.9091e-07\n",
            "Epoch 00063: val_categorical_accuracy did not improve from 0.53800\n",
            "390/390 [==============================] - 210s 537ms/step - categorical_accuracy: 0.7264 - student_loss: 2.2602 - distillation_loss: 2.9092e-07 - val_categorical_accuracy: 0.5170 - val_student_loss: 2.2671 - lr: 0.0700\n",
            "Epoch 64/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.7304 - student_loss: 2.2594 - distillation_loss: 2.9003e-07\n",
            "Epoch 00064: val_categorical_accuracy did not improve from 0.53800\n",
            "390/390 [==============================] - 209s 537ms/step - categorical_accuracy: 0.7304 - student_loss: 2.2594 - distillation_loss: 2.9005e-07 - val_categorical_accuracy: 0.4970 - val_student_loss: 2.2677 - lr: 0.0700\n",
            "Epoch 65/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.7315 - student_loss: 2.2586 - distillation_loss: 2.8850e-07\n",
            "Epoch 00065: val_categorical_accuracy did not improve from 0.53800\n",
            "390/390 [==============================] - 209s 537ms/step - categorical_accuracy: 0.7315 - student_loss: 2.2586 - distillation_loss: 2.8849e-07 - val_categorical_accuracy: 0.5165 - val_student_loss: 2.2718 - lr: 0.0700\n",
            "Epoch 66/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.7362 - student_loss: 2.2578 - distillation_loss: 2.8756e-07\n",
            "Epoch 00066: val_categorical_accuracy improved from 0.53800 to 0.57150, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "390/390 [==============================] - 210s 539ms/step - categorical_accuracy: 0.7362 - student_loss: 2.2578 - distillation_loss: 2.8755e-07 - val_categorical_accuracy: 0.5715 - val_student_loss: 2.2627 - lr: 0.0700\n",
            "Epoch 67/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.7374 - student_loss: 2.2571 - distillation_loss: 2.8676e-07\n",
            "Epoch 00067: val_categorical_accuracy did not improve from 0.57150\n",
            "390/390 [==============================] - 209s 537ms/step - categorical_accuracy: 0.7374 - student_loss: 2.2571 - distillation_loss: 2.8676e-07 - val_categorical_accuracy: 0.5334 - val_student_loss: 2.2632 - lr: 0.0700\n",
            "Epoch 68/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.7411 - student_loss: 2.2563 - distillation_loss: 2.8597e-07\n",
            "Epoch 00068: val_categorical_accuracy did not improve from 0.57150\n",
            "390/390 [==============================] - 209s 537ms/step - categorical_accuracy: 0.7411 - student_loss: 2.2563 - distillation_loss: 2.8600e-07 - val_categorical_accuracy: 0.5386 - val_student_loss: 2.2675 - lr: 0.0700\n",
            "Epoch 69/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.7425 - student_loss: 2.2556 - distillation_loss: 2.8480e-07\n",
            "Epoch 00069: val_categorical_accuracy did not improve from 0.57150\n",
            "390/390 [==============================] - 209s 537ms/step - categorical_accuracy: 0.7425 - student_loss: 2.2556 - distillation_loss: 2.8483e-07 - val_categorical_accuracy: 0.5541 - val_student_loss: 2.2638 - lr: 0.0700\n",
            "Epoch 70/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.7459 - student_loss: 2.2548 - distillation_loss: 2.8396e-07\n",
            "Epoch 00070: val_categorical_accuracy did not improve from 0.57150\n",
            "390/390 [==============================] - 209s 537ms/step - categorical_accuracy: 0.7459 - student_loss: 2.2548 - distillation_loss: 2.8397e-07 - val_categorical_accuracy: 0.5355 - val_student_loss: 2.2616 - lr: 0.0700\n",
            "Epoch 71/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.7496 - student_loss: 2.2540 - distillation_loss: 2.8296e-07\n",
            "Epoch 00071: val_categorical_accuracy did not improve from 0.57150\n",
            "390/390 [==============================] - 209s 537ms/step - categorical_accuracy: 0.7496 - student_loss: 2.2540 - distillation_loss: 2.8299e-07 - val_categorical_accuracy: 0.5640 - val_student_loss: 2.2599 - lr: 0.0700\n",
            "Epoch 72/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.7502 - student_loss: 2.2531 - distillation_loss: 2.8178e-07\n",
            "Epoch 00072: val_categorical_accuracy did not improve from 0.57150\n",
            "390/390 [==============================] - 210s 538ms/step - categorical_accuracy: 0.7502 - student_loss: 2.2531 - distillation_loss: 2.8174e-07 - val_categorical_accuracy: 0.5402 - val_student_loss: 2.2600 - lr: 0.0700\n",
            "Epoch 73/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.7560 - student_loss: 2.2523 - distillation_loss: 2.8108e-07\n",
            "Epoch 00073: val_categorical_accuracy improved from 0.57150 to 0.57670, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "390/390 [==============================] - 210s 538ms/step - categorical_accuracy: 0.7560 - student_loss: 2.2523 - distillation_loss: 2.8108e-07 - val_categorical_accuracy: 0.5767 - val_student_loss: 2.2591 - lr: 0.0700\n",
            "Epoch 74/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.7581 - student_loss: 2.2515 - distillation_loss: 2.7994e-07\n",
            "Epoch 00074: val_categorical_accuracy improved from 0.57670 to 0.64600, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "390/390 [==============================] - 210s 538ms/step - categorical_accuracy: 0.7581 - student_loss: 2.2515 - distillation_loss: 2.7997e-07 - val_categorical_accuracy: 0.6460 - val_student_loss: 2.2514 - lr: 0.0700\n",
            "Epoch 75/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.7603 - student_loss: 2.2507 - distillation_loss: 2.7904e-07\n",
            "Epoch 00075: val_categorical_accuracy did not improve from 0.64600\n",
            "390/390 [==============================] - 209s 537ms/step - categorical_accuracy: 0.7603 - student_loss: 2.2507 - distillation_loss: 2.7906e-07 - val_categorical_accuracy: 0.6102 - val_student_loss: 2.2525 - lr: 0.0700\n",
            "Epoch 76/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.7609 - student_loss: 2.2499 - distillation_loss: 2.7777e-07\n",
            "Epoch 00076: val_categorical_accuracy did not improve from 0.64600\n",
            "390/390 [==============================] - 209s 537ms/step - categorical_accuracy: 0.7609 - student_loss: 2.2499 - distillation_loss: 2.7775e-07 - val_categorical_accuracy: 0.5651 - val_student_loss: 2.2596 - lr: 0.0700\n",
            "Epoch 77/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.7635 - student_loss: 2.2492 - distillation_loss: 2.7711e-07\n",
            "Epoch 00077: val_categorical_accuracy did not improve from 0.64600\n",
            "390/390 [==============================] - 209s 537ms/step - categorical_accuracy: 0.7635 - student_loss: 2.2492 - distillation_loss: 2.7714e-07 - val_categorical_accuracy: 0.6340 - val_student_loss: 2.2573 - lr: 0.0700\n",
            "Epoch 78/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.7652 - student_loss: 2.2483 - distillation_loss: 2.7619e-07\n",
            "Epoch 00078: val_categorical_accuracy did not improve from 0.64600\n",
            "390/390 [==============================] - 210s 538ms/step - categorical_accuracy: 0.7652 - student_loss: 2.2483 - distillation_loss: 2.7618e-07 - val_categorical_accuracy: 0.6313 - val_student_loss: 2.2476 - lr: 0.0700\n",
            "Epoch 79/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.7680 - student_loss: 2.2475 - distillation_loss: 2.7499e-07\n",
            "Epoch 00079: val_categorical_accuracy did not improve from 0.64600\n",
            "390/390 [==============================] - 210s 537ms/step - categorical_accuracy: 0.7680 - student_loss: 2.2475 - distillation_loss: 2.7492e-07 - val_categorical_accuracy: 0.6313 - val_student_loss: 2.2532 - lr: 0.0700\n",
            "Epoch 80/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.7712 - student_loss: 2.2466 - distillation_loss: 2.7370e-07\n",
            "Epoch 00080: val_categorical_accuracy did not improve from 0.64600\n",
            "390/390 [==============================] - 209s 537ms/step - categorical_accuracy: 0.7712 - student_loss: 2.2466 - distillation_loss: 2.7370e-07 - val_categorical_accuracy: 0.5916 - val_student_loss: 2.2553 - lr: 0.0700\n",
            "Epoch 81/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.7744 - student_loss: 2.2458 - distillation_loss: 2.7347e-07\n",
            "Epoch 00081: val_categorical_accuracy did not improve from 0.64600\n",
            "390/390 [==============================] - 209s 537ms/step - categorical_accuracy: 0.7744 - student_loss: 2.2458 - distillation_loss: 2.7349e-07 - val_categorical_accuracy: 0.5924 - val_student_loss: 2.2506 - lr: 0.0700\n",
            "Epoch 82/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.7776 - student_loss: 2.2449 - distillation_loss: 2.7163e-07\n",
            "Epoch 00082: val_categorical_accuracy improved from 0.64600 to 0.67800, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "390/390 [==============================] - 210s 538ms/step - categorical_accuracy: 0.7776 - student_loss: 2.2449 - distillation_loss: 2.7163e-07 - val_categorical_accuracy: 0.6780 - val_student_loss: 2.2464 - lr: 0.0700\n",
            "Epoch 83/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.7809 - student_loss: 2.2441 - distillation_loss: 2.7059e-07\n",
            "Epoch 00083: val_categorical_accuracy did not improve from 0.67800\n",
            "390/390 [==============================] - 209s 537ms/step - categorical_accuracy: 0.7809 - student_loss: 2.2441 - distillation_loss: 2.7065e-07 - val_categorical_accuracy: 0.6296 - val_student_loss: 2.2476 - lr: 0.0700\n",
            "Epoch 84/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.7824 - student_loss: 2.2433 - distillation_loss: 2.6920e-07\n",
            "Epoch 00084: val_categorical_accuracy did not improve from 0.67800\n",
            "390/390 [==============================] - 210s 537ms/step - categorical_accuracy: 0.7824 - student_loss: 2.2433 - distillation_loss: 2.6922e-07 - val_categorical_accuracy: 0.6277 - val_student_loss: 2.2433 - lr: 0.0700\n",
            "Epoch 85/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.7831 - student_loss: 2.2424 - distillation_loss: 2.6863e-07\n",
            "Epoch 00085: val_categorical_accuracy did not improve from 0.67800\n",
            "390/390 [==============================] - 209s 537ms/step - categorical_accuracy: 0.7831 - student_loss: 2.2424 - distillation_loss: 2.6861e-07 - val_categorical_accuracy: 0.6681 - val_student_loss: 2.2444 - lr: 0.0700\n",
            "Epoch 86/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.7870 - student_loss: 2.2415 - distillation_loss: 2.6767e-07\n",
            "Epoch 00086: val_categorical_accuracy improved from 0.67800 to 0.68120, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "390/390 [==============================] - 210s 538ms/step - categorical_accuracy: 0.7870 - student_loss: 2.2415 - distillation_loss: 2.6766e-07 - val_categorical_accuracy: 0.6812 - val_student_loss: 2.2435 - lr: 0.0700\n",
            "Epoch 87/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.7875 - student_loss: 2.2407 - distillation_loss: 2.6656e-07\n",
            "Epoch 00087: val_categorical_accuracy did not improve from 0.68120\n",
            "390/390 [==============================] - 209s 537ms/step - categorical_accuracy: 0.7875 - student_loss: 2.2407 - distillation_loss: 2.6656e-07 - val_categorical_accuracy: 0.6398 - val_student_loss: 2.2473 - lr: 0.0700\n",
            "Epoch 88/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.7905 - student_loss: 2.2399 - distillation_loss: 2.6535e-07\n",
            "Epoch 00088: val_categorical_accuracy improved from 0.68120 to 0.68320, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "390/390 [==============================] - 210s 538ms/step - categorical_accuracy: 0.7905 - student_loss: 2.2399 - distillation_loss: 2.6530e-07 - val_categorical_accuracy: 0.6832 - val_student_loss: 2.2406 - lr: 0.0700\n",
            "Epoch 89/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.7938 - student_loss: 2.2389 - distillation_loss: 2.6422e-07\n",
            "Epoch 00089: val_categorical_accuracy improved from 0.68320 to 0.69080, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "390/390 [==============================] - 210s 538ms/step - categorical_accuracy: 0.7938 - student_loss: 2.2389 - distillation_loss: 2.6421e-07 - val_categorical_accuracy: 0.6908 - val_student_loss: 2.2349 - lr: 0.0700\n",
            "Epoch 90/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.7944 - student_loss: 2.2381 - distillation_loss: 2.6394e-07\n",
            "Epoch 00090: val_categorical_accuracy did not improve from 0.69080\n",
            "390/390 [==============================] - 210s 537ms/step - categorical_accuracy: 0.7944 - student_loss: 2.2381 - distillation_loss: 2.6393e-07 - val_categorical_accuracy: 0.6367 - val_student_loss: 2.2449 - lr: 0.0700\n",
            "Epoch 91/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.7968 - student_loss: 2.2372 - distillation_loss: 2.6243e-07\n",
            "Epoch 00091: val_categorical_accuracy did not improve from 0.69080\n",
            "390/390 [==============================] - 210s 537ms/step - categorical_accuracy: 0.7968 - student_loss: 2.2372 - distillation_loss: 2.6242e-07 - val_categorical_accuracy: 0.6307 - val_student_loss: 2.2403 - lr: 0.0700\n",
            "Epoch 92/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.8009 - student_loss: 2.2363 - distillation_loss: 2.6166e-07\n",
            "Epoch 00092: val_categorical_accuracy improved from 0.69080 to 0.70860, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "390/390 [==============================] - 210s 538ms/step - categorical_accuracy: 0.8009 - student_loss: 2.2363 - distillation_loss: 2.6169e-07 - val_categorical_accuracy: 0.7086 - val_student_loss: 2.2372 - lr: 0.0700\n",
            "Epoch 93/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.8032 - student_loss: 2.2354 - distillation_loss: 2.6054e-07\n",
            "Epoch 00093: val_categorical_accuracy improved from 0.70860 to 0.72540, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "390/390 [==============================] - 210s 538ms/step - categorical_accuracy: 0.8032 - student_loss: 2.2354 - distillation_loss: 2.6054e-07 - val_categorical_accuracy: 0.7254 - val_student_loss: 2.2345 - lr: 0.0700\n",
            "Epoch 94/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.8050 - student_loss: 2.2345 - distillation_loss: 2.5965e-07\n",
            "Epoch 00094: val_categorical_accuracy did not improve from 0.72540\n",
            "390/390 [==============================] - 209s 537ms/step - categorical_accuracy: 0.8050 - student_loss: 2.2345 - distillation_loss: 2.5967e-07 - val_categorical_accuracy: 0.7223 - val_student_loss: 2.2316 - lr: 0.0700\n",
            "Epoch 95/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.8084 - student_loss: 2.2336 - distillation_loss: 2.5837e-07\n",
            "Epoch 00095: val_categorical_accuracy did not improve from 0.72540\n",
            "390/390 [==============================] - 209s 537ms/step - categorical_accuracy: 0.8084 - student_loss: 2.2336 - distillation_loss: 2.5835e-07 - val_categorical_accuracy: 0.7018 - val_student_loss: 2.2376 - lr: 0.0700\n",
            "Epoch 96/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.8082 - student_loss: 2.2328 - distillation_loss: 2.5672e-07\n",
            "Epoch 00096: val_categorical_accuracy did not improve from 0.72540\n",
            "390/390 [==============================] - 209s 537ms/step - categorical_accuracy: 0.8082 - student_loss: 2.2328 - distillation_loss: 2.5673e-07 - val_categorical_accuracy: 0.6933 - val_student_loss: 2.2316 - lr: 0.0700\n",
            "Epoch 97/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.8135 - student_loss: 2.2318 - distillation_loss: 2.5628e-07\n",
            "Epoch 00097: val_categorical_accuracy did not improve from 0.72540\n",
            "390/390 [==============================] - 209s 537ms/step - categorical_accuracy: 0.8135 - student_loss: 2.2318 - distillation_loss: 2.5627e-07 - val_categorical_accuracy: 0.6720 - val_student_loss: 2.2345 - lr: 0.0700\n",
            "Epoch 98/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.8133 - student_loss: 2.2310 - distillation_loss: 2.5505e-07\n",
            "Epoch 00098: val_categorical_accuracy did not improve from 0.72540\n",
            "390/390 [==============================] - 209s 537ms/step - categorical_accuracy: 0.8133 - student_loss: 2.2310 - distillation_loss: 2.5506e-07 - val_categorical_accuracy: 0.7127 - val_student_loss: 2.2314 - lr: 0.0700\n",
            "Epoch 99/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.8153 - student_loss: 2.2301 - distillation_loss: 2.5442e-07\n",
            "Epoch 00099: val_categorical_accuracy did not improve from 0.72540\n",
            "390/390 [==============================] - 209s 536ms/step - categorical_accuracy: 0.8153 - student_loss: 2.2301 - distillation_loss: 2.5442e-07 - val_categorical_accuracy: 0.7033 - val_student_loss: 2.2334 - lr: 0.0700\n",
            "Epoch 100/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.8179 - student_loss: 2.2292 - distillation_loss: 2.5311e-07\n",
            "Epoch 00100: val_categorical_accuracy improved from 0.72540 to 0.74670, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "390/390 [==============================] - 210s 538ms/step - categorical_accuracy: 0.8179 - student_loss: 2.2292 - distillation_loss: 2.5310e-07 - val_categorical_accuracy: 0.7467 - val_student_loss: 2.2302 - lr: 0.0700\n",
            "Epoch 101/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.8184 - student_loss: 2.2283 - distillation_loss: 2.5203e-07\n",
            "Epoch 00101: val_categorical_accuracy did not improve from 0.74670\n",
            "390/390 [==============================] - 209s 537ms/step - categorical_accuracy: 0.8184 - student_loss: 2.2283 - distillation_loss: 2.5206e-07 - val_categorical_accuracy: 0.7069 - val_student_loss: 2.2230 - lr: 0.0700\n",
            "Epoch 102/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.8196 - student_loss: 2.2274 - distillation_loss: 2.5052e-07\n",
            "Epoch 00102: val_categorical_accuracy did not improve from 0.74670\n",
            "390/390 [==============================] - 210s 537ms/step - categorical_accuracy: 0.8196 - student_loss: 2.2274 - distillation_loss: 2.5051e-07 - val_categorical_accuracy: 0.7251 - val_student_loss: 2.2301 - lr: 0.0700\n",
            "Epoch 103/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.8243 - student_loss: 2.2265 - distillation_loss: 2.5001e-07\n",
            "Epoch 00103: val_categorical_accuracy did not improve from 0.74670\n",
            "390/390 [==============================] - 210s 537ms/step - categorical_accuracy: 0.8243 - student_loss: 2.2265 - distillation_loss: 2.5004e-07 - val_categorical_accuracy: 0.7042 - val_student_loss: 2.2216 - lr: 0.0700\n",
            "Epoch 104/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.8269 - student_loss: 2.2254 - distillation_loss: 2.4836e-07\n",
            "Epoch 00104: val_categorical_accuracy did not improve from 0.74670\n",
            "390/390 [==============================] - 209s 537ms/step - categorical_accuracy: 0.8269 - student_loss: 2.2254 - distillation_loss: 2.4835e-07 - val_categorical_accuracy: 0.7245 - val_student_loss: 2.2286 - lr: 0.0700\n",
            "Epoch 105/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.8280 - student_loss: 2.2246 - distillation_loss: 2.4749e-07\n",
            "Epoch 00105: val_categorical_accuracy did not improve from 0.74670\n",
            "390/390 [==============================] - 209s 537ms/step - categorical_accuracy: 0.8280 - student_loss: 2.2246 - distillation_loss: 2.4749e-07 - val_categorical_accuracy: 0.7076 - val_student_loss: 2.2226 - lr: 0.0700\n",
            "Epoch 106/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.8305 - student_loss: 2.2237 - distillation_loss: 2.4698e-07\n",
            "Epoch 00106: val_categorical_accuracy improved from 0.74670 to 0.76560, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "390/390 [==============================] - 210s 538ms/step - categorical_accuracy: 0.8305 - student_loss: 2.2237 - distillation_loss: 2.4696e-07 - val_categorical_accuracy: 0.7656 - val_student_loss: 2.2173 - lr: 0.0700\n",
            "Epoch 107/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.8326 - student_loss: 2.2228 - distillation_loss: 2.4502e-07\n",
            "Epoch 00107: val_categorical_accuracy did not improve from 0.76560\n",
            "390/390 [==============================] - 209s 537ms/step - categorical_accuracy: 0.8326 - student_loss: 2.2228 - distillation_loss: 2.4501e-07 - val_categorical_accuracy: 0.7559 - val_student_loss: 2.2237 - lr: 0.0700\n",
            "Epoch 108/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.8331 - student_loss: 2.2218 - distillation_loss: 2.4411e-07\n",
            "Epoch 00108: val_categorical_accuracy did not improve from 0.76560\n",
            "390/390 [==============================] - 209s 537ms/step - categorical_accuracy: 0.8331 - student_loss: 2.2218 - distillation_loss: 2.4411e-07 - val_categorical_accuracy: 0.7538 - val_student_loss: 2.2145 - lr: 0.0700\n",
            "Epoch 109/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.8364 - student_loss: 2.2209 - distillation_loss: 2.4368e-07\n",
            "Epoch 00109: val_categorical_accuracy did not improve from 0.76560\n",
            "390/390 [==============================] - 210s 537ms/step - categorical_accuracy: 0.8364 - student_loss: 2.2209 - distillation_loss: 2.4366e-07 - val_categorical_accuracy: 0.7641 - val_student_loss: 2.2137 - lr: 0.0700\n",
            "Epoch 110/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.8391 - student_loss: 2.2199 - distillation_loss: 2.4258e-07\n",
            "Epoch 00110: val_categorical_accuracy did not improve from 0.76560\n",
            "390/390 [==============================] - 209s 537ms/step - categorical_accuracy: 0.8391 - student_loss: 2.2200 - distillation_loss: 2.4258e-07 - val_categorical_accuracy: 0.7173 - val_student_loss: 2.2145 - lr: 0.0700\n",
            "Epoch 111/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.8407 - student_loss: 2.2189 - distillation_loss: 2.4135e-07\n",
            "Epoch 00111: val_categorical_accuracy did not improve from 0.76560\n",
            "390/390 [==============================] - 209s 537ms/step - categorical_accuracy: 0.8407 - student_loss: 2.2189 - distillation_loss: 2.4136e-07 - val_categorical_accuracy: 0.7479 - val_student_loss: 2.2186 - lr: 0.0700\n",
            "Epoch 112/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.8448 - student_loss: 2.2180 - distillation_loss: 2.4024e-07\n",
            "Epoch 00112: val_categorical_accuracy improved from 0.76560 to 0.78180, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "390/390 [==============================] - 210s 538ms/step - categorical_accuracy: 0.8448 - student_loss: 2.2180 - distillation_loss: 2.4024e-07 - val_categorical_accuracy: 0.7818 - val_student_loss: 2.2051 - lr: 0.0700\n",
            "Epoch 113/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.8435 - student_loss: 2.2171 - distillation_loss: 2.3894e-07\n",
            "Epoch 00113: val_categorical_accuracy did not improve from 0.78180\n",
            "390/390 [==============================] - 209s 537ms/step - categorical_accuracy: 0.8435 - student_loss: 2.2171 - distillation_loss: 2.3894e-07 - val_categorical_accuracy: 0.7616 - val_student_loss: 2.2090 - lr: 0.0700\n",
            "Epoch 114/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.8454 - student_loss: 2.2162 - distillation_loss: 2.3820e-07\n",
            "Epoch 00114: val_categorical_accuracy did not improve from 0.78180\n",
            "390/390 [==============================] - 209s 537ms/step - categorical_accuracy: 0.8454 - student_loss: 2.2162 - distillation_loss: 2.3822e-07 - val_categorical_accuracy: 0.7770 - val_student_loss: 2.2069 - lr: 0.0700\n",
            "Epoch 115/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.8481 - student_loss: 2.2153 - distillation_loss: 2.3692e-07\n",
            "Epoch 00115: val_categorical_accuracy did not improve from 0.78180\n",
            "390/390 [==============================] - 210s 537ms/step - categorical_accuracy: 0.8481 - student_loss: 2.2153 - distillation_loss: 2.3690e-07 - val_categorical_accuracy: 0.7776 - val_student_loss: 2.2040 - lr: 0.0700\n",
            "Epoch 116/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.8494 - student_loss: 2.2143 - distillation_loss: 2.3606e-07\n",
            "Epoch 00116: val_categorical_accuracy did not improve from 0.78180\n",
            "390/390 [==============================] - 209s 537ms/step - categorical_accuracy: 0.8494 - student_loss: 2.2143 - distillation_loss: 2.3603e-07 - val_categorical_accuracy: 0.7444 - val_student_loss: 2.2116 - lr: 0.0700\n",
            "Epoch 117/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.8508 - student_loss: 2.2134 - distillation_loss: 2.3486e-07\n",
            "Epoch 00117: val_categorical_accuracy improved from 0.78180 to 0.79290, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "390/390 [==============================] - 210s 538ms/step - categorical_accuracy: 0.8508 - student_loss: 2.2134 - distillation_loss: 2.3485e-07 - val_categorical_accuracy: 0.7929 - val_student_loss: 2.1989 - lr: 0.0700\n",
            "Epoch 118/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.8515 - student_loss: 2.2124 - distillation_loss: 2.3423e-07\n",
            "Epoch 00118: val_categorical_accuracy did not improve from 0.79290\n",
            "390/390 [==============================] - 209s 537ms/step - categorical_accuracy: 0.8515 - student_loss: 2.2124 - distillation_loss: 2.3425e-07 - val_categorical_accuracy: 0.7414 - val_student_loss: 2.2013 - lr: 0.0700\n",
            "Epoch 119/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.8541 - student_loss: 2.2114 - distillation_loss: 2.3272e-07\n",
            "Epoch 00119: val_categorical_accuracy did not improve from 0.79290\n",
            "390/390 [==============================] - 209s 537ms/step - categorical_accuracy: 0.8541 - student_loss: 2.2114 - distillation_loss: 2.3271e-07 - val_categorical_accuracy: 0.7578 - val_student_loss: 2.1977 - lr: 0.0700\n",
            "Epoch 120/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.8559 - student_loss: 2.2105 - distillation_loss: 2.3164e-07\n",
            "Epoch 00120: val_categorical_accuracy did not improve from 0.79290\n",
            "390/390 [==============================] - 209s 537ms/step - categorical_accuracy: 0.8559 - student_loss: 2.2105 - distillation_loss: 2.3161e-07 - val_categorical_accuracy: 0.7506 - val_student_loss: 2.1981 - lr: 0.0700\n",
            "Epoch 121/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.8566 - student_loss: 2.2096 - distillation_loss: 2.3037e-07\n",
            "Epoch 00121: val_categorical_accuracy did not improve from 0.79290\n",
            "390/390 [==============================] - 209s 537ms/step - categorical_accuracy: 0.8566 - student_loss: 2.2096 - distillation_loss: 2.3037e-07 - val_categorical_accuracy: 0.7738 - val_student_loss: 2.2069 - lr: 0.0700\n",
            "Epoch 122/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.8607 - student_loss: 2.2085 - distillation_loss: 2.2909e-07\n",
            "Epoch 00122: val_categorical_accuracy did not improve from 0.79290\n",
            "390/390 [==============================] - 209s 536ms/step - categorical_accuracy: 0.8607 - student_loss: 2.2085 - distillation_loss: 2.2911e-07 - val_categorical_accuracy: 0.7663 - val_student_loss: 2.1949 - lr: 0.0700\n",
            "Epoch 123/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.8602 - student_loss: 2.2078 - distillation_loss: 2.2871e-07\n",
            "Epoch 00123: val_categorical_accuracy did not improve from 0.79290\n",
            "390/390 [==============================] - 209s 537ms/step - categorical_accuracy: 0.8602 - student_loss: 2.2078 - distillation_loss: 2.2870e-07 - val_categorical_accuracy: 0.7895 - val_student_loss: 2.1864 - lr: 0.0700\n",
            "Epoch 124/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.8620 - student_loss: 2.2067 - distillation_loss: 2.2808e-07\n",
            "Epoch 00124: val_categorical_accuracy did not improve from 0.79290\n",
            "390/390 [==============================] - 209s 537ms/step - categorical_accuracy: 0.8620 - student_loss: 2.2067 - distillation_loss: 2.2812e-07 - val_categorical_accuracy: 0.7169 - val_student_loss: 2.2048 - lr: 0.0700\n",
            "Epoch 125/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.8635 - student_loss: 2.2058 - distillation_loss: 2.2634e-07\n",
            "Epoch 00125: val_categorical_accuracy improved from 0.79290 to 0.79980, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "390/390 [==============================] - 210s 538ms/step - categorical_accuracy: 0.8635 - student_loss: 2.2058 - distillation_loss: 2.2634e-07 - val_categorical_accuracy: 0.7998 - val_student_loss: 2.1903 - lr: 0.0700\n",
            "Epoch 126/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.8664 - student_loss: 2.2047 - distillation_loss: 2.2503e-07\n",
            "Epoch 00126: val_categorical_accuracy did not improve from 0.79980\n",
            "390/390 [==============================] - 209s 537ms/step - categorical_accuracy: 0.8664 - student_loss: 2.2047 - distillation_loss: 2.2506e-07 - val_categorical_accuracy: 0.7944 - val_student_loss: 2.1969 - lr: 0.0700\n",
            "Epoch 127/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.8685 - student_loss: 2.2039 - distillation_loss: 2.2370e-07\n",
            "Epoch 00127: val_categorical_accuracy did not improve from 0.79980\n",
            "390/390 [==============================] - 210s 537ms/step - categorical_accuracy: 0.8685 - student_loss: 2.2039 - distillation_loss: 2.2367e-07 - val_categorical_accuracy: 0.7836 - val_student_loss: 2.1908 - lr: 0.0700\n",
            "Epoch 128/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.8705 - student_loss: 2.2028 - distillation_loss: 2.2330e-07\n",
            "Epoch 00128: val_categorical_accuracy did not improve from 0.79980\n",
            "\n",
            "Epoch 00128: ReduceLROnPlateau reducing learning rate to 0.04900000020861625.\n",
            "390/390 [==============================] - 209s 537ms/step - categorical_accuracy: 0.8705 - student_loss: 2.2028 - distillation_loss: 2.2329e-07 - val_categorical_accuracy: 0.7820 - val_student_loss: 2.1937 - lr: 0.0700\n",
            "Epoch 129/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.8749 - student_loss: 2.2015 - distillation_loss: 2.2189e-07\n",
            "Epoch 00129: val_categorical_accuracy did not improve from 0.79980\n",
            "390/390 [==============================] - 209s 537ms/step - categorical_accuracy: 0.8749 - student_loss: 2.2015 - distillation_loss: 2.2191e-07 - val_categorical_accuracy: 0.7840 - val_student_loss: 2.1941 - lr: 0.0490\n",
            "Epoch 130/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.8742 - student_loss: 2.2010 - distillation_loss: 2.2181e-07\n",
            "Epoch 00130: val_categorical_accuracy improved from 0.79980 to 0.81390, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "390/390 [==============================] - 210s 538ms/step - categorical_accuracy: 0.8742 - student_loss: 2.2010 - distillation_loss: 2.2180e-07 - val_categorical_accuracy: 0.8139 - val_student_loss: 2.1829 - lr: 0.0490\n",
            "Epoch 131/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.8761 - student_loss: 2.2003 - distillation_loss: 2.2050e-07\n",
            "Epoch 00131: val_categorical_accuracy did not improve from 0.81390\n",
            "390/390 [==============================] - 209s 537ms/step - categorical_accuracy: 0.8761 - student_loss: 2.2003 - distillation_loss: 2.2053e-07 - val_categorical_accuracy: 0.7912 - val_student_loss: 2.1964 - lr: 0.0490\n",
            "Epoch 132/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.8771 - student_loss: 2.1996 - distillation_loss: 2.1996e-07\n",
            "Epoch 00132: val_categorical_accuracy did not improve from 0.81390\n",
            "390/390 [==============================] - 209s 537ms/step - categorical_accuracy: 0.8771 - student_loss: 2.1996 - distillation_loss: 2.1997e-07 - val_categorical_accuracy: 0.7961 - val_student_loss: 2.1873 - lr: 0.0490\n",
            "Epoch 133/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.8805 - student_loss: 2.1988 - distillation_loss: 2.1861e-07\n",
            "Epoch 00133: val_categorical_accuracy did not improve from 0.81390\n",
            "390/390 [==============================] - 210s 539ms/step - categorical_accuracy: 0.8805 - student_loss: 2.1988 - distillation_loss: 2.1861e-07 - val_categorical_accuracy: 0.7894 - val_student_loss: 2.1943 - lr: 0.0490\n",
            "Epoch 134/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.8797 - student_loss: 2.1982 - distillation_loss: 2.1776e-07\n",
            "Epoch 00134: val_categorical_accuracy did not improve from 0.81390\n",
            "390/390 [==============================] - 209s 537ms/step - categorical_accuracy: 0.8797 - student_loss: 2.1982 - distillation_loss: 2.1776e-07 - val_categorical_accuracy: 0.8042 - val_student_loss: 2.1871 - lr: 0.0490\n",
            "Epoch 135/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.8829 - student_loss: 2.1974 - distillation_loss: 2.1724e-07\n",
            "Epoch 00135: val_categorical_accuracy did not improve from 0.81390\n",
            "\n",
            "Epoch 00135: ReduceLROnPlateau reducing learning rate to 0.03429999910295009.\n",
            "390/390 [==============================] - 211s 542ms/step - categorical_accuracy: 0.8829 - student_loss: 2.1974 - distillation_loss: 2.1725e-07 - val_categorical_accuracy: 0.7895 - val_student_loss: 2.1843 - lr: 0.0490\n",
            "Epoch 136/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.8848 - student_loss: 2.1966 - distillation_loss: 2.1654e-07\n",
            "Epoch 00136: val_categorical_accuracy improved from 0.81390 to 0.82060, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "390/390 [==============================] - 211s 541ms/step - categorical_accuracy: 0.8848 - student_loss: 2.1966 - distillation_loss: 2.1655e-07 - val_categorical_accuracy: 0.8206 - val_student_loss: 2.1839 - lr: 0.0343\n",
            "Epoch 137/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.8862 - student_loss: 2.1961 - distillation_loss: 2.1605e-07\n",
            "Epoch 00137: val_categorical_accuracy did not improve from 0.82060\n",
            "390/390 [==============================] - 210s 539ms/step - categorical_accuracy: 0.8862 - student_loss: 2.1961 - distillation_loss: 2.1605e-07 - val_categorical_accuracy: 0.8069 - val_student_loss: 2.1841 - lr: 0.0343\n",
            "Epoch 138/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.8875 - student_loss: 2.1955 - distillation_loss: 2.1554e-07\n",
            "Epoch 00138: val_categorical_accuracy did not improve from 0.82060\n",
            "390/390 [==============================] - 212s 542ms/step - categorical_accuracy: 0.8875 - student_loss: 2.1955 - distillation_loss: 2.1552e-07 - val_categorical_accuracy: 0.8188 - val_student_loss: 2.1812 - lr: 0.0343\n",
            "Epoch 139/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.8893 - student_loss: 2.1951 - distillation_loss: 2.1512e-07\n",
            "Epoch 00139: val_categorical_accuracy did not improve from 0.82060\n",
            "390/390 [==============================] - 211s 542ms/step - categorical_accuracy: 0.8893 - student_loss: 2.1952 - distillation_loss: 2.1510e-07 - val_categorical_accuracy: 0.7948 - val_student_loss: 2.1845 - lr: 0.0343\n",
            "Epoch 140/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.8882 - student_loss: 2.1947 - distillation_loss: 2.1413e-07\n",
            "Epoch 00140: val_categorical_accuracy did not improve from 0.82060\n",
            "390/390 [==============================] - 210s 537ms/step - categorical_accuracy: 0.8882 - student_loss: 2.1947 - distillation_loss: 2.1413e-07 - val_categorical_accuracy: 0.8111 - val_student_loss: 2.1874 - lr: 0.0343\n",
            "Epoch 141/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.8889 - student_loss: 2.1941 - distillation_loss: 2.1318e-07\n",
            "Epoch 00141: val_categorical_accuracy did not improve from 0.82060\n",
            "390/390 [==============================] - 209s 537ms/step - categorical_accuracy: 0.8889 - student_loss: 2.1941 - distillation_loss: 2.1317e-07 - val_categorical_accuracy: 0.8128 - val_student_loss: 2.1860 - lr: 0.0343\n",
            "Epoch 142/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.8907 - student_loss: 2.1936 - distillation_loss: 2.1348e-07\n",
            "Epoch 00142: val_categorical_accuracy did not improve from 0.82060\n",
            "390/390 [==============================] - 211s 540ms/step - categorical_accuracy: 0.8907 - student_loss: 2.1936 - distillation_loss: 2.1350e-07 - val_categorical_accuracy: 0.8169 - val_student_loss: 2.1825 - lr: 0.0343\n",
            "Epoch 143/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.8888 - student_loss: 2.1932 - distillation_loss: 2.1301e-07\n",
            "Epoch 00143: val_categorical_accuracy did not improve from 0.82060\n",
            "390/390 [==============================] - 210s 538ms/step - categorical_accuracy: 0.8888 - student_loss: 2.1932 - distillation_loss: 2.1303e-07 - val_categorical_accuracy: 0.8036 - val_student_loss: 2.1766 - lr: 0.0343\n",
            "Epoch 144/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.8917 - student_loss: 2.1927 - distillation_loss: 2.1227e-07\n",
            "Epoch 00144: val_categorical_accuracy did not improve from 0.82060\n",
            "390/390 [==============================] - 210s 538ms/step - categorical_accuracy: 0.8917 - student_loss: 2.1927 - distillation_loss: 2.1228e-07 - val_categorical_accuracy: 0.8167 - val_student_loss: 2.1861 - lr: 0.0343\n",
            "Epoch 145/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.8933 - student_loss: 2.1922 - distillation_loss: 2.1188e-07\n",
            "Epoch 00145: val_categorical_accuracy did not improve from 0.82060\n",
            "390/390 [==============================] - 210s 538ms/step - categorical_accuracy: 0.8933 - student_loss: 2.1922 - distillation_loss: 2.1189e-07 - val_categorical_accuracy: 0.8206 - val_student_loss: 2.1759 - lr: 0.0343\n",
            "Epoch 146/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.8930 - student_loss: 2.1917 - distillation_loss: 2.1132e-07\n",
            "Epoch 00146: val_categorical_accuracy improved from 0.82060 to 0.82110, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "390/390 [==============================] - 211s 541ms/step - categorical_accuracy: 0.8930 - student_loss: 2.1917 - distillation_loss: 2.1130e-07 - val_categorical_accuracy: 0.8211 - val_student_loss: 2.1846 - lr: 0.0343\n",
            "Epoch 147/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.8939 - student_loss: 2.1912 - distillation_loss: 2.1079e-07\n",
            "Epoch 00147: val_categorical_accuracy did not improve from 0.82110\n",
            "390/390 [==============================] - 211s 540ms/step - categorical_accuracy: 0.8939 - student_loss: 2.1912 - distillation_loss: 2.1075e-07 - val_categorical_accuracy: 0.8166 - val_student_loss: 2.1826 - lr: 0.0343\n",
            "Epoch 148/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.8934 - student_loss: 2.1907 - distillation_loss: 2.0994e-07\n",
            "Epoch 00148: val_categorical_accuracy improved from 0.82110 to 0.83750, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "390/390 [==============================] - 210s 539ms/step - categorical_accuracy: 0.8934 - student_loss: 2.1907 - distillation_loss: 2.0995e-07 - val_categorical_accuracy: 0.8375 - val_student_loss: 2.1700 - lr: 0.0343\n",
            "Epoch 149/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.8955 - student_loss: 2.1902 - distillation_loss: 2.0956e-07\n",
            "Epoch 00149: val_categorical_accuracy did not improve from 0.83750\n",
            "390/390 [==============================] - 211s 540ms/step - categorical_accuracy: 0.8955 - student_loss: 2.1902 - distillation_loss: 2.0957e-07 - val_categorical_accuracy: 0.8305 - val_student_loss: 2.1740 - lr: 0.0343\n",
            "Epoch 150/150\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.8944 - student_loss: 2.1899 - distillation_loss: 2.0926e-07\n",
            "Epoch 00150: val_categorical_accuracy did not improve from 0.83750\n",
            "390/390 [==============================] - 211s 541ms/step - categorical_accuracy: 0.8944 - student_loss: 2.1900 - distillation_loss: 2.0924e-07 - val_categorical_accuracy: 0.8264 - val_student_loss: 2.1738 - lr: 0.0343\n",
            "313/313 [==============================] - 10s 32ms/step - categorical_accuracy: 0.8264 - student_loss: 2.1862\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8263999819755554"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#distiller = keras.models.load_model('Desktop/Trained_models/student_keras_vgg16_cifar10.h5')"
      ],
      "metadata": {
        "id": "s3I2FeMRKQcx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "outputId": "d3de6d7a-fd6b-4b1c-936d-dd3cf5eddbe6"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-e92cbc27883a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdistiller\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Desktop/Trained_models/student_keras_vgg16_cifar10.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    182\u001b[0m     if (h5py is not None and (\n\u001b[1;32m    183\u001b[0m         isinstance(filepath, h5py.File) or h5py.is_hdf5(filepath))):\n\u001b[0;32m--> 184\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mhdf5_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model_from_hdf5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36mload_model_from_hdf5\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0mmodel_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model_config'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmodel_config\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No model found in config file.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m     \u001b[0mmodel_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     model = model_config_lib.model_from_config(model_config,\n",
            "\u001b[0;31mValueError\u001b[0m: No model found in config file."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize = (9,6))\n",
        "plt.plot(history_distiller.history['val_student_loss'],label='Test loss')\n",
        "plt.plot(history_distiller.history['student_loss'],label='Train loss')\n",
        "plt.title('Loss curve for resnet model')\n",
        "plt.ylabel('Loss value')\n",
        "plt.xlabel('No. epoch')\n",
        "plt.show()\n",
        "\n",
        "# Plot history: Accuracy\n",
        "plt.figure(figsize = (9,6))\n",
        "plt.plot(history_distiller.history['val_categorical_accuracy'],label = 'Test accuracy')\n",
        "plt.plot(history_distiller.history['categorical_accuracy'],label = 'Train accuracy')\n",
        "plt.title('Accuracy curve for resnet model')\n",
        "plt.ylabel('Accuracy value (%)')\n",
        "plt.xlabel('No. epoch')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "l6ftcZ7WoKSS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 791
        },
        "outputId": "78d3e75b-1779-4169-ecae-1f5b2e1816db"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 648x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 648x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_prediction(model , image , true_value):\n",
        "  true_label_index = -1\n",
        "  for i in range(len(true_value)):\n",
        "\n",
        "    if(true_value[i]==1):\n",
        "      true_label_index = i\n",
        "      break\n",
        "\n",
        "  prediction = model.predict(image)[0]\n",
        "  probability = float('-inf')\n",
        "  predicted_label_index = -1\n",
        "\n",
        "  for i in range(len(prediction)):\n",
        "\n",
        "    if(prediction[i]>probability):\n",
        "      probability = prediction[i]\n",
        "      predicted_label_index = i\n",
        "\n",
        "  if(true_label_index!=predicted_label_index):\n",
        "    return 1\n",
        "  return 0"
      ],
      "metadata": {
        "id": "HPpOPmwZoKMp"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras,os\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential,Model\n",
        "from keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D , Dropout\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from keras.initializers import glorot_uniform\n",
        "from keras import applications\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50 , preprocess_input\n",
        "from keras import regularizers\n",
        "from absl import app, flags\n",
        "\n",
        "# Install bleeding edge version of cleverhans\n",
        "!pip install git+https://github.com/tensorflow/cleverhans.git#egg=cleverhans\n",
        "from cleverhans.tf2.attacks.projected_gradient_descent import projected_gradient_descent\n",
        "from cleverhans.tf2.attacks.fast_gradient_method import fast_gradient_method"
      ],
      "metadata": {
        "id": "u3phCvWqUjWD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d397702c-948c-4156-a834-94fb7002610c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting cleverhans\n",
            "  Cloning https://github.com/tensorflow/cleverhans.git to /tmp/pip-install-o5aw9hdt/cleverhans\n",
            "Collecting nose\n",
            "  Downloading nose-1.3.7-py3-none-any.whl (154 kB)\n",
            "\u001b[K     || 154 kB 20.0 MB/s \n",
            "\u001b[?25hCollecting pycodestyle\n",
            "  Downloading pycodestyle-2.8.0-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[K     || 42 kB 1.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from cleverhans) (1.4.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from cleverhans) (3.3.1)\n",
            "Collecting mnist\n",
            "  Downloading mnist-0.2.2-py2.py3-none-any.whl (3.5 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from cleverhans) (1.19.1)\n",
            "Collecting tensorflow-probability\n",
            "  Downloading tensorflow_probability-0.16.0-py2.py3-none-any.whl (6.3 MB)\n",
            "\u001b[K     || 6.3 MB 57.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib in ./.local/lib/python3.6/site-packages (from cleverhans) (1.1.0)\n",
            "Collecting easydict\n",
            "  Downloading easydict-1.9.tar.gz (6.4 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from cleverhans) (0.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cleverhans) (1.15.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->cleverhans) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->cleverhans) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.6/dist-packages (from matplotlib->cleverhans) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2020.06.20 in /usr/local/lib/python3.6/dist-packages (from matplotlib->cleverhans) (2020.6.20)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.6/dist-packages (from matplotlib->cleverhans) (7.2.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->cleverhans) (1.2.0)\n",
            "Requirement already satisfied: decorator in ./.local/lib/python3.6/site-packages (from tensorflow-probability->cleverhans) (4.4.2)\n",
            "Collecting dm-tree\n",
            "  Downloading dm_tree-0.1.6-cp36-cp36m-manylinux2014_x86_64.whl (115 kB)\n",
            "\u001b[K     || 115 kB 65.5 MB/s \n",
            "\u001b[?25hCollecting cloudpickle>=1.3\n",
            "  Downloading cloudpickle-2.0.0-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability->cleverhans) (0.3.3)\n",
            "Building wheels for collected packages: cleverhans, easydict\n",
            "  Building wheel for cleverhans (setup.py) ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Created wheel for cleverhans: filename=cleverhans-4.0.0-py3-none-any.whl size=92414 sha256=53dcc79716048820ddefe9262c075c6bb8e66f8fffda2fb54b205355b2e14fb9\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-uxpgmr0m/wheels/e7/a5/97/914123520cb1f3595c663c305802d2bec669028d3a190138f5\n",
            "  Building wheel for easydict (setup.py) ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Created wheel for easydict: filename=easydict-1.9-py3-none-any.whl size=6350 sha256=d38c5976a381e350b821d9672d3cc39733c994f7d3036c52de226ec5080906f1\n",
            "  Stored in directory: /home/temp01/.cache/pip/wheels/5d/79/e4/4e55effe206295359b37e0f9db3e68a1197ba396682807dadb\n",
            "Successfully built cleverhans easydict\n",
            "Installing collected packages: nose, pycodestyle, mnist, dm-tree, cloudpickle, tensorflow-probability, easydict, cleverhans\n",
            "\u001b[33m  WARNING: The scripts nosetests and nosetests-3.4 are installed in '/home/temp01/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
            "\u001b[33m  WARNING: The script pycodestyle is installed in '/home/temp01/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
            "Successfully installed cleverhans-4.0.0 cloudpickle-2.0.0 dm-tree-0.1.6 easydict-1.9 mnist-0.2.2 nose-1.3.7 pycodestyle-2.8.0 tensorflow-probability-0.16.0\n",
            "\u001b[33mWARNING: You are using pip version 20.2.2; however, version 21.3.1 is available.\n",
            "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import img_to_array\n",
        "\n",
        "epsilon_end = 0.8\n",
        "#logits_model = tf.keras.Model(model.input,model.layers[-1].output)\n",
        "total_images = 200\n",
        "\n",
        "result={}\n",
        "epsilon = 0.1\n",
        "while(epsilon <= epsilon_end):\n",
        "\n",
        "  fgsm_counter = 0\n",
        "  pgd_counter = 0\n",
        "  print(\"Epsilon value - \",epsilon)\n",
        "  print()\n",
        "  for image_index in range(total_images):\n",
        "  \n",
        "  \n",
        "    image = x_test[image_index]\n",
        "    image = img_to_array(image)\n",
        "    image = image.reshape(1, 32, 32, 3)\n",
        "    true_value = y_test[image_index]\n",
        "    original_prediction = make_prediction(distiller,image,true_value)\n",
        "    fgsm_sample = fast_gradient_method(distiller, image, epsilon, np.inf, targeted=False)\n",
        "    pgd_sample = projected_gradient_descent(distiller, image, epsilon, 0.01, 40, np.inf)\n",
        "    print(epsilon,' ',image_index)\n",
        "    fgsm_prediction = make_prediction(distiller , fgsm_sample , true_value)\n",
        "    pgd_prediction = make_prediction(distiller , pgd_sample , true_value)\n",
        "    pgd_counter+=pgd_prediction\n",
        "    fgsm_counter+=fgsm_prediction\n",
        "    print(pgd_counter,' ',fgsm_counter)\n",
        "  result[epsilon] = (fgsm_counter , pgd_counter)\n",
        "  epsilon+=0.1\n"
      ],
      "metadata": {
        "id": "LjVpMPnXURRO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d774fbd-2805-4ec8-a7ac-73a97bb97fac"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epsilon value -  0.1\n",
            "\n",
            "0.1   0\n",
            "0   0\n",
            "0.1   1\n",
            "0   0\n",
            "0.1   2\n",
            "0   0\n",
            "0.1   3\n",
            "0   0\n",
            "0.1   4\n",
            "0   0\n",
            "0.1   5\n",
            "0   0\n",
            "0.1   6\n",
            "0   0\n",
            "0.1   7\n",
            "1   1\n",
            "0.1   8\n",
            "1   1\n",
            "0.1   9\n",
            "1   1\n",
            "0.1   10\n",
            "1   1\n",
            "0.1   11\n",
            "1   1\n",
            "0.1   12\n",
            "1   1\n",
            "0.1   13\n",
            "1   1\n",
            "0.1   14\n",
            "1   1\n",
            "0.1   15\n",
            "1   1\n",
            "0.1   16\n",
            "1   1\n",
            "0.1   17\n",
            "1   1\n",
            "0.1   18\n",
            "1   1\n",
            "0.1   19\n",
            "1   1\n",
            "0.1   20\n",
            "1   1\n",
            "0.1   21\n",
            "1   1\n",
            "0.1   22\n",
            "2   2\n",
            "0.1   23\n",
            "2   2\n",
            "0.1   24\n",
            "2   2\n",
            "0.1   25\n",
            "2   2\n",
            "0.1   26\n",
            "2   2\n",
            "0.1   27\n",
            "2   2\n",
            "0.1   28\n",
            "2   2\n",
            "0.1   29\n",
            "2   2\n",
            "0.1   30\n",
            "2   2\n",
            "0.1   31\n",
            "2   2\n",
            "0.1   32\n",
            "2   2\n",
            "0.1   33\n",
            "2   2\n",
            "0.1   34\n",
            "2   2\n",
            "0.1   35\n",
            "2   2\n",
            "0.1   36\n",
            "2   2\n",
            "0.1   37\n",
            "2   2\n",
            "0.1   38\n",
            "2   2\n",
            "0.1   39\n",
            "2   2\n",
            "0.1   40\n",
            "2   2\n",
            "0.1   41\n",
            "2   2\n",
            "0.1   42\n",
            "2   2\n",
            "0.1   43\n",
            "2   2\n",
            "0.1   44\n",
            "2   2\n",
            "0.1   45\n",
            "2   2\n",
            "0.1   46\n",
            "2   2\n",
            "0.1   47\n",
            "3   3\n",
            "0.1   48\n",
            "3   3\n",
            "0.1   49\n",
            "3   3\n",
            "0.1   50\n",
            "3   3\n",
            "0.1   51\n",
            "3   3\n",
            "0.1   52\n",
            "4   4\n",
            "0.1   53\n",
            "4   4\n",
            "0.1   54\n",
            "4   4\n",
            "0.1   55\n",
            "4   4\n",
            "0.1   56\n",
            "4   4\n",
            "0.1   57\n",
            "5   5\n",
            "0.1   58\n",
            "6   6\n",
            "0.1   59\n",
            "7   7\n",
            "0.1   60\n",
            "7   7\n",
            "0.1   61\n",
            "8   8\n",
            "0.1   62\n",
            "8   8\n",
            "0.1   63\n",
            "8   8\n",
            "0.1   64\n",
            "8   8\n",
            "0.1   65\n",
            "8   8\n",
            "0.1   66\n",
            "9   9\n",
            "0.1   67\n",
            "9   9\n",
            "0.1   68\n",
            "9   9\n",
            "0.1   69\n",
            "9   9\n",
            "0.1   70\n",
            "10   10\n",
            "0.1   71\n",
            "10   10\n",
            "0.1   72\n",
            "10   10\n",
            "0.1   73\n",
            "10   10\n",
            "0.1   74\n",
            "10   10\n",
            "0.1   75\n",
            "10   10\n",
            "0.1   76\n",
            "10   10\n",
            "0.1   77\n",
            "10   10\n",
            "0.1   78\n",
            "11   11\n",
            "0.1   79\n",
            "11   11\n",
            "0.1   80\n",
            "11   11\n",
            "0.1   81\n",
            "11   11\n",
            "0.1   82\n",
            "11   11\n",
            "0.1   83\n",
            "11   11\n",
            "0.1   84\n",
            "11   11\n",
            "0.1   85\n",
            "12   12\n",
            "0.1   86\n",
            "12   12\n",
            "0.1   87\n",
            "13   13\n",
            "0.1   88\n",
            "13   13\n",
            "0.1   89\n",
            "13   13\n",
            "0.1   90\n",
            "13   13\n",
            "0.1   91\n",
            "13   13\n",
            "0.1   92\n",
            "13   13\n",
            "0.1   93\n",
            "13   13\n",
            "0.1   94\n",
            "13   13\n",
            "0.1   95\n",
            "13   13\n",
            "0.1   96\n",
            "13   13\n",
            "0.1   97\n",
            "13   13\n",
            "0.1   98\n",
            "13   13\n",
            "0.1   99\n",
            "13   13\n",
            "0.1   100\n",
            "13   13\n",
            "0.1   101\n",
            "13   13\n",
            "0.1   102\n",
            "13   13\n",
            "0.1   103\n",
            "13   13\n",
            "0.1   104\n",
            "13   13\n",
            "0.1   105\n",
            "13   13\n",
            "0.1   106\n",
            "13   13\n",
            "0.1   107\n",
            "13   13\n",
            "0.1   108\n",
            "13   13\n",
            "0.1   109\n",
            "13   13\n",
            "0.1   110\n",
            "13   13\n",
            "0.1   111\n",
            "13   13\n",
            "0.1   112\n",
            "13   13\n",
            "0.1   113\n",
            "13   13\n",
            "0.1   114\n",
            "13   13\n",
            "0.1   115\n",
            "13   13\n",
            "0.1   116\n",
            "13   13\n",
            "0.1   117\n",
            "13   13\n",
            "0.1   118\n",
            "13   13\n",
            "0.1   119\n",
            "13   13\n",
            "0.1   120\n",
            "13   13\n",
            "0.1   121\n",
            "13   13\n",
            "0.1   122\n",
            "13   13\n",
            "0.1   123\n",
            "13   13\n",
            "0.1   124\n",
            "13   13\n",
            "0.1   125\n",
            "14   14\n",
            "0.1   126\n",
            "14   14\n",
            "0.1   127\n",
            "14   14\n",
            "0.1   128\n",
            "14   14\n",
            "0.1   129\n",
            "14   14\n",
            "0.1   130\n",
            "15   15\n",
            "0.1   131\n",
            "15   15\n",
            "0.1   132\n",
            "15   15\n",
            "0.1   133\n",
            "15   15\n",
            "0.1   134\n",
            "15   15\n",
            "0.1   135\n",
            "15   15\n",
            "0.1   136\n",
            "15   15\n",
            "0.1   137\n",
            "15   15\n",
            "0.1   138\n",
            "15   15\n",
            "0.1   139\n",
            "16   16\n",
            "0.1   140\n",
            "16   16\n",
            "0.1   141\n",
            "16   16\n",
            "0.1   142\n",
            "16   16\n",
            "0.1   143\n",
            "17   17\n",
            "0.1   144\n",
            "17   17\n",
            "0.1   145\n",
            "18   18\n",
            "0.1   146\n",
            "18   18\n",
            "0.1   147\n",
            "19   19\n",
            "0.1   148\n",
            "19   19\n",
            "0.1   149\n",
            "19   19\n",
            "0.1   150\n",
            "19   19\n",
            "0.1   151\n",
            "19   19\n",
            "0.1   152\n",
            "19   19\n",
            "0.1   153\n",
            "19   19\n",
            "0.1   154\n",
            "19   19\n",
            "0.1   155\n",
            "19   19\n",
            "0.1   156\n",
            "19   19\n",
            "0.1   157\n",
            "19   19\n",
            "0.1   158\n",
            "19   19\n",
            "0.1   159\n",
            "19   19\n",
            "0.1   160\n",
            "20   20\n",
            "0.1   161\n",
            "20   20\n",
            "0.1   162\n",
            "20   20\n",
            "0.1   163\n",
            "20   20\n",
            "0.1   164\n",
            "21   21\n",
            "0.1   165\n",
            "21   21\n",
            "0.1   166\n",
            "21   21\n",
            "0.1   167\n",
            "22   22\n",
            "0.1   168\n",
            "22   22\n",
            "0.1   169\n",
            "23   23\n",
            "0.1   170\n",
            "23   23\n",
            "0.1   171\n",
            "23   23\n",
            "0.1   172\n",
            "23   23\n",
            "0.1   173\n",
            "23   23\n",
            "0.1   174\n",
            "23   23\n",
            "0.1   175\n",
            "23   23\n",
            "0.1   176\n",
            "23   23\n",
            "0.1   177\n",
            "23   23\n",
            "0.1   178\n",
            "24   24\n",
            "0.1   179\n",
            "24   24\n",
            "0.1   180\n",
            "24   24\n",
            "0.1   181\n",
            "24   24\n",
            "0.1   182\n",
            "24   24\n",
            "0.1   183\n",
            "24   24\n",
            "0.1   184\n",
            "25   25\n",
            "0.1   185\n",
            "25   25\n",
            "0.1   186\n",
            "25   25\n",
            "0.1   187\n",
            "25   25\n",
            "0.1   188\n",
            "26   26\n",
            "0.1   189\n",
            "26   26\n",
            "0.1   190\n",
            "26   26\n",
            "0.1   191\n",
            "26   26\n",
            "0.1   192\n",
            "26   26\n",
            "0.1   193\n",
            "26   26\n",
            "0.1   194\n",
            "26   26\n",
            "0.1   195\n",
            "26   26\n",
            "0.1   196\n",
            "26   26\n",
            "0.1   197\n",
            "26   26\n",
            "0.1   198\n",
            "26   26\n",
            "0.1   199\n",
            "26   26\n",
            "Epsilon value -  0.2\n",
            "\n",
            "0.2   0\n",
            "0   0\n",
            "0.2   1\n",
            "0   0\n",
            "0.2   2\n",
            "0   0\n",
            "0.2   3\n",
            "0   0\n",
            "0.2   4\n",
            "0   0\n",
            "0.2   5\n",
            "0   0\n",
            "0.2   6\n",
            "0   0\n",
            "0.2   7\n",
            "0   0\n",
            "0.2   8\n",
            "0   0\n",
            "0.2   9\n",
            "0   0\n",
            "0.2   10\n",
            "0   0\n",
            "0.2   11\n",
            "0   0\n",
            "0.2   12\n",
            "0   0\n",
            "0.2   13\n",
            "0   0\n",
            "0.2   14\n",
            "0   0\n",
            "0.2   15\n",
            "0   0\n",
            "0.2   16\n",
            "0   0\n",
            "0.2   17\n",
            "0   0\n",
            "0.2   18\n",
            "0   0\n",
            "0.2   19\n",
            "0   0\n",
            "0.2   20\n",
            "0   0\n",
            "0.2   21\n",
            "0   0\n",
            "0.2   22\n",
            "0   0\n",
            "0.2   23\n",
            "0   0\n",
            "0.2   24\n",
            "0   0\n",
            "0.2   25\n",
            "0   0\n",
            "0.2   26\n",
            "0   0\n",
            "0.2   27\n",
            "0   0\n",
            "0.2   28\n",
            "0   0\n",
            "0.2   29\n",
            "0   0\n",
            "0.2   30\n",
            "0   0\n",
            "0.2   31\n",
            "0   0\n",
            "0.2   32\n",
            "0   0\n",
            "0.2   33\n",
            "0   0\n",
            "0.2   34\n",
            "0   0\n",
            "0.2   35\n",
            "0   0\n",
            "0.2   36\n",
            "0   0\n",
            "0.2   37\n",
            "0   0\n",
            "0.2   38\n",
            "0   0\n",
            "0.2   39\n",
            "0   0\n",
            "0.2   40\n",
            "1   1\n",
            "0.2   41\n",
            "1   1\n",
            "0.2   42\n",
            "1   1\n",
            "0.2   43\n",
            "1   1\n",
            "0.2   44\n",
            "1   1\n",
            "0.2   45\n",
            "1   1\n",
            "0.2   46\n",
            "1   1\n",
            "0.2   47\n",
            "2   2\n",
            "0.2   48\n",
            "2   2\n",
            "0.2   49\n",
            "2   2\n",
            "0.2   50\n",
            "2   2\n",
            "0.2   51\n",
            "2   2\n",
            "0.2   52\n",
            "3   3\n",
            "0.2   53\n",
            "4   4\n",
            "0.2   54\n",
            "4   4\n",
            "0.2   55\n",
            "4   4\n",
            "0.2   56\n",
            "4   4\n",
            "0.2   57\n",
            "5   5\n",
            "0.2   58\n",
            "6   6\n",
            "0.2   59\n",
            "7   7\n",
            "0.2   60\n",
            "7   7\n",
            "0.2   61\n",
            "8   8\n",
            "0.2   62\n",
            "8   8\n",
            "0.2   63\n",
            "8   8\n",
            "0.2   64\n",
            "8   8\n",
            "0.2   65\n",
            "8   8\n",
            "0.2   66\n",
            "9   9\n",
            "0.2   67\n",
            "9   9\n",
            "0.2   68\n",
            "9   9\n",
            "0.2   69\n",
            "9   9\n",
            "0.2   70\n",
            "10   10\n",
            "0.2   71\n",
            "10   10\n",
            "0.2   72\n",
            "10   10\n",
            "0.2   73\n",
            "10   10\n",
            "0.2   74\n",
            "10   10\n",
            "0.2   75\n",
            "10   10\n",
            "0.2   76\n",
            "11   11\n",
            "0.2   77\n",
            "11   11\n",
            "0.2   78\n",
            "12   12\n",
            "0.2   79\n",
            "12   12\n",
            "0.2   80\n",
            "12   12\n",
            "0.2   81\n",
            "12   12\n",
            "0.2   82\n",
            "12   12\n",
            "0.2   83\n",
            "12   12\n",
            "0.2   84\n",
            "12   12\n",
            "0.2   85\n",
            "12   12\n",
            "0.2   86\n",
            "13   13\n",
            "0.2   87\n",
            "14   14\n",
            "0.2   88\n",
            "14   14\n",
            "0.2   89\n",
            "14   14\n",
            "0.2   90\n",
            "14   14\n",
            "0.2   91\n",
            "14   14\n",
            "0.2   92\n",
            "14   14\n",
            "0.2   93\n",
            "14   14\n",
            "0.2   94\n",
            "14   14\n",
            "0.2   95\n",
            "14   14\n",
            "0.2   96\n",
            "14   14\n",
            "0.2   97\n",
            "14   14\n",
            "0.2   98\n",
            "14   14\n",
            "0.2   99\n",
            "14   14\n",
            "0.2   100\n",
            "15   15\n",
            "0.2   101\n",
            "15   15\n",
            "0.2   102\n",
            "15   15\n",
            "0.2   103\n",
            "15   15\n",
            "0.2   104\n",
            "15   15\n",
            "0.2   105\n",
            "15   15\n",
            "0.2   106\n",
            "15   15\n",
            "0.2   107\n",
            "15   15\n",
            "0.2   108\n",
            "15   15\n",
            "0.2   109\n",
            "15   15\n",
            "0.2   110\n",
            "15   15\n",
            "0.2   111\n",
            "15   15\n",
            "0.2   112\n",
            "15   15\n",
            "0.2   113\n",
            "15   15\n",
            "0.2   114\n",
            "15   15\n",
            "0.2   115\n",
            "15   15\n",
            "0.2   116\n",
            "15   15\n",
            "0.2   117\n",
            "15   15\n",
            "0.2   118\n",
            "15   15\n",
            "0.2   119\n",
            "15   15\n",
            "0.2   120\n",
            "15   15\n",
            "0.2   121\n",
            "15   15\n",
            "0.2   122\n",
            "15   15\n",
            "0.2   123\n",
            "15   15\n",
            "0.2   124\n",
            "15   15\n",
            "0.2   125\n",
            "16   16\n",
            "0.2   126\n",
            "16   16\n",
            "0.2   127\n",
            "16   16\n",
            "0.2   128\n",
            "16   16\n",
            "0.2   129\n",
            "16   16\n",
            "0.2   130\n",
            "17   17\n",
            "0.2   131\n",
            "17   17\n",
            "0.2   132\n",
            "17   17\n",
            "0.2   133\n",
            "17   17\n",
            "0.2   134\n",
            "17   17\n",
            "0.2   135\n",
            "17   17\n",
            "0.2   136\n",
            "17   17\n",
            "0.2   137\n",
            "17   17\n",
            "0.2   138\n",
            "17   17\n",
            "0.2   139\n",
            "18   18\n",
            "0.2   140\n",
            "18   18\n",
            "0.2   141\n",
            "18   18\n",
            "0.2   142\n",
            "18   18\n",
            "0.2   143\n",
            "19   19\n",
            "0.2   144\n",
            "19   19\n",
            "0.2   145\n",
            "20   20\n",
            "0.2   146\n",
            "20   20\n",
            "0.2   147\n",
            "21   21\n",
            "0.2   148\n",
            "21   21\n",
            "0.2   149\n",
            "21   21\n",
            "0.2   150\n",
            "21   21\n",
            "0.2   151\n",
            "22   22\n",
            "0.2   152\n",
            "22   22\n",
            "0.2   153\n",
            "22   22\n",
            "0.2   154\n",
            "22   22\n",
            "0.2   155\n",
            "22   22\n",
            "0.2   156\n",
            "22   22\n",
            "0.2   157\n",
            "22   22\n",
            "0.2   158\n",
            "22   22\n",
            "0.2   159\n",
            "22   22\n",
            "0.2   160\n",
            "22   22\n",
            "0.2   161\n",
            "22   22\n",
            "0.2   162\n",
            "22   22\n",
            "0.2   163\n",
            "22   22\n",
            "0.2   164\n",
            "23   23\n",
            "0.2   165\n",
            "23   23\n",
            "0.2   166\n",
            "23   23\n",
            "0.2   167\n",
            "23   23\n",
            "0.2   168\n",
            "23   23\n",
            "0.2   169\n",
            "24   24\n",
            "0.2   170\n",
            "24   24\n",
            "0.2   171\n",
            "24   24\n",
            "0.2   172\n",
            "24   24\n",
            "0.2   173\n",
            "24   24\n",
            "0.2   174\n",
            "24   24\n",
            "0.2   175\n",
            "24   24\n",
            "0.2   176\n",
            "24   24\n",
            "0.2   177\n",
            "24   24\n",
            "0.2   178\n",
            "25   25\n",
            "0.2   179\n",
            "25   25\n",
            "0.2   180\n",
            "25   25\n",
            "0.2   181\n",
            "25   25\n",
            "0.2   182\n",
            "25   25\n",
            "0.2   183\n",
            "25   25\n",
            "0.2   184\n",
            "26   26\n",
            "0.2   185\n",
            "26   26\n",
            "0.2   186\n",
            "26   26\n",
            "0.2   187\n",
            "26   26\n",
            "0.2   188\n",
            "27   27\n",
            "0.2   189\n",
            "27   27\n",
            "0.2   190\n",
            "27   27\n",
            "0.2   191\n",
            "27   27\n",
            "0.2   192\n",
            "27   27\n",
            "0.2   193\n",
            "27   27\n",
            "0.2   194\n",
            "27   27\n",
            "0.2   195\n",
            "27   27\n",
            "0.2   196\n",
            "27   27\n",
            "0.2   197\n",
            "27   27\n",
            "0.2   198\n",
            "27   27\n",
            "0.2   199\n",
            "28   28\n",
            "Epsilon value -  0.30000000000000004\n",
            "\n",
            "0.30000000000000004   0\n",
            "0   0\n",
            "0.30000000000000004   1\n",
            "0   0\n",
            "0.30000000000000004   2\n",
            "0   0\n",
            "0.30000000000000004   3\n",
            "0   0\n",
            "0.30000000000000004   4\n",
            "0   0\n",
            "0.30000000000000004   5\n",
            "0   0\n",
            "0.30000000000000004   6\n",
            "0   0\n",
            "0.30000000000000004   7\n",
            "0   0\n",
            "0.30000000000000004   8\n",
            "0   0\n",
            "0.30000000000000004   9\n",
            "0   0\n",
            "0.30000000000000004   10\n",
            "0   0\n",
            "0.30000000000000004   11\n",
            "0   0\n",
            "0.30000000000000004   12\n",
            "0   0\n",
            "0.30000000000000004   13\n",
            "0   0\n",
            "0.30000000000000004   14\n",
            "0   0\n",
            "0.30000000000000004   15\n",
            "0   0\n",
            "0.30000000000000004   16\n",
            "0   0\n",
            "0.30000000000000004   17\n",
            "0   0\n",
            "0.30000000000000004   18\n",
            "0   0\n",
            "0.30000000000000004   19\n",
            "0   0\n",
            "0.30000000000000004   20\n",
            "0   0\n",
            "0.30000000000000004   21\n",
            "0   0\n",
            "0.30000000000000004   22\n",
            "0   0\n",
            "0.30000000000000004   23\n",
            "0   0\n",
            "0.30000000000000004   24\n",
            "1   1\n",
            "0.30000000000000004   25\n",
            "1   1\n",
            "0.30000000000000004   26\n",
            "2   2\n",
            "0.30000000000000004   27\n",
            "2   2\n",
            "0.30000000000000004   28\n",
            "2   2\n",
            "0.30000000000000004   29\n",
            "2   2\n",
            "0.30000000000000004   30\n",
            "2   2\n",
            "0.30000000000000004   31\n",
            "2   2\n",
            "0.30000000000000004   32\n",
            "2   2\n",
            "0.30000000000000004   33\n",
            "2   2\n",
            "0.30000000000000004   34\n",
            "2   2\n",
            "0.30000000000000004   35\n",
            "2   2\n",
            "0.30000000000000004   36\n",
            "2   2\n",
            "0.30000000000000004   37\n",
            "2   2\n",
            "0.30000000000000004   38\n",
            "2   2\n",
            "0.30000000000000004   39\n",
            "2   2\n",
            "0.30000000000000004   40\n",
            "3   3\n",
            "0.30000000000000004   41\n",
            "3   3\n",
            "0.30000000000000004   42\n",
            "3   3\n",
            "0.30000000000000004   43\n",
            "3   3\n",
            "0.30000000000000004   44\n",
            "3   3\n",
            "0.30000000000000004   45\n",
            "3   3\n",
            "0.30000000000000004   46\n",
            "3   3\n",
            "0.30000000000000004   47\n",
            "4   4\n",
            "0.30000000000000004   48\n",
            "4   4\n",
            "0.30000000000000004   49\n",
            "4   4\n",
            "0.30000000000000004   50\n",
            "4   4\n",
            "0.30000000000000004   51\n",
            "4   4\n",
            "0.30000000000000004   52\n",
            "5   5\n",
            "0.30000000000000004   53\n",
            "6   6\n",
            "0.30000000000000004   54\n",
            "6   6\n",
            "0.30000000000000004   55\n",
            "6   6\n",
            "0.30000000000000004   56\n",
            "6   6\n",
            "0.30000000000000004   57\n",
            "7   7\n",
            "0.30000000000000004   58\n",
            "8   8\n",
            "0.30000000000000004   59\n",
            "9   9\n",
            "0.30000000000000004   60\n",
            "9   9\n",
            "0.30000000000000004   61\n",
            "10   10\n",
            "0.30000000000000004   62\n",
            "10   10\n",
            "0.30000000000000004   63\n",
            "10   10\n",
            "0.30000000000000004   64\n",
            "10   10\n",
            "0.30000000000000004   65\n",
            "10   10\n",
            "0.30000000000000004   66\n",
            "11   11\n",
            "0.30000000000000004   67\n",
            "11   11\n",
            "0.30000000000000004   68\n",
            "11   11\n",
            "0.30000000000000004   69\n",
            "11   11\n",
            "0.30000000000000004   70\n",
            "12   12\n",
            "0.30000000000000004   71\n",
            "12   12\n",
            "0.30000000000000004   72\n",
            "12   12\n",
            "0.30000000000000004   73\n",
            "12   12\n",
            "0.30000000000000004   74\n",
            "13   13\n",
            "0.30000000000000004   75\n",
            "13   13\n",
            "0.30000000000000004   76\n",
            "14   14\n",
            "0.30000000000000004   77\n",
            "14   14\n",
            "0.30000000000000004   78\n",
            "15   15\n",
            "0.30000000000000004   79\n",
            "15   15\n",
            "0.30000000000000004   80\n",
            "15   15\n",
            "0.30000000000000004   81\n",
            "16   16\n",
            "0.30000000000000004   82\n",
            "16   16\n",
            "0.30000000000000004   83\n",
            "16   16\n",
            "0.30000000000000004   84\n",
            "16   16\n",
            "0.30000000000000004   85\n",
            "16   16\n",
            "0.30000000000000004   86\n",
            "17   17\n",
            "0.30000000000000004   87\n",
            "18   18\n",
            "0.30000000000000004   88\n",
            "18   18\n",
            "0.30000000000000004   89\n",
            "18   18\n",
            "0.30000000000000004   90\n",
            "18   18\n",
            "0.30000000000000004   91\n",
            "18   18\n",
            "0.30000000000000004   92\n",
            "18   18\n",
            "0.30000000000000004   93\n",
            "18   18\n",
            "0.30000000000000004   94\n",
            "18   18\n",
            "0.30000000000000004   95\n",
            "18   18\n",
            "0.30000000000000004   96\n",
            "18   18\n",
            "0.30000000000000004   97\n",
            "18   18\n",
            "0.30000000000000004   98\n",
            "18   18\n",
            "0.30000000000000004   99\n",
            "18   18\n",
            "0.30000000000000004   100\n",
            "19   19\n",
            "0.30000000000000004   101\n",
            "19   19\n",
            "0.30000000000000004   102\n",
            "19   19\n",
            "0.30000000000000004   103\n",
            "19   19\n",
            "0.30000000000000004   104\n",
            "19   19\n",
            "0.30000000000000004   105\n",
            "19   19\n",
            "0.30000000000000004   106\n",
            "19   19\n",
            "0.30000000000000004   107\n",
            "19   19\n",
            "0.30000000000000004   108\n",
            "19   19\n",
            "0.30000000000000004   109\n",
            "19   19\n",
            "0.30000000000000004   110\n",
            "19   19\n",
            "0.30000000000000004   111\n",
            "19   19\n",
            "0.30000000000000004   112\n",
            "19   19\n",
            "0.30000000000000004   113\n",
            "19   19\n",
            "0.30000000000000004   114\n",
            "19   19\n",
            "0.30000000000000004   115\n",
            "19   19\n",
            "0.30000000000000004   116\n",
            "19   19\n",
            "0.30000000000000004   117\n",
            "19   19\n",
            "0.30000000000000004   118\n",
            "19   19\n",
            "0.30000000000000004   119\n",
            "19   19\n",
            "0.30000000000000004   120\n",
            "19   19\n",
            "0.30000000000000004   121\n",
            "20   20\n",
            "0.30000000000000004   122\n",
            "20   20\n",
            "0.30000000000000004   123\n",
            "20   20\n",
            "0.30000000000000004   124\n",
            "20   20\n",
            "0.30000000000000004   125\n",
            "20   20\n",
            "0.30000000000000004   126\n",
            "20   20\n",
            "0.30000000000000004   127\n",
            "21   21\n",
            "0.30000000000000004   128\n",
            "21   21\n",
            "0.30000000000000004   129\n",
            "21   21\n",
            "0.30000000000000004   130\n",
            "22   22\n",
            "0.30000000000000004   131\n",
            "22   22\n",
            "0.30000000000000004   132\n",
            "22   22\n",
            "0.30000000000000004   133\n",
            "22   22\n",
            "0.30000000000000004   134\n",
            "22   22\n",
            "0.30000000000000004   135\n",
            "22   22\n",
            "0.30000000000000004   136\n",
            "22   22\n",
            "0.30000000000000004   137\n",
            "22   22\n",
            "0.30000000000000004   138\n",
            "22   22\n",
            "0.30000000000000004   139\n",
            "23   23\n",
            "0.30000000000000004   140\n",
            "23   23\n",
            "0.30000000000000004   141\n",
            "23   23\n",
            "0.30000000000000004   142\n",
            "23   23\n",
            "0.30000000000000004   143\n",
            "24   24\n",
            "0.30000000000000004   144\n",
            "24   24\n",
            "0.30000000000000004   145\n",
            "25   25\n",
            "0.30000000000000004   146\n",
            "25   25\n",
            "0.30000000000000004   147\n",
            "26   26\n",
            "0.30000000000000004   148\n",
            "26   26\n",
            "0.30000000000000004   149\n",
            "26   26\n",
            "0.30000000000000004   150\n",
            "26   26\n",
            "0.30000000000000004   151\n",
            "27   27\n",
            "0.30000000000000004   152\n",
            "27   27\n",
            "0.30000000000000004   153\n",
            "27   27\n",
            "0.30000000000000004   154\n",
            "27   27\n",
            "0.30000000000000004   155\n",
            "27   27\n",
            "0.30000000000000004   156\n",
            "27   27\n",
            "0.30000000000000004   157\n",
            "27   27\n",
            "0.30000000000000004   158\n",
            "27   27\n",
            "0.30000000000000004   159\n",
            "27   27\n",
            "0.30000000000000004   160\n",
            "27   27\n",
            "0.30000000000000004   161\n",
            "27   27\n",
            "0.30000000000000004   162\n",
            "28   28\n",
            "0.30000000000000004   163\n",
            "28   28\n",
            "0.30000000000000004   164\n",
            "29   29\n",
            "0.30000000000000004   165\n",
            "30   30\n",
            "0.30000000000000004   166\n",
            "31   31\n",
            "0.30000000000000004   167\n",
            "31   31\n",
            "0.30000000000000004   168\n",
            "31   31\n",
            "0.30000000000000004   169\n",
            "32   32\n",
            "0.30000000000000004   170\n",
            "32   32\n",
            "0.30000000000000004   171\n",
            "32   32\n",
            "0.30000000000000004   172\n",
            "32   32\n",
            "0.30000000000000004   173\n",
            "32   32\n",
            "0.30000000000000004   174\n",
            "32   32\n",
            "0.30000000000000004   175\n",
            "32   32\n",
            "0.30000000000000004   176\n",
            "32   32\n",
            "0.30000000000000004   177\n",
            "32   32\n",
            "0.30000000000000004   178\n",
            "32   32\n",
            "0.30000000000000004   179\n",
            "32   32\n",
            "0.30000000000000004   180\n",
            "32   32\n",
            "0.30000000000000004   181\n",
            "32   32\n",
            "0.30000000000000004   182\n",
            "32   32\n",
            "0.30000000000000004   183\n",
            "32   32\n",
            "0.30000000000000004   184\n",
            "33   33\n",
            "0.30000000000000004   185\n",
            "33   33\n",
            "0.30000000000000004   186\n",
            "33   33\n",
            "0.30000000000000004   187\n",
            "33   33\n",
            "0.30000000000000004   188\n",
            "34   34\n",
            "0.30000000000000004   189\n",
            "34   34\n",
            "0.30000000000000004   190\n",
            "34   34\n",
            "0.30000000000000004   191\n",
            "34   34\n",
            "0.30000000000000004   192\n",
            "34   34\n",
            "0.30000000000000004   193\n",
            "34   34\n",
            "0.30000000000000004   194\n",
            "34   34\n",
            "0.30000000000000004   195\n",
            "34   34\n",
            "0.30000000000000004   196\n",
            "34   34\n",
            "0.30000000000000004   197\n",
            "34   34\n",
            "0.30000000000000004   198\n",
            "34   34\n",
            "0.30000000000000004   199\n",
            "35   35\n",
            "Epsilon value -  0.4\n",
            "\n",
            "0.4   0\n",
            "0   0\n",
            "0.4   1\n",
            "0   0\n",
            "0.4   2\n",
            "0   0\n",
            "0.4   3\n",
            "0   0\n",
            "0.4   4\n",
            "0   0\n",
            "0.4   5\n",
            "0   0\n",
            "0.4   6\n",
            "0   0\n",
            "0.4   7\n",
            "0   0\n",
            "0.4   8\n",
            "0   0\n",
            "0.4   9\n",
            "0   0\n",
            "0.4   10\n",
            "0   0\n",
            "0.4   11\n",
            "0   0\n",
            "0.4   12\n",
            "0   0\n",
            "0.4   13\n",
            "0   0\n",
            "0.4   14\n",
            "0   0\n",
            "0.4   15\n",
            "0   0\n",
            "0.4   16\n",
            "0   0\n",
            "0.4   17\n",
            "0   0\n",
            "0.4   18\n",
            "0   0\n",
            "0.4   19\n",
            "0   0\n",
            "0.4   20\n",
            "1   0\n",
            "0.4   21\n",
            "1   0\n",
            "0.4   22\n",
            "1   0\n",
            "0.4   23\n",
            "1   0\n",
            "0.4   24\n",
            "2   1\n",
            "0.4   25\n",
            "2   1\n",
            "0.4   26\n",
            "3   2\n",
            "0.4   27\n",
            "3   2\n",
            "0.4   28\n",
            "3   2\n",
            "0.4   29\n",
            "3   2\n",
            "0.4   30\n",
            "3   2\n",
            "0.4   31\n",
            "3   2\n",
            "0.4   32\n",
            "3   2\n",
            "0.4   33\n",
            "4   3\n",
            "0.4   34\n",
            "4   3\n",
            "0.4   35\n",
            "4   3\n",
            "0.4   36\n",
            "4   3\n",
            "0.4   37\n",
            "4   3\n",
            "0.4   38\n",
            "4   3\n",
            "0.4   39\n",
            "4   3\n",
            "0.4   40\n",
            "5   4\n",
            "0.4   41\n",
            "5   4\n",
            "0.4   42\n",
            "5   4\n",
            "0.4   43\n",
            "5   4\n",
            "0.4   44\n",
            "5   4\n",
            "0.4   45\n",
            "5   4\n",
            "0.4   46\n",
            "6   4\n",
            "0.4   47\n",
            "7   5\n",
            "0.4   48\n",
            "7   5\n",
            "0.4   49\n",
            "8   6\n",
            "0.4   50\n",
            "8   6\n",
            "0.4   51\n",
            "8   6\n",
            "0.4   52\n",
            "9   7\n",
            "0.4   53\n",
            "10   8\n",
            "0.4   54\n",
            "10   8\n",
            "0.4   55\n",
            "10   8\n",
            "0.4   56\n",
            "10   8\n",
            "0.4   57\n",
            "11   9\n",
            "0.4   58\n",
            "12   10\n",
            "0.4   59\n",
            "13   11\n",
            "0.4   60\n",
            "13   11\n",
            "0.4   61\n",
            "14   12\n",
            "0.4   62\n",
            "14   12\n",
            "0.4   63\n",
            "14   12\n",
            "0.4   64\n",
            "14   12\n",
            "0.4   65\n",
            "15   13\n",
            "0.4   66\n",
            "16   14\n",
            "0.4   67\n",
            "16   14\n",
            "0.4   68\n",
            "16   14\n",
            "0.4   69\n",
            "16   14\n",
            "0.4   70\n",
            "17   15\n",
            "0.4   71\n",
            "18   15\n",
            "0.4   72\n",
            "18   15\n",
            "0.4   73\n",
            "18   15\n",
            "0.4   74\n",
            "19   16\n",
            "0.4   75\n",
            "19   16\n",
            "0.4   76\n",
            "20   17\n",
            "0.4   77\n",
            "20   17\n",
            "0.4   78\n",
            "21   18\n",
            "0.4   79\n",
            "21   18\n",
            "0.4   80\n",
            "21   18\n",
            "0.4   81\n",
            "22   19\n",
            "0.4   82\n",
            "22   19\n",
            "0.4   83\n",
            "23   20\n",
            "0.4   84\n",
            "23   20\n",
            "0.4   85\n",
            "23   20\n",
            "0.4   86\n",
            "24   21\n",
            "0.4   87\n",
            "24   21\n",
            "0.4   88\n",
            "25   22\n",
            "0.4   89\n",
            "25   22\n",
            "0.4   90\n",
            "25   22\n",
            "0.4   91\n",
            "25   22\n",
            "0.4   92\n",
            "25   22\n",
            "0.4   93\n",
            "25   22\n",
            "0.4   94\n",
            "25   22\n",
            "0.4   95\n",
            "25   22\n",
            "0.4   96\n",
            "25   22\n",
            "0.4   97\n",
            "25   22\n",
            "0.4   98\n",
            "25   22\n",
            "0.4   99\n",
            "25   22\n",
            "0.4   100\n",
            "26   23\n",
            "0.4   101\n",
            "26   23\n",
            "0.4   102\n",
            "26   23\n",
            "0.4   103\n",
            "26   23\n",
            "0.4   104\n",
            "26   23\n",
            "0.4   105\n",
            "26   23\n",
            "0.4   106\n",
            "26   23\n",
            "0.4   107\n",
            "26   23\n",
            "0.4   108\n",
            "26   23\n",
            "0.4   109\n",
            "26   23\n",
            "0.4   110\n",
            "26   23\n",
            "0.4   111\n",
            "26   23\n",
            "0.4   112\n",
            "26   23\n",
            "0.4   113\n",
            "26   23\n",
            "0.4   114\n",
            "26   23\n",
            "0.4   115\n",
            "26   23\n",
            "0.4   116\n",
            "26   23\n",
            "0.4   117\n",
            "26   23\n",
            "0.4   118\n",
            "26   23\n",
            "0.4   119\n",
            "27   24\n",
            "0.4   120\n",
            "27   24\n",
            "0.4   121\n",
            "28   25\n",
            "0.4   122\n",
            "28   25\n",
            "0.4   123\n",
            "28   25\n",
            "0.4   124\n",
            "28   25\n",
            "0.4   125\n",
            "28   25\n",
            "0.4   126\n",
            "28   25\n",
            "0.4   127\n",
            "29   26\n",
            "0.4   128\n",
            "29   26\n",
            "0.4   129\n",
            "29   26\n",
            "0.4   130\n",
            "30   27\n",
            "0.4   131\n",
            "30   27\n",
            "0.4   132\n",
            "30   27\n",
            "0.4   133\n",
            "30   27\n",
            "0.4   134\n",
            "30   27\n",
            "0.4   135\n",
            "30   27\n",
            "0.4   136\n",
            "30   27\n",
            "0.4   137\n",
            "30   27\n",
            "0.4   138\n",
            "30   27\n",
            "0.4   139\n",
            "31   28\n",
            "0.4   140\n",
            "31   28\n",
            "0.4   141\n",
            "31   28\n",
            "0.4   142\n",
            "31   28\n",
            "0.4   143\n",
            "32   29\n",
            "0.4   144\n",
            "32   29\n",
            "0.4   145\n",
            "33   30\n",
            "0.4   146\n",
            "33   30\n",
            "0.4   147\n",
            "34   31\n",
            "0.4   148\n",
            "34   31\n",
            "0.4   149\n",
            "34   31\n",
            "0.4   150\n",
            "34   31\n",
            "0.4   151\n",
            "35   32\n",
            "0.4   152\n",
            "35   32\n",
            "0.4   153\n",
            "35   32\n",
            "0.4   154\n",
            "35   32\n",
            "0.4   155\n",
            "35   32\n",
            "0.4   156\n",
            "35   32\n",
            "0.4   157\n",
            "35   32\n",
            "0.4   158\n",
            "35   32\n",
            "0.4   159\n",
            "35   32\n",
            "0.4   160\n",
            "35   32\n",
            "0.4   161\n",
            "35   32\n",
            "0.4   162\n",
            "36   33\n",
            "0.4   163\n",
            "36   33\n",
            "0.4   164\n",
            "37   34\n",
            "0.4   165\n",
            "38   35\n",
            "0.4   166\n",
            "39   36\n",
            "0.4   167\n",
            "39   36\n",
            "0.4   168\n",
            "39   36\n",
            "0.4   169\n",
            "40   37\n",
            "0.4   170\n",
            "40   37\n",
            "0.4   171\n",
            "40   37\n",
            "0.4   172\n",
            "40   37\n",
            "0.4   173\n",
            "40   37\n",
            "0.4   174\n",
            "40   37\n",
            "0.4   175\n",
            "40   37\n",
            "0.4   176\n",
            "40   37\n",
            "0.4   177\n",
            "40   37\n",
            "0.4   178\n",
            "40   37\n",
            "0.4   179\n",
            "40   37\n",
            "0.4   180\n",
            "40   37\n",
            "0.4   181\n",
            "40   37\n",
            "0.4   182\n",
            "40   37\n",
            "0.4   183\n",
            "40   37\n",
            "0.4   184\n",
            "41   38\n",
            "0.4   185\n",
            "41   38\n",
            "0.4   186\n",
            "41   38\n",
            "0.4   187\n",
            "41   38\n",
            "0.4   188\n",
            "42   39\n",
            "0.4   189\n",
            "42   39\n",
            "0.4   190\n",
            "42   39\n",
            "0.4   191\n",
            "42   39\n",
            "0.4   192\n",
            "42   39\n",
            "0.4   193\n",
            "42   39\n",
            "0.4   194\n",
            "42   39\n",
            "0.4   195\n",
            "42   39\n",
            "0.4   196\n",
            "42   39\n",
            "0.4   197\n",
            "42   39\n",
            "0.4   198\n",
            "42   39\n",
            "0.4   199\n",
            "43   40\n",
            "Epsilon value -  0.5\n",
            "\n",
            "0.5   0\n",
            "0   0\n",
            "0.5   1\n",
            "0   0\n",
            "0.5   2\n",
            "0   0\n",
            "0.5   3\n",
            "0   0\n",
            "0.5   4\n",
            "0   0\n",
            "0.5   5\n",
            "0   0\n",
            "0.5   6\n",
            "0   0\n",
            "0.5   7\n",
            "0   0\n",
            "0.5   8\n",
            "0   0\n",
            "0.5   9\n",
            "0   0\n",
            "0.5   10\n",
            "0   0\n",
            "0.5   11\n",
            "0   0\n",
            "0.5   12\n",
            "0   0\n",
            "0.5   13\n",
            "0   0\n",
            "0.5   14\n",
            "0   0\n",
            "0.5   15\n",
            "0   0\n",
            "0.5   16\n",
            "0   0\n",
            "0.5   17\n",
            "0   0\n",
            "0.5   18\n",
            "0   0\n",
            "0.5   19\n",
            "0   0\n",
            "0.5   20\n",
            "1   1\n",
            "0.5   21\n",
            "1   1\n",
            "0.5   22\n",
            "1   1\n",
            "0.5   23\n",
            "1   1\n",
            "0.5   24\n",
            "2   2\n",
            "0.5   25\n",
            "2   2\n",
            "0.5   26\n",
            "3   3\n",
            "0.5   27\n",
            "3   3\n",
            "0.5   28\n",
            "3   3\n",
            "0.5   29\n",
            "3   3\n",
            "0.5   30\n",
            "3   3\n",
            "0.5   31\n",
            "3   3\n",
            "0.5   32\n",
            "3   3\n",
            "0.5   33\n",
            "4   4\n",
            "0.5   34\n",
            "4   4\n",
            "0.5   35\n",
            "4   4\n",
            "0.5   36\n",
            "4   4\n",
            "0.5   37\n",
            "4   4\n",
            "0.5   38\n",
            "4   4\n",
            "0.5   39\n",
            "4   4\n",
            "0.5   40\n",
            "5   5\n",
            "0.5   41\n",
            "5   5\n",
            "0.5   42\n",
            "5   6\n",
            "0.5   43\n",
            "5   6\n",
            "0.5   44\n",
            "5   6\n",
            "0.5   45\n",
            "5   6\n",
            "0.5   46\n",
            "6   7\n",
            "0.5   47\n",
            "7   8\n",
            "0.5   48\n",
            "7   8\n",
            "0.5   49\n",
            "8   9\n",
            "0.5   50\n",
            "8   9\n",
            "0.5   51\n",
            "8   9\n",
            "0.5   52\n",
            "9   10\n",
            "0.5   53\n",
            "10   11\n",
            "0.5   54\n",
            "10   11\n",
            "0.5   55\n",
            "10   11\n",
            "0.5   56\n",
            "10   11\n",
            "0.5   57\n",
            "11   12\n",
            "0.5   58\n",
            "12   13\n",
            "0.5   59\n",
            "13   14\n",
            "0.5   60\n",
            "13   14\n",
            "0.5   61\n",
            "14   15\n",
            "0.5   62\n",
            "14   15\n",
            "0.5   63\n",
            "14   15\n",
            "0.5   64\n",
            "14   15\n",
            "0.5   65\n",
            "15   16\n",
            "0.5   66\n",
            "16   17\n",
            "0.5   67\n",
            "16   17\n",
            "0.5   68\n",
            "16   17\n",
            "0.5   69\n",
            "16   17\n",
            "0.5   70\n",
            "17   17\n",
            "0.5   71\n",
            "18   18\n",
            "0.5   72\n",
            "18   18\n",
            "0.5   73\n",
            "18   18\n",
            "0.5   74\n",
            "19   19\n",
            "0.5   75\n",
            "19   19\n",
            "0.5   76\n",
            "20   20\n",
            "0.5   77\n",
            "20   20\n",
            "0.5   78\n",
            "21   21\n",
            "0.5   79\n",
            "21   21\n",
            "0.5   80\n",
            "21   21\n",
            "0.5   81\n",
            "22   22\n",
            "0.5   82\n",
            "22   22\n",
            "0.5   83\n",
            "23   23\n",
            "0.5   84\n",
            "23   23\n",
            "0.5   85\n",
            "23   23\n",
            "0.5   86\n",
            "24   24\n",
            "0.5   87\n",
            "24   24\n",
            "0.5   88\n",
            "25   25\n",
            "0.5   89\n",
            "25   25\n",
            "0.5   90\n",
            "25   25\n",
            "0.5   91\n",
            "25   25\n",
            "0.5   92\n",
            "25   25\n",
            "0.5   93\n",
            "25   25\n",
            "0.5   94\n",
            "25   25\n",
            "0.5   95\n",
            "25   25\n",
            "0.5   96\n",
            "25   25\n",
            "0.5   97\n",
            "25   25\n",
            "0.5   98\n",
            "25   25\n",
            "0.5   99\n",
            "25   25\n",
            "0.5   100\n",
            "26   26\n",
            "0.5   101\n",
            "26   26\n",
            "0.5   102\n",
            "26   26\n",
            "0.5   103\n",
            "26   27\n",
            "0.5   104\n",
            "26   27\n",
            "0.5   105\n",
            "26   27\n",
            "0.5   106\n",
            "26   27\n",
            "0.5   107\n",
            "26   27\n",
            "0.5   108\n",
            "26   27\n",
            "0.5   109\n",
            "26   27\n",
            "0.5   110\n",
            "26   28\n",
            "0.5   111\n",
            "26   28\n",
            "0.5   112\n",
            "26   28\n",
            "0.5   113\n",
            "26   28\n",
            "0.5   114\n",
            "26   28\n",
            "0.5   115\n",
            "26   28\n",
            "0.5   116\n",
            "26   28\n",
            "0.5   117\n",
            "26   28\n",
            "0.5   118\n",
            "26   28\n",
            "0.5   119\n",
            "27   29\n",
            "0.5   120\n",
            "27   29\n",
            "0.5   121\n",
            "28   30\n",
            "0.5   122\n",
            "28   30\n",
            "0.5   123\n",
            "28   30\n",
            "0.5   124\n",
            "28   30\n",
            "0.5   125\n",
            "28   30\n",
            "0.5   126\n",
            "28   30\n",
            "0.5   127\n",
            "29   31\n",
            "0.5   128\n",
            "29   31\n",
            "0.5   129\n",
            "29   31\n",
            "0.5   130\n",
            "30   32\n",
            "0.5   131\n",
            "30   32\n",
            "0.5   132\n",
            "30   32\n",
            "0.5   133\n",
            "30   32\n",
            "0.5   134\n",
            "30   32\n",
            "0.5   135\n",
            "30   32\n",
            "0.5   136\n",
            "30   32\n",
            "0.5   137\n",
            "30   32\n",
            "0.5   138\n",
            "30   32\n",
            "0.5   139\n",
            "31   33\n",
            "0.5   140\n",
            "31   33\n",
            "0.5   141\n",
            "31   33\n",
            "0.5   142\n",
            "31   33\n",
            "0.5   143\n",
            "32   33\n",
            "0.5   144\n",
            "32   33\n",
            "0.5   145\n",
            "33   34\n",
            "0.5   146\n",
            "33   34\n",
            "0.5   147\n",
            "34   35\n",
            "0.5   148\n",
            "34   35\n",
            "0.5   149\n",
            "34   35\n",
            "0.5   150\n",
            "34   35\n",
            "0.5   151\n",
            "35   36\n",
            "0.5   152\n",
            "35   36\n",
            "0.5   153\n",
            "35   36\n",
            "0.5   154\n",
            "35   36\n",
            "0.5   155\n",
            "35   36\n",
            "0.5   156\n",
            "35   36\n",
            "0.5   157\n",
            "35   36\n",
            "0.5   158\n",
            "35   36\n",
            "0.5   159\n",
            "35   36\n",
            "0.5   160\n",
            "35   36\n",
            "0.5   161\n",
            "35   36\n",
            "0.5   162\n",
            "36   37\n",
            "0.5   163\n",
            "36   37\n",
            "0.5   164\n",
            "37   38\n",
            "0.5   165\n",
            "38   39\n",
            "0.5   166\n",
            "39   40\n",
            "0.5   167\n",
            "39   40\n",
            "0.5   168\n",
            "39   40\n",
            "0.5   169\n",
            "40   41\n",
            "0.5   170\n",
            "40   41\n",
            "0.5   171\n",
            "40   41\n",
            "0.5   172\n",
            "40   41\n",
            "0.5   173\n",
            "40   42\n",
            "0.5   174\n",
            "40   42\n",
            "0.5   175\n",
            "40   42\n",
            "0.5   176\n",
            "40   42\n",
            "0.5   177\n",
            "40   42\n",
            "0.5   178\n",
            "40   42\n",
            "0.5   179\n",
            "40   42\n",
            "0.5   180\n",
            "40   42\n",
            "0.5   181\n",
            "40   42\n",
            "0.5   182\n",
            "40   42\n",
            "0.5   183\n",
            "40   42\n",
            "0.5   184\n",
            "41   43\n",
            "0.5   185\n",
            "41   43\n",
            "0.5   186\n",
            "41   43\n",
            "0.5   187\n",
            "41   43\n",
            "0.5   188\n",
            "42   44\n",
            "0.5   189\n",
            "42   44\n",
            "0.5   190\n",
            "42   44\n",
            "0.5   191\n",
            "42   44\n",
            "0.5   192\n",
            "42   44\n",
            "0.5   193\n",
            "42   44\n",
            "0.5   194\n",
            "42   44\n",
            "0.5   195\n",
            "42   44\n",
            "0.5   196\n",
            "42   44\n",
            "0.5   197\n",
            "42   44\n",
            "0.5   198\n",
            "42   44\n",
            "0.5   199\n",
            "43   45\n",
            "Epsilon value -  0.6\n",
            "\n",
            "0.6   0\n",
            "0   0\n",
            "0.6   1\n",
            "0   0\n",
            "0.6   2\n",
            "0   1\n",
            "0.6   3\n",
            "0   1\n",
            "0.6   4\n",
            "0   1\n",
            "0.6   5\n",
            "0   1\n",
            "0.6   6\n",
            "0   1\n",
            "0.6   7\n",
            "0   1\n",
            "0.6   8\n",
            "0   1\n",
            "0.6   9\n",
            "0   1\n",
            "0.6   10\n",
            "0   1\n",
            "0.6   11\n",
            "0   1\n",
            "0.6   12\n",
            "0   1\n",
            "0.6   13\n",
            "0   1\n",
            "0.6   14\n",
            "0   1\n",
            "0.6   15\n",
            "0   1\n",
            "0.6   16\n",
            "0   1\n",
            "0.6   17\n",
            "0   1\n",
            "0.6   18\n",
            "0   1\n",
            "0.6   19\n",
            "0   1\n",
            "0.6   20\n",
            "1   2\n",
            "0.6   21\n",
            "1   2\n",
            "0.6   22\n",
            "1   2\n",
            "0.6   23\n",
            "1   2\n",
            "0.6   24\n",
            "2   3\n",
            "0.6   25\n",
            "2   3\n",
            "0.6   26\n",
            "3   4\n",
            "0.6   27\n",
            "3   4\n",
            "0.6   28\n",
            "3   4\n",
            "0.6   29\n",
            "3   4\n",
            "0.6   30\n",
            "3   4\n",
            "0.6   31\n",
            "3   4\n",
            "0.6   32\n",
            "3   4\n",
            "0.6   33\n",
            "4   5\n",
            "0.6   34\n",
            "4   5\n",
            "0.6   35\n",
            "4   5\n",
            "0.6   36\n",
            "4   6\n",
            "0.6   37\n",
            "4   6\n",
            "0.6   38\n",
            "4   6\n",
            "0.6   39\n",
            "4   6\n",
            "0.6   40\n",
            "5   7\n",
            "0.6   41\n",
            "5   7\n",
            "0.6   42\n",
            "5   8\n",
            "0.6   43\n",
            "5   8\n",
            "0.6   44\n",
            "5   8\n",
            "0.6   45\n",
            "5   8\n",
            "0.6   46\n",
            "6   9\n",
            "0.6   47\n",
            "7   10\n",
            "0.6   48\n",
            "7   10\n",
            "0.6   49\n",
            "8   11\n",
            "0.6   50\n",
            "8   11\n",
            "0.6   51\n",
            "8   11\n",
            "0.6   52\n",
            "9   11\n",
            "0.6   53\n",
            "10   12\n",
            "0.6   54\n",
            "10   12\n",
            "0.6   55\n",
            "10   12\n",
            "0.6   56\n",
            "10   12\n",
            "0.6   57\n",
            "11   13\n",
            "0.6   58\n",
            "12   14\n",
            "0.6   59\n",
            "13   15\n",
            "0.6   60\n",
            "13   15\n",
            "0.6   61\n",
            "14   16\n",
            "0.6   62\n",
            "14   17\n",
            "0.6   63\n",
            "14   17\n",
            "0.6   64\n",
            "14   17\n",
            "0.6   65\n",
            "15   18\n",
            "0.6   66\n",
            "16   19\n",
            "0.6   67\n",
            "16   19\n",
            "0.6   68\n",
            "16   20\n",
            "0.6   69\n",
            "16   20\n",
            "0.6   70\n",
            "17   20\n",
            "0.6   71\n",
            "18   21\n",
            "0.6   72\n",
            "18   21\n",
            "0.6   73\n",
            "18   21\n",
            "0.6   74\n",
            "19   22\n",
            "0.6   75\n",
            "19   22\n",
            "0.6   76\n",
            "20   23\n",
            "0.6   77\n",
            "20   23\n",
            "0.6   78\n",
            "21   24\n",
            "0.6   79\n",
            "21   24\n",
            "0.6   80\n",
            "21   24\n",
            "0.6   81\n",
            "22   25\n",
            "0.6   82\n",
            "22   25\n",
            "0.6   83\n",
            "23   26\n",
            "0.6   84\n",
            "23   26\n",
            "0.6   85\n",
            "23   26\n",
            "0.6   86\n",
            "24   27\n",
            "0.6   87\n",
            "24   27\n",
            "0.6   88\n",
            "25   28\n",
            "0.6   89\n",
            "25   28\n",
            "0.6   90\n",
            "25   28\n",
            "0.6   91\n",
            "25   28\n",
            "0.6   92\n",
            "25   28\n",
            "0.6   93\n",
            "25   28\n",
            "0.6   94\n",
            "25   28\n",
            "0.6   95\n",
            "25   28\n",
            "0.6   96\n",
            "25   28\n",
            "0.6   97\n",
            "25   28\n",
            "0.6   98\n",
            "25   28\n",
            "0.6   99\n",
            "25   28\n",
            "0.6   100\n",
            "26   29\n",
            "0.6   101\n",
            "26   29\n",
            "0.6   102\n",
            "26   29\n",
            "0.6   103\n",
            "26   30\n",
            "0.6   104\n",
            "26   30\n",
            "0.6   105\n",
            "26   30\n",
            "0.6   106\n",
            "26   30\n",
            "0.6   107\n",
            "26   30\n",
            "0.6   108\n",
            "26   30\n",
            "0.6   109\n",
            "26   30\n",
            "0.6   110\n",
            "26   31\n",
            "0.6   111\n",
            "26   31\n",
            "0.6   112\n",
            "26   31\n",
            "0.6   113\n",
            "26   31\n",
            "0.6   114\n",
            "26   31\n",
            "0.6   115\n",
            "26   31\n",
            "0.6   116\n",
            "26   31\n",
            "0.6   117\n",
            "26   31\n",
            "0.6   118\n",
            "26   31\n",
            "0.6   119\n",
            "27   32\n",
            "0.6   120\n",
            "27   32\n",
            "0.6   121\n",
            "28   33\n",
            "0.6   122\n",
            "28   33\n",
            "0.6   123\n",
            "28   33\n",
            "0.6   124\n",
            "28   33\n",
            "0.6   125\n",
            "28   33\n",
            "0.6   126\n",
            "28   33\n",
            "0.6   127\n",
            "29   34\n",
            "0.6   128\n",
            "29   34\n",
            "0.6   129\n",
            "29   34\n",
            "0.6   130\n",
            "30   35\n",
            "0.6   131\n",
            "30   35\n",
            "0.6   132\n",
            "30   35\n",
            "0.6   133\n",
            "30   35\n",
            "0.6   134\n",
            "30   35\n",
            "0.6   135\n",
            "30   35\n",
            "0.6   136\n",
            "30   35\n",
            "0.6   137\n",
            "30   35\n",
            "0.6   138\n",
            "30   35\n",
            "0.6   139\n",
            "31   36\n",
            "0.6   140\n",
            "31   37\n",
            "0.6   141\n",
            "31   37\n",
            "0.6   142\n",
            "31   37\n",
            "0.6   143\n",
            "32   37\n",
            "0.6   144\n",
            "32   37\n",
            "0.6   145\n",
            "33   38\n",
            "0.6   146\n",
            "33   38\n",
            "0.6   147\n",
            "34   39\n",
            "0.6   148\n",
            "34   39\n",
            "0.6   149\n",
            "34   39\n",
            "0.6   150\n",
            "34   39\n",
            "0.6   151\n",
            "35   40\n",
            "0.6   152\n",
            "35   40\n",
            "0.6   153\n",
            "35   40\n",
            "0.6   154\n",
            "35   40\n",
            "0.6   155\n",
            "35   40\n",
            "0.6   156\n",
            "35   40\n",
            "0.6   157\n",
            "35   40\n",
            "0.6   158\n",
            "35   40\n",
            "0.6   159\n",
            "35   40\n",
            "0.6   160\n",
            "35   40\n",
            "0.6   161\n",
            "35   40\n",
            "0.6   162\n",
            "36   41\n",
            "0.6   163\n",
            "36   41\n",
            "0.6   164\n",
            "37   42\n",
            "0.6   165\n",
            "38   43\n",
            "0.6   166\n",
            "39   44\n",
            "0.6   167\n",
            "39   44\n",
            "0.6   168\n",
            "39   44\n",
            "0.6   169\n",
            "40   45\n",
            "0.6   170\n",
            "40   45\n",
            "0.6   171\n",
            "40   45\n",
            "0.6   172\n",
            "40   45\n",
            "0.6   173\n",
            "40   46\n",
            "0.6   174\n",
            "40   46\n",
            "0.6   175\n",
            "40   46\n",
            "0.6   176\n",
            "40   46\n",
            "0.6   177\n",
            "40   46\n",
            "0.6   178\n",
            "40   46\n",
            "0.6   179\n",
            "40   46\n",
            "0.6   180\n",
            "40   46\n",
            "0.6   181\n",
            "40   46\n",
            "0.6   182\n",
            "40   46\n",
            "0.6   183\n",
            "40   46\n",
            "0.6   184\n",
            "41   47\n",
            "0.6   185\n",
            "41   47\n",
            "0.6   186\n",
            "41   47\n",
            "0.6   187\n",
            "41   47\n",
            "0.6   188\n",
            "42   48\n",
            "0.6   189\n",
            "42   48\n",
            "0.6   190\n",
            "42   48\n",
            "0.6   191\n",
            "42   48\n",
            "0.6   192\n",
            "42   48\n",
            "0.6   193\n",
            "42   48\n",
            "0.6   194\n",
            "42   48\n",
            "0.6   195\n",
            "42   48\n",
            "0.6   196\n",
            "42   48\n",
            "0.6   197\n",
            "42   49\n",
            "0.6   198\n",
            "42   49\n",
            "0.6   199\n",
            "43   50\n",
            "Epsilon value -  0.7\n",
            "\n",
            "0.7   0\n",
            "0   1\n",
            "0.7   1\n",
            "0   1\n",
            "0.7   2\n",
            "0   2\n",
            "0.7   3\n",
            "0   2\n",
            "0.7   4\n",
            "0   2\n",
            "0.7   5\n",
            "0   2\n",
            "0.7   6\n",
            "0   2\n",
            "0.7   7\n",
            "0   2\n",
            "0.7   8\n",
            "0   2\n",
            "0.7   9\n",
            "0   2\n",
            "0.7   10\n",
            "0   2\n",
            "0.7   11\n",
            "0   2\n",
            "0.7   12\n",
            "0   2\n",
            "0.7   13\n",
            "0   2\n",
            "0.7   14\n",
            "0   2\n",
            "0.7   15\n",
            "0   2\n",
            "0.7   16\n",
            "0   2\n",
            "0.7   17\n",
            "0   2\n",
            "0.7   18\n",
            "0   2\n",
            "0.7   19\n",
            "0   2\n",
            "0.7   20\n",
            "1   3\n",
            "0.7   21\n",
            "1   3\n",
            "0.7   22\n",
            "1   3\n",
            "0.7   23\n",
            "1   3\n",
            "0.7   24\n",
            "2   4\n",
            "0.7   25\n",
            "2   4\n",
            "0.7   26\n",
            "3   5\n",
            "0.7   27\n",
            "3   5\n",
            "0.7   28\n",
            "3   5\n",
            "0.7   29\n",
            "3   5\n",
            "0.7   30\n",
            "3   5\n",
            "0.7   31\n",
            "3   5\n",
            "0.7   32\n",
            "3   5\n",
            "0.7   33\n",
            "4   6\n",
            "0.7   34\n",
            "4   6\n",
            "0.7   35\n",
            "4   6\n",
            "0.7   36\n",
            "4   7\n",
            "0.7   37\n",
            "4   7\n",
            "0.7   38\n",
            "4   7\n",
            "0.7   39\n",
            "4   7\n",
            "0.7   40\n",
            "5   8\n",
            "0.7   41\n",
            "5   8\n",
            "0.7   42\n",
            "5   9\n",
            "0.7   43\n",
            "5   9\n",
            "0.7   44\n",
            "5   9\n",
            "0.7   45\n",
            "5   9\n",
            "0.7   46\n",
            "6   10\n",
            "0.7   47\n",
            "7   11\n",
            "0.7   48\n",
            "7   11\n",
            "0.7   49\n",
            "8   12\n",
            "0.7   50\n",
            "8   12\n",
            "0.7   51\n",
            "8   12\n",
            "0.7   52\n",
            "9   12\n",
            "0.7   53\n",
            "10   13\n",
            "0.7   54\n",
            "10   13\n",
            "0.7   55\n",
            "10   13\n",
            "0.7   56\n",
            "10   13\n",
            "0.7   57\n",
            "11   14\n",
            "0.7   58\n",
            "12   15\n",
            "0.7   59\n",
            "13   16\n",
            "0.7   60\n",
            "13   16\n",
            "0.7   61\n",
            "14   17\n",
            "0.7   62\n",
            "14   18\n",
            "0.7   63\n",
            "14   18\n",
            "0.7   64\n",
            "14   18\n",
            "0.7   65\n",
            "15   19\n",
            "0.7   66\n",
            "16   20\n",
            "0.7   67\n",
            "16   20\n",
            "0.7   68\n",
            "16   21\n",
            "0.7   69\n",
            "16   21\n",
            "0.7   70\n",
            "17   21\n",
            "0.7   71\n",
            "18   22\n",
            "0.7   72\n",
            "18   22\n",
            "0.7   73\n",
            "18   22\n",
            "0.7   74\n",
            "19   23\n",
            "0.7   75\n",
            "19   23\n",
            "0.7   76\n",
            "20   24\n",
            "0.7   77\n",
            "20   24\n",
            "0.7   78\n",
            "21   25\n",
            "0.7   79\n",
            "21   25\n",
            "0.7   80\n",
            "21   25\n",
            "0.7   81\n",
            "22   26\n",
            "0.7   82\n",
            "22   26\n",
            "0.7   83\n",
            "23   27\n",
            "0.7   84\n",
            "23   27\n",
            "0.7   85\n",
            "23   27\n",
            "0.7   86\n",
            "24   28\n",
            "0.7   87\n",
            "24   28\n",
            "0.7   88\n",
            "25   29\n",
            "0.7   89\n",
            "25   29\n",
            "0.7   90\n",
            "25   29\n",
            "0.7   91\n",
            "25   29\n",
            "0.7   92\n",
            "25   29\n",
            "0.7   93\n",
            "25   29\n",
            "0.7   94\n",
            "25   30\n",
            "0.7   95\n",
            "25   30\n",
            "0.7   96\n",
            "25   30\n",
            "0.7   97\n",
            "25   30\n",
            "0.7   98\n",
            "25   30\n",
            "0.7   99\n",
            "25   30\n",
            "0.7   100\n",
            "26   31\n",
            "0.7   101\n",
            "26   31\n",
            "0.7   102\n",
            "26   31\n",
            "0.7   103\n",
            "26   32\n",
            "0.7   104\n",
            "26   32\n",
            "0.7   105\n",
            "26   32\n",
            "0.7   106\n",
            "26   33\n",
            "0.7   107\n",
            "26   33\n",
            "0.7   108\n",
            "26   33\n",
            "0.7   109\n",
            "26   33\n",
            "0.7   110\n",
            "26   34\n",
            "0.7   111\n",
            "26   34\n",
            "0.7   112\n",
            "26   34\n",
            "0.7   113\n",
            "26   34\n",
            "0.7   114\n",
            "26   34\n",
            "0.7   115\n",
            "26   34\n",
            "0.7   116\n",
            "26   34\n",
            "0.7   117\n",
            "26   34\n",
            "0.7   118\n",
            "26   34\n",
            "0.7   119\n",
            "27   35\n",
            "0.7   120\n",
            "27   35\n",
            "0.7   121\n",
            "28   36\n",
            "0.7   122\n",
            "28   36\n",
            "0.7   123\n",
            "28   36\n",
            "0.7   124\n",
            "28   36\n",
            "0.7   125\n",
            "28   36\n",
            "0.7   126\n",
            "28   36\n",
            "0.7   127\n",
            "29   37\n",
            "0.7   128\n",
            "29   38\n",
            "0.7   129\n",
            "29   38\n",
            "0.7   130\n",
            "30   39\n",
            "0.7   131\n",
            "30   39\n",
            "0.7   132\n",
            "30   39\n",
            "0.7   133\n",
            "30   39\n",
            "0.7   134\n",
            "30   39\n",
            "0.7   135\n",
            "30   40\n",
            "0.7   136\n",
            "30   40\n",
            "0.7   137\n",
            "30   40\n",
            "0.7   138\n",
            "30   40\n",
            "0.7   139\n",
            "31   41\n",
            "0.7   140\n",
            "31   42\n",
            "0.7   141\n",
            "31   42\n",
            "0.7   142\n",
            "31   42\n",
            "0.7   143\n",
            "32   42\n",
            "0.7   144\n",
            "32   42\n",
            "0.7   145\n",
            "33   43\n",
            "0.7   146\n",
            "33   43\n",
            "0.7   147\n",
            "34   44\n",
            "0.7   148\n",
            "34   44\n",
            "0.7   149\n",
            "34   44\n",
            "0.7   150\n",
            "34   44\n",
            "0.7   151\n",
            "35   45\n",
            "0.7   152\n",
            "35   45\n",
            "0.7   153\n",
            "35   45\n",
            "0.7   154\n",
            "35   45\n",
            "0.7   155\n",
            "35   45\n",
            "0.7   156\n",
            "35   45\n",
            "0.7   157\n",
            "35   45\n",
            "0.7   158\n",
            "35   46\n",
            "0.7   159\n",
            "35   46\n",
            "0.7   160\n",
            "35   46\n",
            "0.7   161\n",
            "35   46\n",
            "0.7   162\n",
            "36   47\n",
            "0.7   163\n",
            "36   47\n",
            "0.7   164\n",
            "37   48\n",
            "0.7   165\n",
            "38   49\n",
            "0.7   166\n",
            "39   50\n",
            "0.7   167\n",
            "39   50\n",
            "0.7   168\n",
            "39   50\n",
            "0.7   169\n",
            "40   51\n",
            "0.7   170\n",
            "40   51\n",
            "0.7   171\n",
            "40   51\n",
            "0.7   172\n",
            "40   51\n",
            "0.7   173\n",
            "40   52\n",
            "0.7   174\n",
            "40   52\n",
            "0.7   175\n",
            "40   52\n",
            "0.7   176\n",
            "40   52\n",
            "0.7   177\n",
            "40   52\n",
            "0.7   178\n",
            "40   52\n",
            "0.7   179\n",
            "40   52\n",
            "0.7   180\n",
            "40   52\n",
            "0.7   181\n",
            "40   52\n",
            "0.7   182\n",
            "40   52\n",
            "0.7   183\n",
            "40   52\n",
            "0.7   184\n",
            "41   53\n",
            "0.7   185\n",
            "41   53\n",
            "0.7   186\n",
            "41   53\n",
            "0.7   187\n",
            "41   53\n",
            "0.7   188\n",
            "42   54\n",
            "0.7   189\n",
            "42   54\n",
            "0.7   190\n",
            "42   54\n",
            "0.7   191\n",
            "42   54\n",
            "0.7   192\n",
            "42   54\n",
            "0.7   193\n",
            "42   54\n",
            "0.7   194\n",
            "42   54\n",
            "0.7   195\n",
            "42   54\n",
            "0.7   196\n",
            "42   54\n",
            "0.7   197\n",
            "42   55\n",
            "0.7   198\n",
            "42   55\n",
            "0.7   199\n",
            "43   56\n",
            "Epsilon value -  0.7999999999999999\n",
            "\n",
            "0.7999999999999999   0\n",
            "0   1\n",
            "0.7999999999999999   1\n",
            "0   1\n",
            "0.7999999999999999   2\n",
            "0   2\n",
            "0.7999999999999999   3\n",
            "0   2\n",
            "0.7999999999999999   4\n",
            "0   2\n",
            "0.7999999999999999   5\n",
            "0   3\n",
            "0.7999999999999999   6\n",
            "0   3\n",
            "0.7999999999999999   7\n",
            "0   3\n",
            "0.7999999999999999   8\n",
            "0   3\n",
            "0.7999999999999999   9\n",
            "0   3\n",
            "0.7999999999999999   10\n",
            "0   3\n",
            "0.7999999999999999   11\n",
            "0   3\n",
            "0.7999999999999999   12\n",
            "0   3\n",
            "0.7999999999999999   13\n",
            "0   3\n",
            "0.7999999999999999   14\n",
            "0   3\n",
            "0.7999999999999999   15\n",
            "0   3\n",
            "0.7999999999999999   16\n",
            "0   3\n",
            "0.7999999999999999   17\n",
            "0   3\n",
            "0.7999999999999999   18\n",
            "0   3\n",
            "0.7999999999999999   19\n",
            "0   3\n",
            "0.7999999999999999   20\n",
            "1   4\n",
            "0.7999999999999999   21\n",
            "1   4\n",
            "0.7999999999999999   22\n",
            "1   4\n",
            "0.7999999999999999   23\n",
            "1   4\n",
            "0.7999999999999999   24\n",
            "2   5\n",
            "0.7999999999999999   25\n",
            "2   5\n",
            "0.7999999999999999   26\n",
            "3   6\n",
            "0.7999999999999999   27\n",
            "3   6\n",
            "0.7999999999999999   28\n",
            "3   6\n",
            "0.7999999999999999   29\n",
            "3   6\n",
            "0.7999999999999999   30\n",
            "3   7\n",
            "0.7999999999999999   31\n",
            "3   7\n",
            "0.7999999999999999   32\n",
            "3   7\n",
            "0.7999999999999999   33\n",
            "4   8\n",
            "0.7999999999999999   34\n",
            "4   8\n",
            "0.7999999999999999   35\n",
            "4   9\n",
            "0.7999999999999999   36\n",
            "4   10\n",
            "0.7999999999999999   37\n",
            "4   10\n",
            "0.7999999999999999   38\n",
            "4   10\n",
            "0.7999999999999999   39\n",
            "4   10\n",
            "0.7999999999999999   40\n",
            "5   11\n",
            "0.7999999999999999   41\n",
            "5   11\n",
            "0.7999999999999999   42\n",
            "5   12\n",
            "0.7999999999999999   43\n",
            "5   12\n",
            "0.7999999999999999   44\n",
            "5   12\n",
            "0.7999999999999999   45\n",
            "5   12\n",
            "0.7999999999999999   46\n",
            "6   13\n",
            "0.7999999999999999   47\n",
            "7   14\n",
            "0.7999999999999999   48\n",
            "7   14\n",
            "0.7999999999999999   49\n",
            "8   15\n",
            "0.7999999999999999   50\n",
            "8   16\n",
            "0.7999999999999999   51\n",
            "8   16\n",
            "0.7999999999999999   52\n",
            "9   16\n",
            "0.7999999999999999   53\n",
            "10   17\n",
            "0.7999999999999999   54\n",
            "10   17\n",
            "0.7999999999999999   55\n",
            "10   17\n",
            "0.7999999999999999   56\n",
            "10   17\n",
            "0.7999999999999999   57\n",
            "11   18\n",
            "0.7999999999999999   58\n",
            "12   19\n",
            "0.7999999999999999   59\n",
            "13   20\n",
            "0.7999999999999999   60\n",
            "13   20\n",
            "0.7999999999999999   61\n",
            "14   21\n",
            "0.7999999999999999   62\n",
            "14   22\n",
            "0.7999999999999999   63\n",
            "14   22\n",
            "0.7999999999999999   64\n",
            "14   22\n",
            "0.7999999999999999   65\n",
            "15   23\n",
            "0.7999999999999999   66\n",
            "16   24\n",
            "0.7999999999999999   67\n",
            "16   24\n",
            "0.7999999999999999   68\n",
            "16   25\n",
            "0.7999999999999999   69\n",
            "16   25\n",
            "0.7999999999999999   70\n",
            "17   25\n",
            "0.7999999999999999   71\n",
            "18   26\n",
            "0.7999999999999999   72\n",
            "18   26\n",
            "0.7999999999999999   73\n",
            "18   26\n",
            "0.7999999999999999   74\n",
            "19   27\n",
            "0.7999999999999999   75\n",
            "19   27\n",
            "0.7999999999999999   76\n",
            "20   28\n",
            "0.7999999999999999   77\n",
            "20   28\n",
            "0.7999999999999999   78\n",
            "21   29\n",
            "0.7999999999999999   79\n",
            "21   29\n",
            "0.7999999999999999   80\n",
            "21   29\n",
            "0.7999999999999999   81\n",
            "22   30\n",
            "0.7999999999999999   82\n",
            "22   30\n",
            "0.7999999999999999   83\n",
            "23   31\n",
            "0.7999999999999999   84\n",
            "23   31\n",
            "0.7999999999999999   85\n",
            "23   31\n",
            "0.7999999999999999   86\n",
            "24   32\n",
            "0.7999999999999999   87\n",
            "24   32\n",
            "0.7999999999999999   88\n",
            "25   33\n",
            "0.7999999999999999   89\n",
            "25   33\n",
            "0.7999999999999999   90\n",
            "25   33\n",
            "0.7999999999999999   91\n",
            "25   33\n",
            "0.7999999999999999   92\n",
            "25   33\n",
            "0.7999999999999999   93\n",
            "25   33\n",
            "0.7999999999999999   94\n",
            "25   34\n",
            "0.7999999999999999   95\n",
            "25   34\n",
            "0.7999999999999999   96\n",
            "25   34\n",
            "0.7999999999999999   97\n",
            "25   34\n",
            "0.7999999999999999   98\n",
            "25   34\n",
            "0.7999999999999999   99\n",
            "25   34\n",
            "0.7999999999999999   100\n",
            "26   35\n",
            "0.7999999999999999   101\n",
            "26   35\n",
            "0.7999999999999999   102\n",
            "26   35\n",
            "0.7999999999999999   103\n",
            "26   36\n",
            "0.7999999999999999   104\n",
            "26   36\n",
            "0.7999999999999999   105\n",
            "26   36\n",
            "0.7999999999999999   106\n",
            "26   37\n",
            "0.7999999999999999   107\n",
            "26   37\n",
            "0.7999999999999999   108\n",
            "26   37\n",
            "0.7999999999999999   109\n",
            "26   37\n",
            "0.7999999999999999   110\n",
            "26   38\n",
            "0.7999999999999999   111\n",
            "26   38\n",
            "0.7999999999999999   112\n",
            "26   39\n",
            "0.7999999999999999   113\n",
            "26   39\n",
            "0.7999999999999999   114\n",
            "26   39\n",
            "0.7999999999999999   115\n",
            "26   39\n",
            "0.7999999999999999   116\n",
            "26   39\n",
            "0.7999999999999999   117\n",
            "26   40\n",
            "0.7999999999999999   118\n",
            "26   40\n",
            "0.7999999999999999   119\n",
            "27   41\n",
            "0.7999999999999999   120\n",
            "27   41\n",
            "0.7999999999999999   121\n",
            "28   42\n",
            "0.7999999999999999   122\n",
            "28   42\n",
            "0.7999999999999999   123\n",
            "28   42\n",
            "0.7999999999999999   124\n",
            "28   42\n",
            "0.7999999999999999   125\n",
            "28   42\n",
            "0.7999999999999999   126\n",
            "28   43\n",
            "0.7999999999999999   127\n",
            "29   44\n",
            "0.7999999999999999   128\n",
            "29   45\n",
            "0.7999999999999999   129\n",
            "29   45\n",
            "0.7999999999999999   130\n",
            "30   46\n",
            "0.7999999999999999   131\n",
            "30   46\n",
            "0.7999999999999999   132\n",
            "30   46\n",
            "0.7999999999999999   133\n",
            "30   46\n",
            "0.7999999999999999   134\n",
            "30   46\n",
            "0.7999999999999999   135\n",
            "30   47\n",
            "0.7999999999999999   136\n",
            "30   47\n",
            "0.7999999999999999   137\n",
            "30   47\n",
            "0.7999999999999999   138\n",
            "30   47\n",
            "0.7999999999999999   139\n",
            "31   48\n",
            "0.7999999999999999   140\n",
            "31   49\n",
            "0.7999999999999999   141\n",
            "31   49\n",
            "0.7999999999999999   142\n",
            "31   49\n",
            "0.7999999999999999   143\n",
            "32   49\n",
            "0.7999999999999999   144\n",
            "32   49\n",
            "0.7999999999999999   145\n",
            "33   50\n",
            "0.7999999999999999   146\n",
            "33   50\n",
            "0.7999999999999999   147\n",
            "34   51\n",
            "0.7999999999999999   148\n",
            "34   51\n",
            "0.7999999999999999   149\n",
            "34   51\n",
            "0.7999999999999999   150\n",
            "34   51\n",
            "0.7999999999999999   151\n",
            "35   52\n",
            "0.7999999999999999   152\n",
            "35   52\n",
            "0.7999999999999999   153\n",
            "35   52\n",
            "0.7999999999999999   154\n",
            "35   52\n",
            "0.7999999999999999   155\n",
            "35   52\n",
            "0.7999999999999999   156\n",
            "35   52\n",
            "0.7999999999999999   157\n",
            "35   52\n",
            "0.7999999999999999   158\n",
            "35   53\n",
            "0.7999999999999999   159\n",
            "35   53\n",
            "0.7999999999999999   160\n",
            "35   53\n",
            "0.7999999999999999   161\n",
            "35   53\n",
            "0.7999999999999999   162\n",
            "36   54\n",
            "0.7999999999999999   163\n",
            "36   54\n",
            "0.7999999999999999   164\n",
            "37   55\n",
            "0.7999999999999999   165\n",
            "38   56\n",
            "0.7999999999999999   166\n",
            "39   57\n",
            "0.7999999999999999   167\n",
            "39   57\n",
            "0.7999999999999999   168\n",
            "39   57\n",
            "0.7999999999999999   169\n",
            "40   58\n",
            "0.7999999999999999   170\n",
            "40   58\n",
            "0.7999999999999999   171\n",
            "40   58\n",
            "0.7999999999999999   172\n",
            "40   59\n",
            "0.7999999999999999   173\n",
            "40   60\n",
            "0.7999999999999999   174\n",
            "40   60\n",
            "0.7999999999999999   175\n",
            "40   60\n",
            "0.7999999999999999   176\n",
            "40   60\n",
            "0.7999999999999999   177\n",
            "40   60\n",
            "0.7999999999999999   178\n",
            "40   60\n",
            "0.7999999999999999   179\n",
            "40   60\n",
            "0.7999999999999999   180\n",
            "40   60\n",
            "0.7999999999999999   181\n",
            "40   60\n",
            "0.7999999999999999   182\n",
            "40   60\n",
            "0.7999999999999999   183\n",
            "40   60\n",
            "0.7999999999999999   184\n",
            "41   61\n",
            "0.7999999999999999   185\n",
            "41   61\n",
            "0.7999999999999999   186\n",
            "41   61\n",
            "0.7999999999999999   187\n",
            "41   61\n",
            "0.7999999999999999   188\n",
            "42   62\n",
            "0.7999999999999999   189\n",
            "42   62\n",
            "0.7999999999999999   190\n",
            "42   62\n",
            "0.7999999999999999   191\n",
            "42   62\n",
            "0.7999999999999999   192\n",
            "42   62\n",
            "0.7999999999999999   193\n",
            "42   62\n",
            "0.7999999999999999   194\n",
            "42   62\n",
            "0.7999999999999999   195\n",
            "42   62\n",
            "0.7999999999999999   196\n",
            "42   63\n",
            "0.7999999999999999   197\n",
            "42   64\n",
            "0.7999999999999999   198\n",
            "42   64\n",
            "0.7999999999999999   199\n",
            "43   65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for key in result.keys():\n",
        "  print(key,' ',result[key])"
      ],
      "metadata": {
        "id": "1JP3AMDHUXWK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23c4a5b1-e9f4-4b17-f5d8-ed0ce6eba697"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.1   (26, 26)\n",
            "0.2   (28, 28)\n",
            "0.30000000000000004   (35, 35)\n",
            "0.4   (40, 43)\n",
            "0.5   (45, 43)\n",
            "0.6   (50, 43)\n",
            "0.7   (56, 43)\n",
            "0.7999999999999999   (65, 43)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "PUor0wovVRZQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}