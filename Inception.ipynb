{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Inception.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kirath2205/Final-Year-Project/blob/main/Inception.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "wT9vgvoPA5LN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def get_random_eraser(p=0.5, s_l=0.02, s_h=0.4, r_1=0.3, r_2=1/0.3, v_l=0, v_h=255, pixel_level=False):\n",
        "    def eraser(input_img):\n",
        "        img_h, img_w, img_c = input_img.shape\n",
        "        p_1 = np.random.rand()\n",
        "\n",
        "        if p_1 > p:\n",
        "            return input_img\n",
        "\n",
        "        while True:\n",
        "            s = np.random.uniform(s_l, s_h) * img_h * img_w\n",
        "            r = np.random.uniform(r_1, r_2)\n",
        "            w = int(np.sqrt(s / r))\n",
        "            h = int(np.sqrt(s * r))\n",
        "            left = np.random.randint(0, img_w)\n",
        "            top = np.random.randint(0, img_h)\n",
        "\n",
        "            if left + w <= img_w and top + h <= img_h:\n",
        "                break\n",
        "\n",
        "        if pixel_level:\n",
        "            c = np.random.uniform(v_l, v_h, (h, w, img_c))\n",
        "        else:\n",
        "            c = np.random.uniform(v_l, v_h)\n",
        "\n",
        "        input_img[top:top + h, left:left + w, :] = c\n",
        "\n",
        "        return input_img\n",
        "\n",
        "    return eraser"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Keras-applications"
      ],
      "metadata": {
        "id": "Yzs3HWQuBj9r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3c72833-664e-46f9-b0ef-051bbf36989a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: Keras-applications in /usr/local/lib/python3.6/dist-packages (1.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras-applications) (2.10.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from Keras-applications) (1.19.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->Keras-applications) (1.15.0)\n",
            "\u001b[33mWARNING: You are using pip version 20.2.2; however, version 21.3.1 is available.\n",
            "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Input, UpSampling2D, Flatten, BatchNormalization, Dense, Dropout, GlobalAveragePooling2D , UpSampling3D\n",
        "from tensorflow.keras import optimizers\n",
        "from keras.datasets import cifar100,cifar10,fashion_mnist\n",
        "import tensorflow as tf\n",
        "from keras.utils import np_utils\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from skimage.transform import resize\n",
        "from keras import applications\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input , decode_predictions\n",
        "from keras.preprocessing.image import ImageDataGenerator"
      ],
      "metadata": {
        "id": "CtDTfJMaBKTx"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def select_dataset(index=1): #1 for cifar10 , 2 for cifar100 , 3 for fashion mnist\n",
        "  if(index == 1):\n",
        "    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "    channel = 3\n",
        "    num_classes = 10\n",
        "  if(index == 2):\n",
        "    (x_train, y_train), (x_test, y_test) = cifar100.load_data()\n",
        "    num_classes = 100\n",
        "    channel = 3\n",
        "  if(index == 3):\n",
        "    (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "    x_train = x_train.reshape((60000, 28, 28, 1))\n",
        "    x_test =  x_test.reshape((10000, 28, 28, 1))\n",
        "    num_classes = 10\n",
        "    y_train = np_utils.to_categorical(y_train, num_classes)\n",
        "    y_test = np_utils.to_categorical(y_test, num_classes)\n",
        "    channel = 1\n",
        "    return (x_train , y_train , x_test , y_test , num_classes , channel)\n",
        "\n",
        "  #Pre-process the data\n",
        "  x_train = preprocess_input(x_train)\n",
        "  x_test = preprocess_input(x_test)\n",
        "\n",
        "  datagen = ImageDataGenerator(preprocessing_function=get_random_eraser(v_l=0, v_h=1, pixel_level=True))\n",
        "  datagen.fit(x_train)\n",
        "  y_train = np_utils.to_categorical(y_train, num_classes)\n",
        "  y_test = np_utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "  return (x_train , y_train , x_test , y_test , num_classes , channel , datagen)"
      ],
      "metadata": {
        "id": "al1qQJxOBM1C"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Inception(num_classes , channel=3):\n",
        "  if(channel == 3):\n",
        "    inception_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "  else:\n",
        "    inception_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(112, 112, 3))\n",
        "\n",
        "  for layer in inception_model.layers:\n",
        "    if isinstance(layer, BatchNormalization):\n",
        "      layer.trainable = True\n",
        "    else:\n",
        "      layer.trainable = False\n",
        "  model = Sequential()\n",
        "  if(channel==1):\n",
        "    model.add(UpSampling3D((4,4,3)))\n",
        "  else:\n",
        "    model.add(UpSampling2D((7,7)))\n",
        "  model.add(inception_model)\n",
        "  model.add(GlobalAveragePooling2D())\n",
        "  '''cifar-10\n",
        "  model.add(Dense(256, activation='relu'))\n",
        "  model.add(Dropout(.25))\n",
        "  #model.add(BatchNormalization())\n",
        "  model.add(Dense(128, activation='relu'))\n",
        "  model.add(Dropout(.25))\n",
        "  #model.add(BatchNormalization())\n",
        "  model.add(Dense(32, activation='relu'))\n",
        "  model.add(Dropout(.25))\n",
        "  #model.add(BatchNormalization())\n",
        "  '''\n",
        "  '''cifar-100'''\n",
        "  model.add(Dense(1024, activation='relu'))\n",
        "  model.add(Dropout(.3))\n",
        "  #model.add(BatchNormalization())\n",
        "  model.add(Dense(512, activation='relu'))\n",
        "  #model.add(Dropout(.3))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dense(256, activation='relu'))\n",
        "  #model.add(Dropout(.25))\n",
        "  model.add(BatchNormalization())\n",
        "  ''''''\n",
        "  model.add(Dense(num_classes, activation='softmax'))\n",
        "  model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "  return model"
      ],
      "metadata": {
        "id": "8zq0WGLtB0p_"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = 1\n",
        "x_train , y_train , x_test , y_test , num_classes ,channel , datagen = select_dataset(index)\n",
        "model = Inception(num_classes,channel)\n",
        "model_name = 'desktop/Trained_models/inception_cifar10'\n",
        "model_path = 'desktop/Trained_models/inception_cifar10.h5'"
      ],
      "metadata": {
        "id": "Sb309y2jmmYj"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = 2\n",
        "x_train , y_train , x_test , y_test , num_classes ,channel , datagen = select_dataset(index)\n",
        "model = Inception(num_classes,channel)\n",
        "model_name = 'desktop/Trained_models/inception_cifar100'\n",
        "model_path = 'desktop/Trained_models/inception_cifar100.h5'"
      ],
      "metadata": {
        "id": "qMm7-78JnVa9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = 3\n",
        "x_train , y_train , x_test , y_test , num_classes ,channel  = select_dataset(index)\n",
        "model = Inception(num_classes,channel)\n",
        "model_name = 'inception_mnist'\n",
        "model_path = '/content/inception_mnist.h5'"
      ],
      "metadata": {
        "id": "4jlkd0p1B4hX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "epochs=200\n",
        "callbacks = [ \n",
        "    tf.keras.callbacks.ModelCheckpoint(model_name, monitor='val_accuracy', save_best_only=True , verbose=1),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau( factor = 0.9, patience = 5, min_lr = 0.0000001), # cifar-10 - factor-0.8,patience = 3\n",
        "    #tf.keras.callbacks.EarlyStopping(monitor = 'val_loss' , patience = 15) # cifar-10 patience - 10\n",
        "  ]\n",
        "if(channel == 3):\n",
        "  history = model.fit_generator(datagen.flow(x_train, y_train,\n",
        "                                  batch_size=batch_size),\n",
        "                                  steps_per_epoch=x_train.shape[0] // batch_size,\n",
        "                                  epochs=epochs,\n",
        "                                  validation_data=(x_test, y_test),\n",
        "                                  callbacks = callbacks)\n",
        "\n",
        "  model.save(model_path)\n",
        "else:\n",
        "  history = model.fit(x_train , y_train , batch_size=batch_size ,steps_per_epoch=x_train.shape[0] // batch_size, epochs=epochs, validation_data=(x_test, y_test),callbacks=callbacks)\n",
        "  model.save(model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtmcDNu1B6ss",
        "outputId": "687e0071-b733-47c5-e47b-69063158898f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 3.0807 - accuracy: 0.2570\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.47560, saving model to desktop/Trained_models/inception_cifar100\n",
            "INFO:tensorflow:Assets written to: desktop/Trained_models/inception_cifar100/assets\n",
            "781/781 [==============================] - 145s 186ms/step - loss: 3.0807 - accuracy: 0.2570 - val_loss: 1.9065 - val_accuracy: 0.4756 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 2.0483 - accuracy: 0.4504\n",
            "Epoch 00002: val_accuracy improved from 0.47560 to 0.58200, saving model to desktop/Trained_models/inception_cifar100\n",
            "INFO:tensorflow:Assets written to: desktop/Trained_models/inception_cifar100/assets\n",
            "781/781 [==============================] - 147s 188ms/step - loss: 2.0483 - accuracy: 0.4504 - val_loss: 1.5035 - val_accuracy: 0.5820 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 1.7292 - accuracy: 0.5217\n",
            "Epoch 00003: val_accuracy improved from 0.58200 to 0.62300, saving model to desktop/Trained_models/inception_cifar100\n",
            "INFO:tensorflow:Assets written to: desktop/Trained_models/inception_cifar100/assets\n",
            "781/781 [==============================] - 150s 192ms/step - loss: 1.7292 - accuracy: 0.5217 - val_loss: 1.3054 - val_accuracy: 0.6230 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 1.5608 - accuracy: 0.5619\n",
            "Epoch 00004: val_accuracy improved from 0.62300 to 0.65930, saving model to desktop/Trained_models/inception_cifar100\n",
            "INFO:tensorflow:Assets written to: desktop/Trained_models/inception_cifar100/assets\n",
            "781/781 [==============================] - 148s 190ms/step - loss: 1.5608 - accuracy: 0.5619 - val_loss: 1.1885 - val_accuracy: 0.6593 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 1.4374 - accuracy: 0.5950\n",
            "Epoch 00005: val_accuracy improved from 0.65930 to 0.67530, saving model to desktop/Trained_models/inception_cifar100\n",
            "INFO:tensorflow:Assets written to: desktop/Trained_models/inception_cifar100/assets\n",
            "781/781 [==============================] - 148s 190ms/step - loss: 1.4374 - accuracy: 0.5950 - val_loss: 1.1301 - val_accuracy: 0.6753 - lr: 0.0010\n",
            "Epoch 6/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 1.3350 - accuracy: 0.6172\n",
            "Epoch 00006: val_accuracy improved from 0.67530 to 0.68190, saving model to desktop/Trained_models/inception_cifar100\n",
            "INFO:tensorflow:Assets written to: desktop/Trained_models/inception_cifar100/assets\n",
            "781/781 [==============================] - 144s 184ms/step - loss: 1.3350 - accuracy: 0.6172 - val_loss: 1.0852 - val_accuracy: 0.6819 - lr: 0.0010\n",
            "Epoch 7/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 1.2670 - accuracy: 0.6338\n",
            "Epoch 00007: val_accuracy improved from 0.68190 to 0.69180, saving model to desktop/Trained_models/inception_cifar100\n",
            "INFO:tensorflow:Assets written to: desktop/Trained_models/inception_cifar100/assets\n",
            "781/781 [==============================] - 153s 196ms/step - loss: 1.2670 - accuracy: 0.6338 - val_loss: 1.0513 - val_accuracy: 0.6918 - lr: 0.0010\n",
            "Epoch 8/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 1.1939 - accuracy: 0.6545\n",
            "Epoch 00008: val_accuracy improved from 0.69180 to 0.69790, saving model to desktop/Trained_models/inception_cifar100\n",
            "INFO:tensorflow:Assets written to: desktop/Trained_models/inception_cifar100/assets\n",
            "781/781 [==============================] - 144s 185ms/step - loss: 1.1939 - accuracy: 0.6545 - val_loss: 1.0253 - val_accuracy: 0.6979 - lr: 0.0010\n",
            "Epoch 9/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 1.1449 - accuracy: 0.6685\n",
            "Epoch 00009: val_accuracy improved from 0.69790 to 0.71490, saving model to desktop/Trained_models/inception_cifar100\n",
            "INFO:tensorflow:Assets written to: desktop/Trained_models/inception_cifar100/assets\n",
            "781/781 [==============================] - 153s 196ms/step - loss: 1.1449 - accuracy: 0.6685 - val_loss: 0.9778 - val_accuracy: 0.7149 - lr: 0.0010\n",
            "Epoch 10/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 1.0943 - accuracy: 0.6802\n",
            "Epoch 00010: val_accuracy improved from 0.71490 to 0.72060, saving model to desktop/Trained_models/inception_cifar100\n",
            "INFO:tensorflow:Assets written to: desktop/Trained_models/inception_cifar100/assets\n",
            "781/781 [==============================] - 144s 185ms/step - loss: 1.0943 - accuracy: 0.6802 - val_loss: 0.9567 - val_accuracy: 0.7206 - lr: 0.0010\n",
            "Epoch 11/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 1.0629 - accuracy: 0.6909\n",
            "Epoch 00011: val_accuracy improved from 0.72060 to 0.72550, saving model to desktop/Trained_models/inception_cifar100\n",
            "INFO:tensorflow:Assets written to: desktop/Trained_models/inception_cifar100/assets\n",
            "781/781 [==============================] - 153s 195ms/step - loss: 1.0629 - accuracy: 0.6909 - val_loss: 0.9614 - val_accuracy: 0.7255 - lr: 0.0010\n",
            "Epoch 12/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 1.0206 - accuracy: 0.6998\n",
            "Epoch 00012: val_accuracy improved from 0.72550 to 0.73030, saving model to desktop/Trained_models/inception_cifar100\n",
            "INFO:tensorflow:Assets written to: desktop/Trained_models/inception_cifar100/assets\n",
            "781/781 [==============================] - 145s 185ms/step - loss: 1.0206 - accuracy: 0.6998 - val_loss: 0.9495 - val_accuracy: 0.7303 - lr: 0.0010\n",
            "Epoch 13/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.9806 - accuracy: 0.7117\n",
            "Epoch 00013: val_accuracy improved from 0.73030 to 0.73150, saving model to desktop/Trained_models/inception_cifar100\n",
            "INFO:tensorflow:Assets written to: desktop/Trained_models/inception_cifar100/assets\n",
            "781/781 [==============================] - 153s 196ms/step - loss: 0.9806 - accuracy: 0.7117 - val_loss: 0.9308 - val_accuracy: 0.7315 - lr: 0.0010\n",
            "Epoch 14/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.9498 - accuracy: 0.7183\n",
            "Epoch 00014: val_accuracy did not improve from 0.73150\n",
            "781/781 [==============================] - 117s 149ms/step - loss: 0.9498 - accuracy: 0.7183 - val_loss: 0.9561 - val_accuracy: 0.7253 - lr: 0.0010\n",
            "Epoch 15/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.9330 - accuracy: 0.7239\n",
            "Epoch 00015: val_accuracy improved from 0.73150 to 0.73440, saving model to desktop/Trained_models/inception_cifar100\n",
            "INFO:tensorflow:Assets written to: desktop/Trained_models/inception_cifar100/assets\n",
            "781/781 [==============================] - 148s 189ms/step - loss: 0.9330 - accuracy: 0.7239 - val_loss: 0.9142 - val_accuracy: 0.7344 - lr: 0.0010\n",
            "Epoch 16/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.8988 - accuracy: 0.7336\n",
            "Epoch 00016: val_accuracy improved from 0.73440 to 0.74000, saving model to desktop/Trained_models/inception_cifar100\n",
            "INFO:tensorflow:Assets written to: desktop/Trained_models/inception_cifar100/assets\n",
            "781/781 [==============================] - 154s 197ms/step - loss: 0.8988 - accuracy: 0.7336 - val_loss: 0.9081 - val_accuracy: 0.7400 - lr: 0.0010\n",
            "Epoch 17/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.8795 - accuracy: 0.7391\n",
            "Epoch 00017: val_accuracy improved from 0.74000 to 0.74660, saving model to desktop/Trained_models/inception_cifar100\n",
            "INFO:tensorflow:Assets written to: desktop/Trained_models/inception_cifar100/assets\n",
            "781/781 [==============================] - 145s 186ms/step - loss: 0.8795 - accuracy: 0.7391 - val_loss: 0.9026 - val_accuracy: 0.7466 - lr: 0.0010\n",
            "Epoch 18/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.8548 - accuracy: 0.7455\n",
            "Epoch 00018: val_accuracy improved from 0.74660 to 0.74770, saving model to desktop/Trained_models/inception_cifar100\n",
            "INFO:tensorflow:Assets written to: desktop/Trained_models/inception_cifar100/assets\n",
            "781/781 [==============================] - 152s 194ms/step - loss: 0.8548 - accuracy: 0.7455 - val_loss: 0.8902 - val_accuracy: 0.7477 - lr: 0.0010\n",
            "Epoch 19/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.8398 - accuracy: 0.7503\n",
            "Epoch 00019: val_accuracy improved from 0.74770 to 0.74880, saving model to desktop/Trained_models/inception_cifar100\n",
            "INFO:tensorflow:Assets written to: desktop/Trained_models/inception_cifar100/assets\n",
            "781/781 [==============================] - 152s 195ms/step - loss: 0.8398 - accuracy: 0.7503 - val_loss: 0.8892 - val_accuracy: 0.7488 - lr: 0.0010\n",
            "Epoch 20/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.8158 - accuracy: 0.7574\n",
            "Epoch 00020: val_accuracy did not improve from 0.74880\n",
            "781/781 [==============================] - 116s 149ms/step - loss: 0.8158 - accuracy: 0.7574 - val_loss: 0.9153 - val_accuracy: 0.7450 - lr: 0.0010\n",
            "Epoch 21/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.7942 - accuracy: 0.7636\n",
            "Epoch 00021: val_accuracy did not improve from 0.74880\n",
            "781/781 [==============================] - 119s 152ms/step - loss: 0.7942 - accuracy: 0.7636 - val_loss: 0.9096 - val_accuracy: 0.7415 - lr: 0.0010\n",
            "Epoch 22/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.7799 - accuracy: 0.7641\n",
            "Epoch 00022: val_accuracy did not improve from 0.74880\n",
            "781/781 [==============================] - 119s 152ms/step - loss: 0.7799 - accuracy: 0.7641 - val_loss: 0.9175 - val_accuracy: 0.7459 - lr: 0.0010\n",
            "Epoch 23/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.7585 - accuracy: 0.7715\n",
            "Epoch 00023: val_accuracy improved from 0.74880 to 0.75100, saving model to desktop/Trained_models/inception_cifar100\n",
            "INFO:tensorflow:Assets written to: desktop/Trained_models/inception_cifar100/assets\n",
            "781/781 [==============================] - 147s 189ms/step - loss: 0.7585 - accuracy: 0.7715 - val_loss: 0.8921 - val_accuracy: 0.7510 - lr: 0.0010\n",
            "Epoch 24/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.7496 - accuracy: 0.7741\n",
            "Epoch 00024: val_accuracy did not improve from 0.75100\n",
            "781/781 [==============================] - 117s 150ms/step - loss: 0.7496 - accuracy: 0.7741 - val_loss: 0.9046 - val_accuracy: 0.7500 - lr: 0.0010\n",
            "Epoch 25/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.7152 - accuracy: 0.7865\n",
            "Epoch 00025: val_accuracy did not improve from 0.75100\n",
            "781/781 [==============================] - 119s 153ms/step - loss: 0.7152 - accuracy: 0.7865 - val_loss: 0.9171 - val_accuracy: 0.7491 - lr: 9.0000e-04\n",
            "Epoch 26/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.7037 - accuracy: 0.7869\n",
            "Epoch 00026: val_accuracy improved from 0.75100 to 0.75920, saving model to desktop/Trained_models/inception_cifar100\n",
            "INFO:tensorflow:Assets written to: desktop/Trained_models/inception_cifar100/assets\n",
            "781/781 [==============================] - 154s 197ms/step - loss: 0.7037 - accuracy: 0.7869 - val_loss: 0.8842 - val_accuracy: 0.7592 - lr: 9.0000e-04\n",
            "Epoch 27/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.6904 - accuracy: 0.7930\n",
            "Epoch 00027: val_accuracy did not improve from 0.75920\n",
            "781/781 [==============================] - 117s 150ms/step - loss: 0.6904 - accuracy: 0.7930 - val_loss: 0.8863 - val_accuracy: 0.7560 - lr: 9.0000e-04\n",
            "Epoch 28/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.6751 - accuracy: 0.7972\n",
            "Epoch 00028: val_accuracy did not improve from 0.75920\n",
            "781/781 [==============================] - 119s 152ms/step - loss: 0.6751 - accuracy: 0.7972 - val_loss: 0.9054 - val_accuracy: 0.7574 - lr: 9.0000e-04\n",
            "Epoch 29/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.6555 - accuracy: 0.8014\n",
            "Epoch 00029: val_accuracy did not improve from 0.75920\n",
            "781/781 [==============================] - 119s 153ms/step - loss: 0.6555 - accuracy: 0.8014 - val_loss: 0.8978 - val_accuracy: 0.7578 - lr: 9.0000e-04\n",
            "Epoch 30/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.6472 - accuracy: 0.8035\n",
            "Epoch 00030: val_accuracy did not improve from 0.75920\n",
            "781/781 [==============================] - 119s 152ms/step - loss: 0.6472 - accuracy: 0.8035 - val_loss: 0.9029 - val_accuracy: 0.7558 - lr: 9.0000e-04\n",
            "Epoch 31/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.6380 - accuracy: 0.8082\n",
            "Epoch 00031: val_accuracy did not improve from 0.75920\n",
            "781/781 [==============================] - 118s 152ms/step - loss: 0.6380 - accuracy: 0.8082 - val_loss: 0.9220 - val_accuracy: 0.7515 - lr: 9.0000e-04\n",
            "Epoch 32/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.6178 - accuracy: 0.8146\n",
            "Epoch 00032: val_accuracy did not improve from 0.75920\n",
            "781/781 [==============================] - 118s 152ms/step - loss: 0.6178 - accuracy: 0.8146 - val_loss: 0.9014 - val_accuracy: 0.7558 - lr: 8.1000e-04\n",
            "Epoch 33/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.6001 - accuracy: 0.8190\n",
            "Epoch 00033: val_accuracy improved from 0.75920 to 0.76010, saving model to desktop/Trained_models/inception_cifar100\n",
            "INFO:tensorflow:Assets written to: desktop/Trained_models/inception_cifar100/assets\n",
            "781/781 [==============================] - 154s 197ms/step - loss: 0.6001 - accuracy: 0.8190 - val_loss: 0.9147 - val_accuracy: 0.7601 - lr: 8.1000e-04\n",
            "Epoch 34/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.5907 - accuracy: 0.8208\n",
            "Epoch 00034: val_accuracy did not improve from 0.76010\n",
            "781/781 [==============================] - 117s 150ms/step - loss: 0.5907 - accuracy: 0.8208 - val_loss: 0.9078 - val_accuracy: 0.7600 - lr: 8.1000e-04\n",
            "Epoch 35/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.5852 - accuracy: 0.8213\n",
            "Epoch 00035: val_accuracy improved from 0.76010 to 0.76040, saving model to desktop/Trained_models/inception_cifar100\n",
            "INFO:tensorflow:Assets written to: desktop/Trained_models/inception_cifar100/assets\n",
            "781/781 [==============================] - 147s 189ms/step - loss: 0.5852 - accuracy: 0.8213 - val_loss: 0.9154 - val_accuracy: 0.7604 - lr: 8.1000e-04\n",
            "Epoch 36/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.5768 - accuracy: 0.8265\n",
            "Epoch 00036: val_accuracy did not improve from 0.76040\n",
            "781/781 [==============================] - 117s 150ms/step - loss: 0.5768 - accuracy: 0.8265 - val_loss: 0.9114 - val_accuracy: 0.7563 - lr: 8.1000e-04\n",
            "Epoch 37/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.5593 - accuracy: 0.8314\n",
            "Epoch 00037: val_accuracy improved from 0.76040 to 0.76210, saving model to desktop/Trained_models/inception_cifar100\n",
            "INFO:tensorflow:Assets written to: desktop/Trained_models/inception_cifar100/assets\n",
            "781/781 [==============================] - 154s 198ms/step - loss: 0.5593 - accuracy: 0.8314 - val_loss: 0.9164 - val_accuracy: 0.7621 - lr: 7.2900e-04\n",
            "Epoch 38/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.5538 - accuracy: 0.8326\n",
            "Epoch 00038: val_accuracy did not improve from 0.76210\n",
            "781/781 [==============================] - 117s 150ms/step - loss: 0.5538 - accuracy: 0.8326 - val_loss: 0.9208 - val_accuracy: 0.7585 - lr: 7.2900e-04\n",
            "Epoch 39/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.5496 - accuracy: 0.8345\n",
            "Epoch 00039: val_accuracy improved from 0.76210 to 0.76280, saving model to desktop/Trained_models/inception_cifar100\n",
            "INFO:tensorflow:Assets written to: desktop/Trained_models/inception_cifar100/assets\n",
            "781/781 [==============================] - 153s 196ms/step - loss: 0.5496 - accuracy: 0.8345 - val_loss: 0.9114 - val_accuracy: 0.7628 - lr: 7.2900e-04\n",
            "Epoch 40/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.5306 - accuracy: 0.8401\n",
            "Epoch 00040: val_accuracy did not improve from 0.76280\n",
            "781/781 [==============================] - 117s 150ms/step - loss: 0.5306 - accuracy: 0.8401 - val_loss: 0.9204 - val_accuracy: 0.7598 - lr: 7.2900e-04\n",
            "Epoch 41/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.5280 - accuracy: 0.8404\n",
            "Epoch 00041: val_accuracy did not improve from 0.76280\n",
            "781/781 [==============================] - 119s 153ms/step - loss: 0.5280 - accuracy: 0.8404 - val_loss: 0.9360 - val_accuracy: 0.7587 - lr: 7.2900e-04\n",
            "Epoch 42/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.5108 - accuracy: 0.8456\n",
            "Epoch 00042: val_accuracy did not improve from 0.76280\n",
            "781/781 [==============================] - 119s 153ms/step - loss: 0.5108 - accuracy: 0.8456 - val_loss: 0.9368 - val_accuracy: 0.7562 - lr: 6.5610e-04\n",
            "Epoch 43/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.5123 - accuracy: 0.8462\n",
            "Epoch 00043: val_accuracy did not improve from 0.76280\n",
            "781/781 [==============================] - 119s 152ms/step - loss: 0.5123 - accuracy: 0.8462 - val_loss: 0.9336 - val_accuracy: 0.7591 - lr: 6.5610e-04\n",
            "Epoch 44/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.5040 - accuracy: 0.8487\n",
            "Epoch 00044: val_accuracy did not improve from 0.76280\n",
            "781/781 [==============================] - 118s 151ms/step - loss: 0.5040 - accuracy: 0.8487 - val_loss: 0.9238 - val_accuracy: 0.7602 - lr: 6.5610e-04\n",
            "Epoch 45/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.5015 - accuracy: 0.8508\n",
            "Epoch 00045: val_accuracy did not improve from 0.76280\n",
            "781/781 [==============================] - 118s 151ms/step - loss: 0.5015 - accuracy: 0.8508 - val_loss: 0.9150 - val_accuracy: 0.7628 - lr: 6.5610e-04\n",
            "Epoch 46/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.4979 - accuracy: 0.8516\n",
            "Epoch 00046: val_accuracy did not improve from 0.76280\n",
            "781/781 [==============================] - 118s 151ms/step - loss: 0.4979 - accuracy: 0.8516 - val_loss: 0.9269 - val_accuracy: 0.7617 - lr: 6.5610e-04\n",
            "Epoch 47/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.4850 - accuracy: 0.8544\n",
            "Epoch 00047: val_accuracy improved from 0.76280 to 0.76330, saving model to desktop/Trained_models/inception_cifar100\n",
            "INFO:tensorflow:Assets written to: desktop/Trained_models/inception_cifar100/assets\n",
            "781/781 [==============================] - 152s 194ms/step - loss: 0.4850 - accuracy: 0.8544 - val_loss: 0.9291 - val_accuracy: 0.7633 - lr: 5.9049e-04\n",
            "Epoch 48/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.4751 - accuracy: 0.8589\n",
            "Epoch 00048: val_accuracy did not improve from 0.76330\n",
            "781/781 [==============================] - 118s 151ms/step - loss: 0.4751 - accuracy: 0.8589 - val_loss: 0.9286 - val_accuracy: 0.7631 - lr: 5.9049e-04\n",
            "Epoch 49/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.4698 - accuracy: 0.8584\n",
            "Epoch 00049: val_accuracy improved from 0.76330 to 0.76440, saving model to desktop/Trained_models/inception_cifar100\n",
            "INFO:tensorflow:Assets written to: desktop/Trained_models/inception_cifar100/assets\n",
            "781/781 [==============================] - 155s 199ms/step - loss: 0.4698 - accuracy: 0.8584 - val_loss: 0.9217 - val_accuracy: 0.7644 - lr: 5.9049e-04\n",
            "Epoch 50/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.4617 - accuracy: 0.8620\n",
            "Epoch 00050: val_accuracy improved from 0.76440 to 0.76680, saving model to desktop/Trained_models/inception_cifar100\n",
            "INFO:tensorflow:Assets written to: desktop/Trained_models/inception_cifar100/assets\n",
            "781/781 [==============================] - 146s 187ms/step - loss: 0.4617 - accuracy: 0.8620 - val_loss: 0.9333 - val_accuracy: 0.7668 - lr: 5.9049e-04\n",
            "Epoch 51/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.4668 - accuracy: 0.8579\n",
            "Epoch 00051: val_accuracy did not improve from 0.76680\n",
            "781/781 [==============================] - 117s 150ms/step - loss: 0.4668 - accuracy: 0.8579 - val_loss: 0.9443 - val_accuracy: 0.7659 - lr: 5.9049e-04\n",
            "Epoch 52/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.4428 - accuracy: 0.8654\n",
            "Epoch 00052: val_accuracy improved from 0.76680 to 0.76760, saving model to desktop/Trained_models/inception_cifar100\n",
            "INFO:tensorflow:Assets written to: desktop/Trained_models/inception_cifar100/assets\n",
            "781/781 [==============================] - 154s 198ms/step - loss: 0.4428 - accuracy: 0.8654 - val_loss: 0.9483 - val_accuracy: 0.7676 - lr: 5.3144e-04\n",
            "Epoch 53/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.4387 - accuracy: 0.8695\n",
            "Epoch 00053: val_accuracy improved from 0.76760 to 0.76850, saving model to desktop/Trained_models/inception_cifar100\n",
            "INFO:tensorflow:Assets written to: desktop/Trained_models/inception_cifar100/assets\n",
            "781/781 [==============================] - 151s 194ms/step - loss: 0.4387 - accuracy: 0.8695 - val_loss: 0.9396 - val_accuracy: 0.7685 - lr: 5.3144e-04\n",
            "Epoch 54/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.4321 - accuracy: 0.8708\n",
            "Epoch 00054: val_accuracy did not improve from 0.76850\n",
            "781/781 [==============================] - 117s 150ms/step - loss: 0.4321 - accuracy: 0.8708 - val_loss: 0.9398 - val_accuracy: 0.7665 - lr: 5.3144e-04\n",
            "Epoch 55/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.4445 - accuracy: 0.8673\n",
            "Epoch 00055: val_accuracy did not improve from 0.76850\n",
            "781/781 [==============================] - 119s 152ms/step - loss: 0.4445 - accuracy: 0.8673 - val_loss: 0.9479 - val_accuracy: 0.7676 - lr: 5.3144e-04\n",
            "Epoch 56/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.4372 - accuracy: 0.8692\n",
            "Epoch 00056: val_accuracy did not improve from 0.76850\n",
            "781/781 [==============================] - 119s 152ms/step - loss: 0.4372 - accuracy: 0.8692 - val_loss: 0.9468 - val_accuracy: 0.7666 - lr: 5.3144e-04\n",
            "Epoch 57/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.4297 - accuracy: 0.8715\n",
            "Epoch 00057: val_accuracy did not improve from 0.76850\n",
            "781/781 [==============================] - 119s 152ms/step - loss: 0.4297 - accuracy: 0.8715 - val_loss: 0.9553 - val_accuracy: 0.7655 - lr: 4.7830e-04\n",
            "Epoch 58/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.4222 - accuracy: 0.8740\n",
            "Epoch 00058: val_accuracy did not improve from 0.76850\n",
            "781/781 [==============================] - 118s 151ms/step - loss: 0.4222 - accuracy: 0.8740 - val_loss: 0.9497 - val_accuracy: 0.7661 - lr: 4.7830e-04\n",
            "Epoch 59/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.4167 - accuracy: 0.8756\n",
            "Epoch 00059: val_accuracy did not improve from 0.76850\n",
            "781/781 [==============================] - 118s 151ms/step - loss: 0.4167 - accuracy: 0.8756 - val_loss: 0.9461 - val_accuracy: 0.7655 - lr: 4.7830e-04\n",
            "Epoch 60/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.4155 - accuracy: 0.8775\n",
            "Epoch 00060: val_accuracy did not improve from 0.76850\n",
            "781/781 [==============================] - 118s 151ms/step - loss: 0.4155 - accuracy: 0.8775 - val_loss: 0.9551 - val_accuracy: 0.7668 - lr: 4.7830e-04\n",
            "Epoch 61/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.4129 - accuracy: 0.8769\n",
            "Epoch 00061: val_accuracy did not improve from 0.76850\n",
            "781/781 [==============================] - 118s 151ms/step - loss: 0.4129 - accuracy: 0.8769 - val_loss: 0.9639 - val_accuracy: 0.7637 - lr: 4.7830e-04\n",
            "Epoch 62/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.4028 - accuracy: 0.8824\n",
            "Epoch 00062: val_accuracy improved from 0.76850 to 0.76910, saving model to desktop/Trained_models/inception_cifar100\n",
            "INFO:tensorflow:Assets written to: desktop/Trained_models/inception_cifar100/assets\n",
            "781/781 [==============================] - 152s 195ms/step - loss: 0.4028 - accuracy: 0.8824 - val_loss: 0.9515 - val_accuracy: 0.7691 - lr: 4.3047e-04\n",
            "Epoch 63/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.4029 - accuracy: 0.8805\n",
            "Epoch 00063: val_accuracy did not improve from 0.76910\n",
            "781/781 [==============================] - 123s 157ms/step - loss: 0.4029 - accuracy: 0.8805 - val_loss: 0.9695 - val_accuracy: 0.7664 - lr: 4.3047e-04\n",
            "Epoch 64/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3929 - accuracy: 0.8835\n",
            "Epoch 00064: val_accuracy did not improve from 0.76910\n",
            "781/781 [==============================] - 119s 153ms/step - loss: 0.3929 - accuracy: 0.8835 - val_loss: 0.9556 - val_accuracy: 0.7670 - lr: 4.3047e-04\n",
            "Epoch 65/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3972 - accuracy: 0.8838\n",
            "Epoch 00065: val_accuracy did not improve from 0.76910\n",
            "781/781 [==============================] - 119s 152ms/step - loss: 0.3972 - accuracy: 0.8838 - val_loss: 0.9556 - val_accuracy: 0.7679 - lr: 4.3047e-04\n",
            "Epoch 66/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3929 - accuracy: 0.8834\n",
            "Epoch 00066: val_accuracy did not improve from 0.76910\n",
            "781/781 [==============================] - 119s 152ms/step - loss: 0.3929 - accuracy: 0.8834 - val_loss: 0.9667 - val_accuracy: 0.7686 - lr: 4.3047e-04\n",
            "Epoch 67/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3841 - accuracy: 0.8871\n",
            "Epoch 00067: val_accuracy did not improve from 0.76910\n",
            "781/781 [==============================] - 118s 152ms/step - loss: 0.3841 - accuracy: 0.8871 - val_loss: 0.9678 - val_accuracy: 0.7662 - lr: 3.8742e-04\n",
            "Epoch 68/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3749 - accuracy: 0.8888\n",
            "Epoch 00068: val_accuracy did not improve from 0.76910\n",
            "781/781 [==============================] - 125s 161ms/step - loss: 0.3749 - accuracy: 0.8888 - val_loss: 0.9621 - val_accuracy: 0.7689 - lr: 3.8742e-04\n",
            "Epoch 69/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3742 - accuracy: 0.8893\n",
            "Epoch 00069: val_accuracy did not improve from 0.76910\n",
            "781/781 [==============================] - 118s 151ms/step - loss: 0.3742 - accuracy: 0.8893 - val_loss: 0.9599 - val_accuracy: 0.7686 - lr: 3.8742e-04\n",
            "Epoch 70/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3774 - accuracy: 0.8893\n",
            "Epoch 00070: val_accuracy did not improve from 0.76910\n",
            "781/781 [==============================] - 118s 151ms/step - loss: 0.3774 - accuracy: 0.8893 - val_loss: 0.9570 - val_accuracy: 0.7664 - lr: 3.8742e-04\n",
            "Epoch 71/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3715 - accuracy: 0.8894\n",
            "Epoch 00071: val_accuracy did not improve from 0.76910\n",
            "781/781 [==============================] - 118s 151ms/step - loss: 0.3715 - accuracy: 0.8894 - val_loss: 0.9671 - val_accuracy: 0.7675 - lr: 3.8742e-04\n",
            "Epoch 72/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3665 - accuracy: 0.8926\n",
            "Epoch 00072: val_accuracy improved from 0.76910 to 0.77300, saving model to desktop/Trained_models/inception_cifar100\n",
            "INFO:tensorflow:Assets written to: desktop/Trained_models/inception_cifar100/assets\n",
            "781/781 [==============================] - 153s 195ms/step - loss: 0.3665 - accuracy: 0.8926 - val_loss: 0.9627 - val_accuracy: 0.7730 - lr: 3.4868e-04\n",
            "Epoch 73/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3668 - accuracy: 0.8907\n",
            "Epoch 00073: val_accuracy did not improve from 0.77300\n",
            "781/781 [==============================] - 117s 149ms/step - loss: 0.3668 - accuracy: 0.8907 - val_loss: 0.9674 - val_accuracy: 0.7721 - lr: 3.4868e-04\n",
            "Epoch 74/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3632 - accuracy: 0.8925\n",
            "Epoch 00074: val_accuracy did not improve from 0.77300\n",
            "781/781 [==============================] - 125s 160ms/step - loss: 0.3632 - accuracy: 0.8925 - val_loss: 0.9653 - val_accuracy: 0.7727 - lr: 3.4868e-04\n",
            "Epoch 75/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3639 - accuracy: 0.8930\n",
            "Epoch 00075: val_accuracy did not improve from 0.77300\n",
            "781/781 [==============================] - 120s 154ms/step - loss: 0.3639 - accuracy: 0.8930 - val_loss: 0.9608 - val_accuracy: 0.7703 - lr: 3.4868e-04\n",
            "Epoch 76/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3654 - accuracy: 0.8925\n",
            "Epoch 00076: val_accuracy did not improve from 0.77300\n",
            "781/781 [==============================] - 119s 152ms/step - loss: 0.3654 - accuracy: 0.8925 - val_loss: 0.9702 - val_accuracy: 0.7678 - lr: 3.4868e-04\n",
            "Epoch 77/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3577 - accuracy: 0.8952\n",
            "Epoch 00077: val_accuracy did not improve from 0.77300\n",
            "781/781 [==============================] - 118s 151ms/step - loss: 0.3577 - accuracy: 0.8952 - val_loss: 0.9548 - val_accuracy: 0.7700 - lr: 3.1381e-04\n",
            "Epoch 78/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3546 - accuracy: 0.8939\n",
            "Epoch 00078: val_accuracy did not improve from 0.77300\n",
            "781/781 [==============================] - 118s 151ms/step - loss: 0.3546 - accuracy: 0.8939 - val_loss: 0.9667 - val_accuracy: 0.7712 - lr: 3.1381e-04\n",
            "Epoch 79/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3445 - accuracy: 0.8991\n",
            "Epoch 00079: val_accuracy did not improve from 0.77300\n",
            "781/781 [==============================] - 118s 151ms/step - loss: 0.3445 - accuracy: 0.8991 - val_loss: 0.9591 - val_accuracy: 0.7726 - lr: 3.1381e-04\n",
            "Epoch 80/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3563 - accuracy: 0.8964\n",
            "Epoch 00080: val_accuracy did not improve from 0.77300\n",
            "781/781 [==============================] - 118s 151ms/step - loss: 0.3563 - accuracy: 0.8964 - val_loss: 0.9671 - val_accuracy: 0.7724 - lr: 3.1381e-04\n",
            "Epoch 81/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3417 - accuracy: 0.8996\n",
            "Epoch 00081: val_accuracy did not improve from 0.77300\n",
            "781/781 [==============================] - 118s 151ms/step - loss: 0.3417 - accuracy: 0.8996 - val_loss: 0.9646 - val_accuracy: 0.7711 - lr: 3.1381e-04\n",
            "Epoch 82/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3453 - accuracy: 0.8987\n",
            "Epoch 00082: val_accuracy did not improve from 0.77300\n",
            "781/781 [==============================] - 118s 151ms/step - loss: 0.3453 - accuracy: 0.8987 - val_loss: 0.9692 - val_accuracy: 0.7725 - lr: 2.8243e-04\n",
            "Epoch 83/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3445 - accuracy: 0.8996\n",
            "Epoch 00083: val_accuracy improved from 0.77300 to 0.77340, saving model to desktop/Trained_models/inception_cifar100\n",
            "INFO:tensorflow:Assets written to: desktop/Trained_models/inception_cifar100/assets\n",
            "781/781 [==============================] - 152s 195ms/step - loss: 0.3445 - accuracy: 0.8996 - val_loss: 0.9621 - val_accuracy: 0.7734 - lr: 2.8243e-04\n",
            "Epoch 84/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3419 - accuracy: 0.8999\n",
            "Epoch 00084: val_accuracy improved from 0.77340 to 0.77410, saving model to desktop/Trained_models/inception_cifar100\n",
            "INFO:tensorflow:Assets written to: desktop/Trained_models/inception_cifar100/assets\n",
            "781/781 [==============================] - 152s 194ms/step - loss: 0.3419 - accuracy: 0.8999 - val_loss: 0.9555 - val_accuracy: 0.7741 - lr: 2.8243e-04\n",
            "Epoch 85/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3369 - accuracy: 0.9003\n",
            "Epoch 00085: val_accuracy did not improve from 0.77410\n",
            "781/781 [==============================] - 124s 159ms/step - loss: 0.3369 - accuracy: 0.9003 - val_loss: 0.9654 - val_accuracy: 0.7725 - lr: 2.8243e-04\n",
            "Epoch 86/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3386 - accuracy: 0.9005\n",
            "Epoch 00086: val_accuracy did not improve from 0.77410\n",
            "781/781 [==============================] - 129s 165ms/step - loss: 0.3386 - accuracy: 0.9005 - val_loss: 0.9774 - val_accuracy: 0.7715 - lr: 2.8243e-04\n",
            "Epoch 87/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3295 - accuracy: 0.9033\n",
            "Epoch 00087: val_accuracy did not improve from 0.77410\n",
            "781/781 [==============================] - 126s 161ms/step - loss: 0.3295 - accuracy: 0.9033 - val_loss: 0.9654 - val_accuracy: 0.7724 - lr: 2.5419e-04\n",
            "Epoch 88/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3349 - accuracy: 0.9007\n",
            "Epoch 00088: val_accuracy did not improve from 0.77410\n",
            "781/781 [==============================] - 119s 152ms/step - loss: 0.3349 - accuracy: 0.9007 - val_loss: 0.9690 - val_accuracy: 0.7728 - lr: 2.5419e-04\n",
            "Epoch 89/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3303 - accuracy: 0.9031\n",
            "Epoch 00089: val_accuracy did not improve from 0.77410\n",
            "781/781 [==============================] - 125s 160ms/step - loss: 0.3303 - accuracy: 0.9031 - val_loss: 0.9606 - val_accuracy: 0.7707 - lr: 2.5419e-04\n",
            "Epoch 90/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3256 - accuracy: 0.9050\n",
            "Epoch 00090: val_accuracy did not improve from 0.77410\n",
            "781/781 [==============================] - 123s 158ms/step - loss: 0.3256 - accuracy: 0.9050 - val_loss: 0.9793 - val_accuracy: 0.7705 - lr: 2.5419e-04\n",
            "Epoch 91/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3223 - accuracy: 0.9037\n",
            "Epoch 00091: val_accuracy improved from 0.77410 to 0.77460, saving model to desktop/Trained_models/inception_cifar100\n",
            "INFO:tensorflow:Assets written to: desktop/Trained_models/inception_cifar100/assets\n",
            "781/781 [==============================] - 227s 291ms/step - loss: 0.3223 - accuracy: 0.9037 - val_loss: 0.9638 - val_accuracy: 0.7746 - lr: 2.5419e-04\n",
            "Epoch 92/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3191 - accuracy: 0.9065\n",
            "Epoch 00092: val_accuracy did not improve from 0.77460\n",
            "781/781 [==============================] - 114s 147ms/step - loss: 0.3191 - accuracy: 0.9065 - val_loss: 0.9728 - val_accuracy: 0.7720 - lr: 2.2877e-04\n",
            "Epoch 93/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3213 - accuracy: 0.9065\n",
            "Epoch 00093: val_accuracy did not improve from 0.77460\n",
            "781/781 [==============================] - 119s 152ms/step - loss: 0.3213 - accuracy: 0.9065 - val_loss: 0.9664 - val_accuracy: 0.7728 - lr: 2.2877e-04\n",
            "Epoch 94/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3298 - accuracy: 0.9031\n",
            "Epoch 00094: val_accuracy did not improve from 0.77460\n",
            "781/781 [==============================] - 134s 172ms/step - loss: 0.3298 - accuracy: 0.9031 - val_loss: 0.9750 - val_accuracy: 0.7723 - lr: 2.2877e-04\n",
            "Epoch 95/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3157 - accuracy: 0.9071\n",
            "Epoch 00095: val_accuracy did not improve from 0.77460\n",
            "781/781 [==============================] - 124s 159ms/step - loss: 0.3157 - accuracy: 0.9071 - val_loss: 0.9757 - val_accuracy: 0.7707 - lr: 2.2877e-04\n",
            "Epoch 96/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3127 - accuracy: 0.9073\n",
            "Epoch 00096: val_accuracy did not improve from 0.77460\n",
            "781/781 [==============================] - 129s 165ms/step - loss: 0.3127 - accuracy: 0.9073 - val_loss: 0.9831 - val_accuracy: 0.7709 - lr: 2.2877e-04\n",
            "Epoch 97/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3118 - accuracy: 0.9081\n",
            "Epoch 00097: val_accuracy did not improve from 0.77460\n",
            "781/781 [==============================] - 129s 165ms/step - loss: 0.3118 - accuracy: 0.9081 - val_loss: 0.9757 - val_accuracy: 0.7729 - lr: 2.0589e-04\n",
            "Epoch 98/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3092 - accuracy: 0.9103\n",
            "Epoch 00098: val_accuracy did not improve from 0.77460\n",
            "781/781 [==============================] - 120s 154ms/step - loss: 0.3092 - accuracy: 0.9103 - val_loss: 0.9743 - val_accuracy: 0.7722 - lr: 2.0589e-04\n",
            "Epoch 99/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3156 - accuracy: 0.9065\n",
            "Epoch 00099: val_accuracy did not improve from 0.77460\n",
            "781/781 [==============================] - 129s 165ms/step - loss: 0.3156 - accuracy: 0.9065 - val_loss: 0.9877 - val_accuracy: 0.7720 - lr: 2.0589e-04\n",
            "Epoch 100/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3161 - accuracy: 0.9070\n",
            "Epoch 00100: val_accuracy did not improve from 0.77460\n",
            "781/781 [==============================] - 129s 165ms/step - loss: 0.3161 - accuracy: 0.9070 - val_loss: 0.9777 - val_accuracy: 0.7733 - lr: 2.0589e-04\n",
            "Epoch 101/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3130 - accuracy: 0.9081\n",
            "Epoch 00101: val_accuracy improved from 0.77460 to 0.77530, saving model to desktop/Trained_models/inception_cifar100\n",
            "INFO:tensorflow:Assets written to: desktop/Trained_models/inception_cifar100/assets\n",
            "781/781 [==============================] - 164s 210ms/step - loss: 0.3130 - accuracy: 0.9081 - val_loss: 0.9705 - val_accuracy: 0.7753 - lr: 2.0589e-04\n",
            "Epoch 102/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3136 - accuracy: 0.9091\n",
            "Epoch 00102: val_accuracy did not improve from 0.77530\n",
            "781/781 [==============================] - 127s 163ms/step - loss: 0.3136 - accuracy: 0.9091 - val_loss: 0.9839 - val_accuracy: 0.7743 - lr: 1.8530e-04\n",
            "Epoch 103/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3046 - accuracy: 0.9101\n",
            "Epoch 00103: val_accuracy did not improve from 0.77530\n",
            "781/781 [==============================] - 131s 167ms/step - loss: 0.3046 - accuracy: 0.9101 - val_loss: 0.9750 - val_accuracy: 0.7737 - lr: 1.8530e-04\n",
            "Epoch 104/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3056 - accuracy: 0.9096\n",
            "Epoch 00104: val_accuracy did not improve from 0.77530\n",
            "781/781 [==============================] - 127s 163ms/step - loss: 0.3056 - accuracy: 0.9096 - val_loss: 0.9808 - val_accuracy: 0.7725 - lr: 1.8530e-04\n",
            "Epoch 105/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3003 - accuracy: 0.9122\n",
            "Epoch 00105: val_accuracy improved from 0.77530 to 0.77570, saving model to desktop/Trained_models/inception_cifar100\n",
            "INFO:tensorflow:Assets written to: desktop/Trained_models/inception_cifar100/assets\n",
            "781/781 [==============================] - 176s 226ms/step - loss: 0.3003 - accuracy: 0.9122 - val_loss: 0.9796 - val_accuracy: 0.7757 - lr: 1.8530e-04\n",
            "Epoch 106/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3060 - accuracy: 0.9113\n",
            "Epoch 00106: val_accuracy did not improve from 0.77570\n",
            "781/781 [==============================] - 120s 154ms/step - loss: 0.3060 - accuracy: 0.9113 - val_loss: 0.9797 - val_accuracy: 0.7731 - lr: 1.8530e-04\n",
            "Epoch 107/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2992 - accuracy: 0.9130\n",
            "Epoch 00107: val_accuracy did not improve from 0.77570\n",
            "781/781 [==============================] - 123s 157ms/step - loss: 0.2992 - accuracy: 0.9130 - val_loss: 0.9739 - val_accuracy: 0.7735 - lr: 1.6677e-04\n",
            "Epoch 108/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3057 - accuracy: 0.9114\n",
            "Epoch 00108: val_accuracy did not improve from 0.77570\n",
            "781/781 [==============================] - 120s 154ms/step - loss: 0.3057 - accuracy: 0.9114 - val_loss: 0.9740 - val_accuracy: 0.7728 - lr: 1.6677e-04\n",
            "Epoch 109/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2974 - accuracy: 0.9125\n",
            "Epoch 00109: val_accuracy improved from 0.77570 to 0.77700, saving model to desktop/Trained_models/inception_cifar100\n",
            "INFO:tensorflow:Assets written to: desktop/Trained_models/inception_cifar100/assets\n",
            "781/781 [==============================] - 164s 210ms/step - loss: 0.2974 - accuracy: 0.9125 - val_loss: 0.9671 - val_accuracy: 0.7770 - lr: 1.6677e-04\n",
            "Epoch 110/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3041 - accuracy: 0.9110\n",
            "Epoch 00110: val_accuracy did not improve from 0.77700\n",
            "781/781 [==============================] - 123s 158ms/step - loss: 0.3041 - accuracy: 0.9110 - val_loss: 0.9733 - val_accuracy: 0.7736 - lr: 1.6677e-04\n",
            "Epoch 111/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.3019 - accuracy: 0.9128\n",
            "Epoch 00111: val_accuracy did not improve from 0.77700\n",
            "781/781 [==============================] - 119s 153ms/step - loss: 0.3019 - accuracy: 0.9128 - val_loss: 0.9758 - val_accuracy: 0.7741 - lr: 1.6677e-04\n",
            "Epoch 112/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2955 - accuracy: 0.9146\n",
            "Epoch 00112: val_accuracy did not improve from 0.77700\n",
            "781/781 [==============================] - 119s 152ms/step - loss: 0.2955 - accuracy: 0.9146 - val_loss: 0.9761 - val_accuracy: 0.7746 - lr: 1.5009e-04\n",
            "Epoch 113/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2987 - accuracy: 0.9124\n",
            "Epoch 00113: val_accuracy did not improve from 0.77700\n",
            "781/781 [==============================] - 119s 152ms/step - loss: 0.2987 - accuracy: 0.9124 - val_loss: 0.9796 - val_accuracy: 0.7757 - lr: 1.5009e-04\n",
            "Epoch 114/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2938 - accuracy: 0.9147\n",
            "Epoch 00114: val_accuracy did not improve from 0.77700\n",
            "781/781 [==============================] - 119s 152ms/step - loss: 0.2938 - accuracy: 0.9147 - val_loss: 0.9828 - val_accuracy: 0.7755 - lr: 1.5009e-04\n",
            "Epoch 115/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2948 - accuracy: 0.9136\n",
            "Epoch 00115: val_accuracy did not improve from 0.77700\n",
            "781/781 [==============================] - 128s 164ms/step - loss: 0.2948 - accuracy: 0.9136 - val_loss: 0.9772 - val_accuracy: 0.7741 - lr: 1.5009e-04\n",
            "Epoch 116/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2919 - accuracy: 0.9159\n",
            "Epoch 00116: val_accuracy improved from 0.77700 to 0.77810, saving model to desktop/Trained_models/inception_cifar100\n",
            "INFO:tensorflow:Assets written to: desktop/Trained_models/inception_cifar100/assets\n",
            "781/781 [==============================] - 183s 235ms/step - loss: 0.2919 - accuracy: 0.9159 - val_loss: 0.9818 - val_accuracy: 0.7781 - lr: 1.5009e-04\n",
            "Epoch 117/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2897 - accuracy: 0.9160\n",
            "Epoch 00117: val_accuracy did not improve from 0.77810\n",
            "781/781 [==============================] - 116s 148ms/step - loss: 0.2897 - accuracy: 0.9160 - val_loss: 0.9677 - val_accuracy: 0.7771 - lr: 1.3509e-04\n",
            "Epoch 118/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2864 - accuracy: 0.9145\n",
            "Epoch 00118: val_accuracy improved from 0.77810 to 0.77980, saving model to desktop/Trained_models/inception_cifar100\n",
            "INFO:tensorflow:Assets written to: desktop/Trained_models/inception_cifar100/assets\n",
            "781/781 [==============================] - 155s 198ms/step - loss: 0.2864 - accuracy: 0.9145 - val_loss: 0.9729 - val_accuracy: 0.7798 - lr: 1.3509e-04\n",
            "Epoch 119/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2895 - accuracy: 0.9158\n",
            "Epoch 00119: val_accuracy did not improve from 0.77980\n",
            "781/781 [==============================] - 117s 149ms/step - loss: 0.2895 - accuracy: 0.9158 - val_loss: 0.9712 - val_accuracy: 0.7769 - lr: 1.3509e-04\n",
            "Epoch 120/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2901 - accuracy: 0.9153\n",
            "Epoch 00120: val_accuracy did not improve from 0.77980\n",
            "781/781 [==============================] - 119s 152ms/step - loss: 0.2901 - accuracy: 0.9153 - val_loss: 0.9798 - val_accuracy: 0.7758 - lr: 1.3509e-04\n",
            "Epoch 121/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2941 - accuracy: 0.9135\n",
            "Epoch 00121: val_accuracy did not improve from 0.77980\n",
            "781/781 [==============================] - 119s 152ms/step - loss: 0.2941 - accuracy: 0.9135 - val_loss: 0.9797 - val_accuracy: 0.7776 - lr: 1.3509e-04\n",
            "Epoch 122/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2848 - accuracy: 0.9174\n",
            "Epoch 00122: val_accuracy did not improve from 0.77980\n",
            "781/781 [==============================] - 119s 152ms/step - loss: 0.2848 - accuracy: 0.9174 - val_loss: 0.9755 - val_accuracy: 0.7777 - lr: 1.2158e-04\n",
            "Epoch 123/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2822 - accuracy: 0.9172\n",
            "Epoch 00123: val_accuracy did not improve from 0.77980\n",
            "781/781 [==============================] - 118s 152ms/step - loss: 0.2822 - accuracy: 0.9172 - val_loss: 0.9797 - val_accuracy: 0.7779 - lr: 1.2158e-04\n",
            "Epoch 124/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2783 - accuracy: 0.9174\n",
            "Epoch 00124: val_accuracy did not improve from 0.77980\n",
            "781/781 [==============================] - 118s 151ms/step - loss: 0.2783 - accuracy: 0.9174 - val_loss: 0.9838 - val_accuracy: 0.7778 - lr: 1.2158e-04\n",
            "Epoch 125/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2791 - accuracy: 0.9193\n",
            "Epoch 00125: val_accuracy did not improve from 0.77980\n",
            "781/781 [==============================] - 125s 160ms/step - loss: 0.2791 - accuracy: 0.9193 - val_loss: 0.9846 - val_accuracy: 0.7784 - lr: 1.2158e-04\n",
            "Epoch 126/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2861 - accuracy: 0.9158\n",
            "Epoch 00126: val_accuracy did not improve from 0.77980\n",
            "781/781 [==============================] - 121s 155ms/step - loss: 0.2861 - accuracy: 0.9158 - val_loss: 0.9859 - val_accuracy: 0.7785 - lr: 1.2158e-04\n",
            "Epoch 127/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2796 - accuracy: 0.9176\n",
            "Epoch 00127: val_accuracy did not improve from 0.77980\n",
            "781/781 [==============================] - 126s 161ms/step - loss: 0.2796 - accuracy: 0.9176 - val_loss: 0.9814 - val_accuracy: 0.7786 - lr: 1.0942e-04\n",
            "Epoch 128/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2816 - accuracy: 0.9174\n",
            "Epoch 00128: val_accuracy did not improve from 0.77980\n",
            "781/781 [==============================] - 118s 151ms/step - loss: 0.2816 - accuracy: 0.9174 - val_loss: 0.9843 - val_accuracy: 0.7778 - lr: 1.0942e-04\n",
            "Epoch 129/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2753 - accuracy: 0.9207\n",
            "Epoch 00129: val_accuracy did not improve from 0.77980\n",
            "781/781 [==============================] - 118s 151ms/step - loss: 0.2753 - accuracy: 0.9207 - val_loss: 0.9778 - val_accuracy: 0.7797 - lr: 1.0942e-04\n",
            "Epoch 130/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2797 - accuracy: 0.9185\n",
            "Epoch 00130: val_accuracy did not improve from 0.77980\n",
            "781/781 [==============================] - 118s 151ms/step - loss: 0.2797 - accuracy: 0.9185 - val_loss: 0.9794 - val_accuracy: 0.7770 - lr: 1.0942e-04\n",
            "Epoch 131/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2792 - accuracy: 0.9195\n",
            "Epoch 00131: val_accuracy did not improve from 0.77980\n",
            "781/781 [==============================] - 118s 151ms/step - loss: 0.2792 - accuracy: 0.9195 - val_loss: 0.9794 - val_accuracy: 0.7780 - lr: 1.0942e-04\n",
            "Epoch 132/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2774 - accuracy: 0.9196\n",
            "Epoch 00132: val_accuracy did not improve from 0.77980\n",
            "781/781 [==============================] - 118s 151ms/step - loss: 0.2774 - accuracy: 0.9196 - val_loss: 0.9760 - val_accuracy: 0.7793 - lr: 9.8477e-05\n",
            "Epoch 133/200\n",
            "780/781 [============================>.] - ETA: 0s - loss: 0.2767 - accuracy: 0.9183\n",
            "Epoch 00133: val_accuracy improved from 0.77980 to 0.78000, saving model to desktop/Trained_models/inception_cifar100\n",
            "INFO:tensorflow:Assets written to: desktop/Trained_models/inception_cifar100/assets\n",
            "781/781 [==============================] - 154s 197ms/step - loss: 0.2767 - accuracy: 0.9183 - val_loss: 0.9771 - val_accuracy: 0.7800 - lr: 9.8477e-05\n",
            "Epoch 134/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2803 - accuracy: 0.9195\n",
            "Epoch 00134: val_accuracy did not improve from 0.78000\n",
            "781/781 [==============================] - 116s 149ms/step - loss: 0.2803 - accuracy: 0.9195 - val_loss: 0.9728 - val_accuracy: 0.7793 - lr: 9.8477e-05\n",
            "Epoch 135/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2681 - accuracy: 0.9215\n",
            "Epoch 00135: val_accuracy did not improve from 0.78000\n",
            "781/781 [==============================] - 119s 152ms/step - loss: 0.2681 - accuracy: 0.9215 - val_loss: 0.9765 - val_accuracy: 0.7770 - lr: 9.8477e-05\n",
            "Epoch 136/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2792 - accuracy: 0.9207\n",
            "Epoch 00136: val_accuracy did not improve from 0.78000\n",
            "781/781 [==============================] - 119s 152ms/step - loss: 0.2792 - accuracy: 0.9207 - val_loss: 0.9818 - val_accuracy: 0.7775 - lr: 9.8477e-05\n",
            "Epoch 137/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2795 - accuracy: 0.9192\n",
            "Epoch 00137: val_accuracy did not improve from 0.78000\n",
            "781/781 [==============================] - 119s 152ms/step - loss: 0.2795 - accuracy: 0.9192 - val_loss: 0.9851 - val_accuracy: 0.7783 - lr: 8.8629e-05\n",
            "Epoch 138/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2788 - accuracy: 0.9185\n",
            "Epoch 00138: val_accuracy did not improve from 0.78000\n",
            "781/781 [==============================] - 118s 151ms/step - loss: 0.2788 - accuracy: 0.9185 - val_loss: 0.9821 - val_accuracy: 0.7784 - lr: 8.8629e-05\n",
            "Epoch 139/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2782 - accuracy: 0.9183\n",
            "Epoch 00139: val_accuracy did not improve from 0.78000\n",
            "781/781 [==============================] - 118s 151ms/step - loss: 0.2782 - accuracy: 0.9183 - val_loss: 0.9785 - val_accuracy: 0.7794 - lr: 8.8629e-05\n",
            "Epoch 140/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2720 - accuracy: 0.9202\n",
            "Epoch 00140: val_accuracy did not improve from 0.78000\n",
            "781/781 [==============================] - 118s 151ms/step - loss: 0.2720 - accuracy: 0.9202 - val_loss: 0.9887 - val_accuracy: 0.7778 - lr: 8.8629e-05\n",
            "Epoch 141/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2747 - accuracy: 0.9201\n",
            "Epoch 00141: val_accuracy did not improve from 0.78000\n",
            "781/781 [==============================] - 118s 151ms/step - loss: 0.2747 - accuracy: 0.9201 - val_loss: 0.9811 - val_accuracy: 0.7776 - lr: 8.8629e-05\n",
            "Epoch 142/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2665 - accuracy: 0.9212\n",
            "Epoch 00142: val_accuracy did not improve from 0.78000\n",
            "781/781 [==============================] - 118s 151ms/step - loss: 0.2665 - accuracy: 0.9212 - val_loss: 0.9839 - val_accuracy: 0.7797 - lr: 7.9766e-05\n",
            "Epoch 143/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2761 - accuracy: 0.9206\n",
            "Epoch 00143: val_accuracy did not improve from 0.78000\n",
            "781/781 [==============================] - 118s 151ms/step - loss: 0.2761 - accuracy: 0.9206 - val_loss: 0.9765 - val_accuracy: 0.7791 - lr: 7.9766e-05\n",
            "Epoch 144/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2787 - accuracy: 0.9185\n",
            "Epoch 00144: val_accuracy did not improve from 0.78000\n",
            "781/781 [==============================] - 118s 151ms/step - loss: 0.2787 - accuracy: 0.9185 - val_loss: 0.9799 - val_accuracy: 0.7792 - lr: 7.9766e-05\n",
            "Epoch 145/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2735 - accuracy: 0.9209\n",
            "Epoch 00145: val_accuracy did not improve from 0.78000\n",
            "781/781 [==============================] - 118s 151ms/step - loss: 0.2735 - accuracy: 0.9209 - val_loss: 0.9760 - val_accuracy: 0.7778 - lr: 7.9766e-05\n",
            "Epoch 146/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2734 - accuracy: 0.9210\n",
            "Epoch 00146: val_accuracy did not improve from 0.78000\n",
            "781/781 [==============================] - 118s 151ms/step - loss: 0.2734 - accuracy: 0.9210 - val_loss: 0.9809 - val_accuracy: 0.7777 - lr: 7.9766e-05\n",
            "Epoch 147/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2659 - accuracy: 0.9224\n",
            "Epoch 00147: val_accuracy did not improve from 0.78000\n",
            "781/781 [==============================] - 118s 151ms/step - loss: 0.2659 - accuracy: 0.9224 - val_loss: 0.9817 - val_accuracy: 0.7787 - lr: 7.1790e-05\n",
            "Epoch 148/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2692 - accuracy: 0.9206\n",
            "Epoch 00148: val_accuracy did not improve from 0.78000\n",
            "781/781 [==============================] - 118s 151ms/step - loss: 0.2692 - accuracy: 0.9206 - val_loss: 0.9804 - val_accuracy: 0.7781 - lr: 7.1790e-05\n",
            "Epoch 149/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2680 - accuracy: 0.9221\n",
            "Epoch 00149: val_accuracy did not improve from 0.78000\n",
            "781/781 [==============================] - 118s 151ms/step - loss: 0.2680 - accuracy: 0.9221 - val_loss: 0.9834 - val_accuracy: 0.7786 - lr: 7.1790e-05\n",
            "Epoch 150/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2741 - accuracy: 0.9198\n",
            "Epoch 00150: val_accuracy did not improve from 0.78000\n",
            "781/781 [==============================] - 118s 151ms/step - loss: 0.2741 - accuracy: 0.9198 - val_loss: 0.9866 - val_accuracy: 0.7793 - lr: 7.1790e-05\n",
            "Epoch 151/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2651 - accuracy: 0.9219\n",
            "Epoch 00151: val_accuracy did not improve from 0.78000\n",
            "781/781 [==============================] - 118s 151ms/step - loss: 0.2651 - accuracy: 0.9219 - val_loss: 0.9814 - val_accuracy: 0.7770 - lr: 7.1790e-05\n",
            "Epoch 152/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2611 - accuracy: 0.9236\n",
            "Epoch 00152: val_accuracy did not improve from 0.78000\n",
            "781/781 [==============================] - 118s 151ms/step - loss: 0.2611 - accuracy: 0.9236 - val_loss: 0.9825 - val_accuracy: 0.7797 - lr: 6.4611e-05\n",
            "Epoch 153/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2676 - accuracy: 0.9235\n",
            "Epoch 00153: val_accuracy did not improve from 0.78000\n",
            "781/781 [==============================] - 118s 151ms/step - loss: 0.2676 - accuracy: 0.9235 - val_loss: 0.9842 - val_accuracy: 0.7793 - lr: 6.4611e-05\n",
            "Epoch 154/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2666 - accuracy: 0.9225\n",
            "Epoch 00154: val_accuracy did not improve from 0.78000\n",
            "781/781 [==============================] - 118s 151ms/step - loss: 0.2666 - accuracy: 0.9225 - val_loss: 0.9878 - val_accuracy: 0.7790 - lr: 6.4611e-05\n",
            "Epoch 155/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2699 - accuracy: 0.9203\n",
            "Epoch 00155: val_accuracy did not improve from 0.78000\n",
            "781/781 [==============================] - 130s 167ms/step - loss: 0.2699 - accuracy: 0.9203 - val_loss: 0.9827 - val_accuracy: 0.7789 - lr: 6.4611e-05\n",
            "Epoch 156/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2684 - accuracy: 0.9225\n",
            "Epoch 00156: val_accuracy did not improve from 0.78000\n",
            "781/781 [==============================] - 119s 152ms/step - loss: 0.2684 - accuracy: 0.9225 - val_loss: 0.9852 - val_accuracy: 0.7784 - lr: 6.4611e-05\n",
            "Epoch 157/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2675 - accuracy: 0.9220\n",
            "Epoch 00157: val_accuracy did not improve from 0.78000\n",
            "781/781 [==============================] - 118s 152ms/step - loss: 0.2675 - accuracy: 0.9220 - val_loss: 0.9873 - val_accuracy: 0.7785 - lr: 5.8150e-05\n",
            "Epoch 158/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2647 - accuracy: 0.9227\n",
            "Epoch 00158: val_accuracy did not improve from 0.78000\n",
            "781/781 [==============================] - 129s 165ms/step - loss: 0.2647 - accuracy: 0.9227 - val_loss: 0.9843 - val_accuracy: 0.7796 - lr: 5.8150e-05\n",
            "Epoch 159/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2626 - accuracy: 0.9237\n",
            "Epoch 00159: val_accuracy did not improve from 0.78000\n",
            "781/781 [==============================] - 130s 167ms/step - loss: 0.2626 - accuracy: 0.9237 - val_loss: 0.9863 - val_accuracy: 0.7782 - lr: 5.8150e-05\n",
            "Epoch 160/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2658 - accuracy: 0.9224\n",
            "Epoch 00160: val_accuracy did not improve from 0.78000\n",
            "781/781 [==============================] - 130s 166ms/step - loss: 0.2658 - accuracy: 0.9224 - val_loss: 0.9846 - val_accuracy: 0.7779 - lr: 5.8150e-05\n",
            "Epoch 161/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2625 - accuracy: 0.9256\n",
            "Epoch 00161: val_accuracy did not improve from 0.78000\n",
            "781/781 [==============================] - 125s 160ms/step - loss: 0.2625 - accuracy: 0.9256 - val_loss: 0.9841 - val_accuracy: 0.7781 - lr: 5.8150e-05\n",
            "Epoch 162/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2564 - accuracy: 0.9244\n",
            "Epoch 00162: val_accuracy did not improve from 0.78000\n",
            "781/781 [==============================] - 130s 167ms/step - loss: 0.2564 - accuracy: 0.9244 - val_loss: 0.9831 - val_accuracy: 0.7773 - lr: 5.2335e-05\n",
            "Epoch 163/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2691 - accuracy: 0.9203\n",
            "Epoch 00163: val_accuracy did not improve from 0.78000\n",
            "781/781 [==============================] - 130s 166ms/step - loss: 0.2691 - accuracy: 0.9203 - val_loss: 0.9838 - val_accuracy: 0.7780 - lr: 5.2335e-05\n",
            "Epoch 164/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2627 - accuracy: 0.9237\n",
            "Epoch 00164: val_accuracy did not improve from 0.78000\n",
            "781/781 [==============================] - 132s 168ms/step - loss: 0.2627 - accuracy: 0.9237 - val_loss: 0.9895 - val_accuracy: 0.7781 - lr: 5.2335e-05\n",
            "Epoch 165/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2614 - accuracy: 0.9235\n",
            "Epoch 00165: val_accuracy improved from 0.78000 to 0.78040, saving model to desktop/Trained_models/inception_cifar100\n",
            "INFO:tensorflow:Assets written to: desktop/Trained_models/inception_cifar100/assets\n",
            "781/781 [==============================] - 170s 217ms/step - loss: 0.2614 - accuracy: 0.9235 - val_loss: 0.9880 - val_accuracy: 0.7804 - lr: 5.2335e-05\n",
            "Epoch 166/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2638 - accuracy: 0.9237\n",
            "Epoch 00166: val_accuracy did not improve from 0.78040\n",
            "781/781 [==============================] - 125s 159ms/step - loss: 0.2638 - accuracy: 0.9237 - val_loss: 0.9866 - val_accuracy: 0.7779 - lr: 5.2335e-05\n",
            "Epoch 167/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2607 - accuracy: 0.9254\n",
            "Epoch 00167: val_accuracy did not improve from 0.78040\n",
            "781/781 [==============================] - 127s 163ms/step - loss: 0.2607 - accuracy: 0.9254 - val_loss: 0.9829 - val_accuracy: 0.7786 - lr: 4.7101e-05\n",
            "Epoch 168/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2675 - accuracy: 0.9228\n",
            "Epoch 00168: val_accuracy did not improve from 0.78040\n",
            "781/781 [==============================] - 127s 163ms/step - loss: 0.2675 - accuracy: 0.9228 - val_loss: 0.9852 - val_accuracy: 0.7789 - lr: 4.7101e-05\n",
            "Epoch 169/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2612 - accuracy: 0.9239\n",
            "Epoch 00169: val_accuracy did not improve from 0.78040\n",
            "781/781 [==============================] - 126s 162ms/step - loss: 0.2612 - accuracy: 0.9239 - val_loss: 0.9842 - val_accuracy: 0.7800 - lr: 4.7101e-05\n",
            "Epoch 170/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2640 - accuracy: 0.9228\n",
            "Epoch 00170: val_accuracy did not improve from 0.78040\n",
            "781/781 [==============================] - 123s 158ms/step - loss: 0.2640 - accuracy: 0.9228 - val_loss: 0.9826 - val_accuracy: 0.7785 - lr: 4.7101e-05\n",
            "Epoch 171/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2624 - accuracy: 0.9246\n",
            "Epoch 00171: val_accuracy did not improve from 0.78040\n",
            "781/781 [==============================] - 124s 159ms/step - loss: 0.2624 - accuracy: 0.9246 - val_loss: 0.9886 - val_accuracy: 0.7778 - lr: 4.7101e-05\n",
            "Epoch 172/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2613 - accuracy: 0.9236\n",
            "Epoch 00172: val_accuracy did not improve from 0.78040\n",
            "781/781 [==============================] - 125s 160ms/step - loss: 0.2613 - accuracy: 0.9236 - val_loss: 0.9893 - val_accuracy: 0.7784 - lr: 4.2391e-05\n",
            "Epoch 173/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2578 - accuracy: 0.9254\n",
            "Epoch 00173: val_accuracy did not improve from 0.78040\n",
            "781/781 [==============================] - 123s 157ms/step - loss: 0.2578 - accuracy: 0.9254 - val_loss: 0.9899 - val_accuracy: 0.7771 - lr: 4.2391e-05\n",
            "Epoch 174/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2578 - accuracy: 0.9262\n",
            "Epoch 00174: val_accuracy did not improve from 0.78040\n",
            "781/781 [==============================] - 120s 154ms/step - loss: 0.2578 - accuracy: 0.9262 - val_loss: 0.9885 - val_accuracy: 0.7780 - lr: 4.2391e-05\n",
            "Epoch 175/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2640 - accuracy: 0.9238\n",
            "Epoch 00175: val_accuracy did not improve from 0.78040\n",
            "781/781 [==============================] - 123s 158ms/step - loss: 0.2640 - accuracy: 0.9238 - val_loss: 0.9887 - val_accuracy: 0.7767 - lr: 4.2391e-05\n",
            "Epoch 176/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2589 - accuracy: 0.9243\n",
            "Epoch 00176: val_accuracy did not improve from 0.78040\n",
            "781/781 [==============================] - 123s 158ms/step - loss: 0.2589 - accuracy: 0.9243 - val_loss: 0.9899 - val_accuracy: 0.7779 - lr: 4.2391e-05\n",
            "Epoch 177/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2624 - accuracy: 0.9243\n",
            "Epoch 00177: val_accuracy did not improve from 0.78040\n",
            "781/781 [==============================] - 123s 158ms/step - loss: 0.2624 - accuracy: 0.9243 - val_loss: 0.9888 - val_accuracy: 0.7768 - lr: 3.8152e-05\n",
            "Epoch 178/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2592 - accuracy: 0.9249\n",
            "Epoch 00178: val_accuracy did not improve from 0.78040\n",
            "781/781 [==============================] - 128s 164ms/step - loss: 0.2592 - accuracy: 0.9249 - val_loss: 0.9939 - val_accuracy: 0.7778 - lr: 3.8152e-05\n",
            "Epoch 179/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2579 - accuracy: 0.9244\n",
            "Epoch 00179: val_accuracy did not improve from 0.78040\n",
            "781/781 [==============================] - 124s 159ms/step - loss: 0.2579 - accuracy: 0.9244 - val_loss: 0.9917 - val_accuracy: 0.7784 - lr: 3.8152e-05\n",
            "Epoch 180/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2567 - accuracy: 0.9259\n",
            "Epoch 00180: val_accuracy did not improve from 0.78040\n",
            "781/781 [==============================] - 123s 158ms/step - loss: 0.2567 - accuracy: 0.9259 - val_loss: 0.9857 - val_accuracy: 0.7782 - lr: 3.8152e-05\n",
            "Epoch 181/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2584 - accuracy: 0.9249\n",
            "Epoch 00181: val_accuracy did not improve from 0.78040\n",
            "781/781 [==============================] - 123s 157ms/step - loss: 0.2584 - accuracy: 0.9249 - val_loss: 0.9854 - val_accuracy: 0.7790 - lr: 3.8152e-05\n",
            "Epoch 182/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2543 - accuracy: 0.9263\n",
            "Epoch 00182: val_accuracy did not improve from 0.78040\n",
            "781/781 [==============================] - 127s 162ms/step - loss: 0.2543 - accuracy: 0.9263 - val_loss: 0.9878 - val_accuracy: 0.7774 - lr: 3.4337e-05\n",
            "Epoch 183/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2547 - accuracy: 0.9259\n",
            "Epoch 00183: val_accuracy did not improve from 0.78040\n",
            "781/781 [==============================] - 118s 151ms/step - loss: 0.2547 - accuracy: 0.9259 - val_loss: 0.9865 - val_accuracy: 0.7783 - lr: 3.4337e-05\n",
            "Epoch 184/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2557 - accuracy: 0.9258\n",
            "Epoch 00184: val_accuracy did not improve from 0.78040\n",
            "781/781 [==============================] - 118s 151ms/step - loss: 0.2557 - accuracy: 0.9258 - val_loss: 0.9892 - val_accuracy: 0.7784 - lr: 3.4337e-05\n",
            "Epoch 185/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2547 - accuracy: 0.9262\n",
            "Epoch 00185: val_accuracy did not improve from 0.78040\n",
            "781/781 [==============================] - 118s 151ms/step - loss: 0.2547 - accuracy: 0.9262 - val_loss: 0.9909 - val_accuracy: 0.7783 - lr: 3.4337e-05\n",
            "Epoch 186/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2563 - accuracy: 0.9260\n",
            "Epoch 00186: val_accuracy did not improve from 0.78040\n",
            "781/781 [==============================] - 118s 151ms/step - loss: 0.2563 - accuracy: 0.9260 - val_loss: 0.9921 - val_accuracy: 0.7782 - lr: 3.4337e-05\n",
            "Epoch 187/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2532 - accuracy: 0.9260\n",
            "Epoch 00187: val_accuracy did not improve from 0.78040\n",
            "781/781 [==============================] - 118s 151ms/step - loss: 0.2532 - accuracy: 0.9260 - val_loss: 0.9904 - val_accuracy: 0.7787 - lr: 3.0903e-05\n",
            "Epoch 188/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2593 - accuracy: 0.9257\n",
            "Epoch 00188: val_accuracy did not improve from 0.78040\n",
            "781/781 [==============================] - 118s 151ms/step - loss: 0.2593 - accuracy: 0.9257 - val_loss: 0.9933 - val_accuracy: 0.7785 - lr: 3.0903e-05\n",
            "Epoch 189/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2603 - accuracy: 0.9245\n",
            "Epoch 00189: val_accuracy did not improve from 0.78040\n",
            "781/781 [==============================] - 126s 161ms/step - loss: 0.2603 - accuracy: 0.9245 - val_loss: 0.9911 - val_accuracy: 0.7782 - lr: 3.0903e-05\n",
            "Epoch 190/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2539 - accuracy: 0.9253\n",
            "Epoch 00190: val_accuracy did not improve from 0.78040\n",
            "781/781 [==============================] - 121s 155ms/step - loss: 0.2539 - accuracy: 0.9253 - val_loss: 0.9932 - val_accuracy: 0.7781 - lr: 3.0903e-05\n",
            "Epoch 191/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2570 - accuracy: 0.9249\n",
            "Epoch 00191: val_accuracy did not improve from 0.78040\n",
            "781/781 [==============================] - 126s 162ms/step - loss: 0.2570 - accuracy: 0.9249 - val_loss: 0.9888 - val_accuracy: 0.7796 - lr: 3.0903e-05\n",
            "Epoch 192/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2555 - accuracy: 0.9269\n",
            "Epoch 00192: val_accuracy did not improve from 0.78040\n",
            "781/781 [==============================] - 128s 163ms/step - loss: 0.2555 - accuracy: 0.9269 - val_loss: 0.9901 - val_accuracy: 0.7786 - lr: 2.7813e-05\n",
            "Epoch 193/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2572 - accuracy: 0.9259\n",
            "Epoch 00193: val_accuracy did not improve from 0.78040\n",
            "781/781 [==============================] - 127s 163ms/step - loss: 0.2572 - accuracy: 0.9259 - val_loss: 0.9924 - val_accuracy: 0.7783 - lr: 2.7813e-05\n",
            "Epoch 194/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2510 - accuracy: 0.9282\n",
            "Epoch 00194: val_accuracy did not improve from 0.78040\n",
            "781/781 [==============================] - 127s 162ms/step - loss: 0.2510 - accuracy: 0.9282 - val_loss: 0.9884 - val_accuracy: 0.7787 - lr: 2.7813e-05\n",
            "Epoch 195/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2584 - accuracy: 0.9247\n",
            "Epoch 00195: val_accuracy did not improve from 0.78040\n",
            "781/781 [==============================] - 127s 163ms/step - loss: 0.2584 - accuracy: 0.9247 - val_loss: 0.9901 - val_accuracy: 0.7784 - lr: 2.7813e-05\n",
            "Epoch 196/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2538 - accuracy: 0.9261\n",
            "Epoch 00196: val_accuracy did not improve from 0.78040\n",
            "781/781 [==============================] - 128s 164ms/step - loss: 0.2538 - accuracy: 0.9261 - val_loss: 0.9904 - val_accuracy: 0.7790 - lr: 2.7813e-05\n",
            "Epoch 197/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2526 - accuracy: 0.9267\n",
            "Epoch 00197: val_accuracy did not improve from 0.78040\n",
            "781/781 [==============================] - 124s 159ms/step - loss: 0.2526 - accuracy: 0.9267 - val_loss: 0.9916 - val_accuracy: 0.7803 - lr: 2.5032e-05\n",
            "Epoch 198/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2614 - accuracy: 0.9244\n",
            "Epoch 00198: val_accuracy did not improve from 0.78040\n",
            "781/781 [==============================] - 122s 157ms/step - loss: 0.2614 - accuracy: 0.9244 - val_loss: 0.9931 - val_accuracy: 0.7782 - lr: 2.5032e-05\n",
            "Epoch 199/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2559 - accuracy: 0.9258\n",
            "Epoch 00199: val_accuracy did not improve from 0.78040\n",
            "781/781 [==============================] - 118s 151ms/step - loss: 0.2559 - accuracy: 0.9258 - val_loss: 0.9893 - val_accuracy: 0.7783 - lr: 2.5032e-05\n",
            "Epoch 200/200\n",
            "781/781 [==============================] - ETA: 0s - loss: 0.2528 - accuracy: 0.9262\n",
            "Epoch 00200: val_accuracy did not improve from 0.78040\n",
            "781/781 [==============================] - 118s 151ms/step - loss: 0.2528 - accuracy: 0.9262 - val_loss: 0.9921 - val_accuracy: 0.7793 - lr: 2.5032e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize = (9,6))\n",
        "plt.plot(history.history['val_loss'],label='Test loss')\n",
        "plt.plot(history.history['loss'],label='Train loss')\n",
        "plt.title('Loss curve for inception model')\n",
        "plt.ylabel('Loss value')\n",
        "plt.xlabel('No. epoch')\n",
        "plt.show()\n",
        "\n",
        "# Plot history: Accuracy\n",
        "plt.figure(figsize = (9,6))\n",
        "plt.plot(history.history['val_accuracy'],label = 'Test accuracy')\n",
        "plt.plot(history.history['accuracy'],label = 'Train accuracy')\n",
        "plt.title('Accuracy curve for inception model')\n",
        "plt.ylabel('Accuracy value (%)')\n",
        "plt.xlabel('No. epoch')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lKis1belCjmY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 791
        },
        "outputId": "5c2c781e-aeb5-4320-8790-608ad91f8c0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 648x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 648x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras,os\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential,Model\n",
        "from keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D , Dropout\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from keras.initializers import glorot_uniform\n",
        "from keras import applications\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50 , preprocess_input\n",
        "from keras import regularizers\n",
        "from absl import app, flags\n",
        "\n",
        "# Install bleeding edge version of cleverhans\n",
        "!pip install git+https://github.com/tensorflow/cleverhans.git#egg=cleverhans\n",
        "from cleverhans.tf2.attacks.projected_gradient_descent import projected_gradient_descent\n",
        "from cleverhans.tf2.attacks.fast_gradient_method import fast_gradient_method"
      ],
      "metadata": {
        "id": "QHYfZZ8pRRrE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53149b7f-f1c0-4107-e205-7670ff4443dc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: cleverhans from git+https://github.com/tensorflow/cleverhans.git#egg=cleverhans in ./.local/lib/python3.6/site-packages (4.0.0)\n",
            "Requirement already satisfied: nose in ./.local/lib/python3.6/site-packages (from cleverhans) (1.3.7)\n",
            "Requirement already satisfied: pycodestyle in ./.local/lib/python3.6/site-packages (from cleverhans) (2.8.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from cleverhans) (1.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cleverhans) (1.15.0)\n",
            "Requirement already satisfied: easydict in ./.local/lib/python3.6/site-packages (from cleverhans) (1.9)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from cleverhans) (0.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from cleverhans) (1.19.1)\n",
            "Requirement already satisfied: mnist in ./.local/lib/python3.6/site-packages (from cleverhans) (0.2.2)\n",
            "Requirement already satisfied: tensorflow-probability in ./.local/lib/python3.6/site-packages (from cleverhans) (0.14.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from cleverhans) (3.3.1)\n",
            "Requirement already satisfied: joblib in ./.local/lib/python3.6/site-packages (from cleverhans) (1.1.0)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability->cleverhans) (0.3.3)\n",
            "Requirement already satisfied: dm-tree in ./.local/lib/python3.6/site-packages (from tensorflow-probability->cleverhans) (0.1.6)\n",
            "Requirement already satisfied: decorator in ./.local/lib/python3.6/site-packages (from tensorflow-probability->cleverhans) (4.4.2)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in ./.local/lib/python3.6/site-packages (from tensorflow-probability->cleverhans) (2.0.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.6/dist-packages (from matplotlib->cleverhans) (7.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->cleverhans) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->cleverhans) (1.2.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.6/dist-packages (from matplotlib->cleverhans) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2020.06.20 in /usr/local/lib/python3.6/dist-packages (from matplotlib->cleverhans) (2020.6.20)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->cleverhans) (0.10.0)\n",
            "\u001b[33mWARNING: You are using pip version 20.2.2; however, version 21.3.1 is available.\n",
            "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cifar-100"
      ],
      "metadata": {
        "id": "lk6G514c1PGP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def make_prediction(model , image , true_value):\n",
        "  true_label_index = -1\n",
        "  for i in range(len(true_value)):\n",
        "\n",
        "    if(true_value[i]==1):\n",
        "      true_label_index = i\n",
        "      break\n",
        "\n",
        "  prediction = model.predict(image)[0]\n",
        "  probability = float('-inf')\n",
        "  predicted_label_index = -1\n",
        "\n",
        "  for i in range(len(prediction)):\n",
        "\n",
        "    if(prediction[i]>probability):\n",
        "      probability = prediction[i]\n",
        "      predicted_label_index = i\n",
        "\n",
        "  if(true_label_index!=predicted_label_index):\n",
        "    return 1\n",
        "  return 0"
      ],
      "metadata": {
        "id": "1gqyNwA80x7Y"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.load_model(model_path)"
      ],
      "metadata": {
        "id": "2cgH39jZtpoa"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import img_to_array\n",
        "\n",
        "epsilon_end = 0.8\n",
        "#logits_model = tf.keras.Model(model.input,model.layers[-1].output)\n",
        "total_images = 200\n",
        "\n",
        "result={}\n",
        "epsilon = 0.1\n",
        "while(epsilon <= epsilon_end):\n",
        "\n",
        "  fgsm_counter = 0\n",
        "  pgd_counter = 0\n",
        "  print(\"Epsilon value - \",epsilon)\n",
        "  print()\n",
        "  for image_index in range(total_images):\n",
        "  \n",
        "  \n",
        "    image = x_test[image_index]\n",
        "    image = img_to_array(image)\n",
        "    image = image.reshape(1, 32, 32, 3)\n",
        "    true_value = y_test[image_index]\n",
        "    original_prediction = make_prediction(model,image,true_value)\n",
        "    fgsm_sample = fast_gradient_method(model, image, epsilon, np.inf, targeted=False)\n",
        "    pgd_sample = projected_gradient_descent(model, image, epsilon, 0.01, 40, np.inf)\n",
        "    print(epsilon,' ',image_index)\n",
        "    fgsm_prediction = make_prediction(model , fgsm_sample , true_value)\n",
        "    pgd_prediction = make_prediction(model , pgd_sample , true_value)\n",
        "    pgd_counter+=pgd_prediction\n",
        "    fgsm_counter+=fgsm_prediction\n",
        "    print(pgd_counter,' ',fgsm_counter)\n",
        "  result[epsilon] = (fgsm_counter , pgd_counter)\n",
        "  epsilon+=0.1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKLmNCokttNO",
        "outputId": "b4709e0f-1d89-402a-b41e-9860e472e1f3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epsilon value -  0.1\n",
            "\n",
            "0.1   0\n",
            "1   1\n",
            "0.1   1\n",
            "2   2\n",
            "0.1   2\n",
            "3   3\n",
            "0.1   3\n",
            "3   3\n",
            "0.1   4\n",
            "3   3\n",
            "0.1   5\n",
            "4   4\n",
            "0.1   6\n",
            "5   4\n",
            "0.1   7\n",
            "5   4\n",
            "0.1   8\n",
            "6   5\n",
            "0.1   9\n",
            "6   5\n",
            "0.1   10\n",
            "6   5\n",
            "0.1   11\n",
            "7   6\n",
            "0.1   12\n",
            "8   6\n",
            "0.1   13\n",
            "8   6\n",
            "0.1   14\n",
            "8   6\n",
            "0.1   15\n",
            "8   6\n",
            "0.1   16\n",
            "8   6\n",
            "0.1   17\n",
            "8   6\n",
            "0.1   18\n",
            "8   6\n",
            "0.1   19\n",
            "8   6\n",
            "0.1   20\n",
            "8   6\n",
            "0.1   21\n",
            "8   6\n",
            "0.1   22\n",
            "8   6\n",
            "0.1   23\n",
            "9   7\n",
            "0.1   24\n",
            "9   7\n",
            "0.1   25\n",
            "9   7\n",
            "0.1   26\n",
            "9   7\n",
            "0.1   27\n",
            "9   7\n",
            "0.1   28\n",
            "9   7\n",
            "0.1   29\n",
            "10   8\n",
            "0.1   30\n",
            "10   8\n",
            "0.1   31\n",
            "10   8\n",
            "0.1   32\n",
            "10   8\n",
            "0.1   33\n",
            "11   9\n",
            "0.1   34\n",
            "12   10\n",
            "0.1   35\n",
            "12   10\n",
            "0.1   36\n",
            "12   10\n",
            "0.1   37\n",
            "12   10\n",
            "0.1   38\n",
            "12   10\n",
            "0.1   39\n",
            "12   10\n",
            "0.1   40\n",
            "13   10\n",
            "0.1   41\n",
            "14   11\n",
            "0.1   42\n",
            "14   11\n",
            "0.1   43\n",
            "15   12\n",
            "0.1   44\n",
            "15   12\n",
            "0.1   45\n",
            "15   12\n",
            "0.1   46\n",
            "15   12\n",
            "0.1   47\n",
            "15   12\n",
            "0.1   48\n",
            "16   13\n",
            "0.1   49\n",
            "16   13\n",
            "0.1   50\n",
            "16   13\n",
            "0.1   51\n",
            "17   14\n",
            "0.1   52\n",
            "17   14\n",
            "0.1   53\n",
            "17   14\n",
            "0.1   54\n",
            "17   14\n",
            "0.1   55\n",
            "18   15\n",
            "0.1   56\n",
            "18   15\n",
            "0.1   57\n",
            "18   15\n",
            "0.1   58\n",
            "18   15\n",
            "0.1   59\n",
            "18   15\n",
            "0.1   60\n",
            "18   15\n",
            "0.1   61\n",
            "18   15\n",
            "0.1   62\n",
            "19   16\n",
            "0.1   63\n",
            "20   17\n",
            "0.1   64\n",
            "20   17\n",
            "0.1   65\n",
            "20   17\n",
            "0.1   66\n",
            "20   17\n",
            "0.1   67\n",
            "21   18\n",
            "0.1   68\n",
            "21   18\n",
            "0.1   69\n",
            "21   18\n",
            "0.1   70\n",
            "21   18\n",
            "0.1   71\n",
            "22   19\n",
            "0.1   72\n",
            "22   19\n",
            "0.1   73\n",
            "22   19\n",
            "0.1   74\n",
            "23   20\n",
            "0.1   75\n",
            "23   20\n",
            "0.1   76\n",
            "23   20\n",
            "0.1   77\n",
            "24   21\n",
            "0.1   78\n",
            "24   21\n",
            "0.1   79\n",
            "24   21\n",
            "0.1   80\n",
            "24   21\n",
            "0.1   81\n",
            "24   21\n",
            "0.1   82\n",
            "25   21\n",
            "0.1   83\n",
            "25   21\n",
            "0.1   84\n",
            "25   21\n",
            "0.1   85\n",
            "25   21\n",
            "0.1   86\n",
            "25   22\n",
            "0.1   87\n",
            "25   22\n",
            "0.1   88\n",
            "26   23\n",
            "0.1   89\n",
            "27   24\n",
            "0.1   90\n",
            "27   24\n",
            "0.1   91\n",
            "27   24\n",
            "0.1   92\n",
            "27   24\n",
            "0.1   93\n",
            "27   24\n",
            "0.1   94\n",
            "27   24\n",
            "0.1   95\n",
            "28   25\n",
            "0.1   96\n",
            "28   25\n",
            "0.1   97\n",
            "29   26\n",
            "0.1   98\n",
            "29   26\n",
            "0.1   99\n",
            "30   27\n",
            "0.1   100\n",
            "30   27\n",
            "0.1   101\n",
            "30   28\n",
            "0.1   102\n",
            "30   28\n",
            "0.1   103\n",
            "31   29\n",
            "0.1   104\n",
            "31   29\n",
            "0.1   105\n",
            "31   29\n",
            "0.1   106\n",
            "31   29\n",
            "0.1   107\n",
            "31   29\n",
            "0.1   108\n",
            "31   29\n",
            "0.1   109\n",
            "31   29\n",
            "0.1   110\n",
            "31   29\n",
            "0.1   111\n",
            "31   29\n",
            "0.1   112\n",
            "31   29\n",
            "0.1   113\n",
            "32   30\n",
            "0.1   114\n",
            "32   30\n",
            "0.1   115\n",
            "33   31\n",
            "0.1   116\n",
            "33   31\n",
            "0.1   117\n",
            "33   31\n",
            "0.1   118\n",
            "34   32\n",
            "0.1   119\n",
            "34   32\n",
            "0.1   120\n",
            "34   32\n",
            "0.1   121\n",
            "35   33\n",
            "0.1   122\n",
            "36   33\n",
            "0.1   123\n",
            "37   34\n",
            "0.1   124\n",
            "37   34\n",
            "0.1   125\n",
            "38   35\n",
            "0.1   126\n",
            "38   35\n",
            "0.1   127\n",
            "39   36\n",
            "0.1   128\n",
            "39   36\n",
            "0.1   129\n",
            "39   36\n",
            "0.1   130\n",
            "40   37\n",
            "0.1   131\n",
            "40   37\n",
            "0.1   132\n",
            "40   37\n",
            "0.1   133\n",
            "40   37\n",
            "0.1   134\n",
            "41   38\n",
            "0.1   135\n",
            "41   38\n",
            "0.1   136\n",
            "41   38\n",
            "0.1   137\n",
            "41   38\n",
            "0.1   138\n",
            "41   38\n",
            "0.1   139\n",
            "41   38\n",
            "0.1   140\n",
            "42   39\n",
            "0.1   141\n",
            "42   39\n",
            "0.1   142\n",
            "43   40\n",
            "0.1   143\n",
            "44   41\n",
            "0.1   144\n",
            "44   41\n",
            "0.1   145\n",
            "44   41\n",
            "0.1   146\n",
            "45   42\n",
            "0.1   147\n",
            "45   42\n",
            "0.1   148\n",
            "45   42\n",
            "0.1   149\n",
            "45   42\n",
            "0.1   150\n",
            "46   43\n",
            "0.1   151\n",
            "47   43\n",
            "0.1   152\n",
            "48   44\n",
            "0.1   153\n",
            "49   45\n",
            "0.1   154\n",
            "49   45\n",
            "0.1   155\n",
            "50   46\n",
            "0.1   156\n",
            "50   46\n",
            "0.1   157\n",
            "51   47\n",
            "0.1   158\n",
            "51   47\n",
            "0.1   159\n",
            "52   48\n",
            "0.1   160\n",
            "53   48\n",
            "0.1   161\n",
            "53   48\n",
            "0.1   162\n",
            "54   49\n",
            "0.1   163\n",
            "54   49\n",
            "0.1   164\n",
            "55   50\n",
            "0.1   165\n",
            "56   51\n",
            "0.1   166\n",
            "56   51\n",
            "0.1   167\n",
            "56   51\n",
            "0.1   168\n",
            "56   51\n",
            "0.1   169\n",
            "56   51\n",
            "0.1   170\n",
            "56   51\n",
            "0.1   171\n",
            "56   51\n",
            "0.1   172\n",
            "57   52\n",
            "0.1   173\n",
            "58   52\n",
            "0.1   174\n",
            "58   52\n",
            "0.1   175\n",
            "59   53\n",
            "0.1   176\n",
            "59   53\n",
            "0.1   177\n",
            "60   54\n",
            "0.1   178\n",
            "60   54\n",
            "0.1   179\n",
            "60   54\n",
            "0.1   180\n",
            "60   54\n",
            "0.1   181\n",
            "61   55\n",
            "0.1   182\n",
            "61   55\n",
            "0.1   183\n",
            "62   56\n",
            "0.1   184\n",
            "63   57\n",
            "0.1   185\n",
            "63   57\n",
            "0.1   186\n",
            "64   58\n",
            "0.1   187\n",
            "64   58\n",
            "0.1   188\n",
            "65   59\n",
            "0.1   189\n",
            "65   59\n",
            "0.1   190\n",
            "65   59\n",
            "0.1   191\n",
            "65   59\n",
            "0.1   192\n",
            "66   60\n",
            "0.1   193\n",
            "66   60\n",
            "0.1   194\n",
            "66   60\n",
            "0.1   195\n",
            "67   60\n",
            "0.1   196\n",
            "67   60\n",
            "0.1   197\n",
            "68   60\n",
            "0.1   198\n",
            "68   60\n",
            "0.1   199\n",
            "68   60\n",
            "Epsilon value -  0.2\n",
            "\n",
            "0.2   0\n",
            "1   1\n",
            "0.2   1\n",
            "2   2\n",
            "0.2   2\n",
            "3   3\n",
            "0.2   3\n",
            "3   3\n",
            "0.2   4\n",
            "4   3\n",
            "0.2   5\n",
            "5   4\n",
            "0.2   6\n",
            "6   5\n",
            "0.2   7\n",
            "6   5\n",
            "0.2   8\n",
            "7   6\n",
            "0.2   9\n",
            "7   6\n",
            "0.2   10\n",
            "8   7\n",
            "0.2   11\n",
            "9   8\n",
            "0.2   12\n",
            "10   9\n",
            "0.2   13\n",
            "11   9\n",
            "0.2   14\n",
            "11   9\n",
            "0.2   15\n",
            "11   9\n",
            "0.2   16\n",
            "11   9\n",
            "0.2   17\n",
            "11   9\n",
            "0.2   18\n",
            "11   9\n",
            "0.2   19\n",
            "11   9\n",
            "0.2   20\n",
            "11   9\n",
            "0.2   21\n",
            "12   9\n",
            "0.2   22\n",
            "12   9\n",
            "0.2   23\n",
            "13   10\n",
            "0.2   24\n",
            "13   10\n",
            "0.2   25\n",
            "13   10\n",
            "0.2   26\n",
            "13   10\n",
            "0.2   27\n",
            "13   10\n",
            "0.2   28\n",
            "13   10\n",
            "0.2   29\n",
            "14   11\n",
            "0.2   30\n",
            "14   11\n",
            "0.2   31\n",
            "14   11\n",
            "0.2   32\n",
            "15   11\n",
            "0.2   33\n",
            "16   11\n",
            "0.2   34\n",
            "17   12\n",
            "0.2   35\n",
            "17   12\n",
            "0.2   36\n",
            "17   12\n",
            "0.2   37\n",
            "18   12\n",
            "0.2   38\n",
            "19   13\n",
            "0.2   39\n",
            "19   13\n",
            "0.2   40\n",
            "20   14\n",
            "0.2   41\n",
            "21   15\n",
            "0.2   42\n",
            "22   16\n",
            "0.2   43\n",
            "23   17\n",
            "0.2   44\n",
            "23   17\n",
            "0.2   45\n",
            "23   17\n",
            "0.2   46\n",
            "23   17\n",
            "0.2   47\n",
            "23   17\n",
            "0.2   48\n",
            "24   18\n",
            "0.2   49\n",
            "24   18\n",
            "0.2   50\n",
            "25   19\n",
            "0.2   51\n",
            "26   20\n",
            "0.2   52\n",
            "26   20\n",
            "0.2   53\n",
            "26   20\n",
            "0.2   54\n",
            "26   20\n",
            "0.2   55\n",
            "27   21\n",
            "0.2   56\n",
            "28   21\n",
            "0.2   57\n",
            "29   22\n",
            "0.2   58\n",
            "29   22\n",
            "0.2   59\n",
            "30   22\n",
            "0.2   60\n",
            "30   22\n",
            "0.2   61\n",
            "31   23\n",
            "0.2   62\n",
            "32   24\n",
            "0.2   63\n",
            "33   25\n",
            "0.2   64\n",
            "33   25\n",
            "0.2   65\n",
            "33   25\n",
            "0.2   66\n",
            "33   25\n",
            "0.2   67\n",
            "34   26\n",
            "0.2   68\n",
            "34   26\n",
            "0.2   69\n",
            "34   26\n",
            "0.2   70\n",
            "35   27\n",
            "0.2   71\n",
            "36   28\n",
            "0.2   72\n",
            "36   28\n",
            "0.2   73\n",
            "36   28\n",
            "0.2   74\n",
            "37   29\n",
            "0.2   75\n",
            "37   29\n",
            "0.2   76\n",
            "37   29\n",
            "0.2   77\n",
            "38   30\n",
            "0.2   78\n",
            "38   30\n",
            "0.2   79\n",
            "38   30\n",
            "0.2   80\n",
            "38   30\n",
            "0.2   81\n",
            "38   30\n",
            "0.2   82\n",
            "39   31\n",
            "0.2   83\n",
            "39   31\n",
            "0.2   84\n",
            "40   31\n",
            "0.2   85\n",
            "40   31\n",
            "0.2   86\n",
            "40   31\n",
            "0.2   87\n",
            "40   31\n",
            "0.2   88\n",
            "41   32\n",
            "0.2   89\n",
            "42   33\n",
            "0.2   90\n",
            "42   33\n",
            "0.2   91\n",
            "43   33\n",
            "0.2   92\n",
            "43   33\n",
            "0.2   93\n",
            "43   33\n",
            "0.2   94\n",
            "43   33\n",
            "0.2   95\n",
            "44   34\n",
            "0.2   96\n",
            "44   34\n",
            "0.2   97\n",
            "45   35\n",
            "0.2   98\n",
            "46   35\n",
            "0.2   99\n",
            "47   36\n",
            "0.2   100\n",
            "48   37\n",
            "0.2   101\n",
            "48   37\n",
            "0.2   102\n",
            "48   37\n",
            "0.2   103\n",
            "49   38\n",
            "0.2   104\n",
            "49   38\n",
            "0.2   105\n",
            "50   38\n",
            "0.2   106\n",
            "50   38\n",
            "0.2   107\n",
            "50   38\n",
            "0.2   108\n",
            "50   38\n",
            "0.2   109\n",
            "51   38\n",
            "0.2   110\n",
            "52   39\n",
            "0.2   111\n",
            "52   39\n",
            "0.2   112\n",
            "53   40\n",
            "0.2   113\n",
            "54   41\n",
            "0.2   114\n",
            "55   41\n",
            "0.2   115\n",
            "56   42\n",
            "0.2   116\n",
            "56   42\n",
            "0.2   117\n",
            "56   42\n",
            "0.2   118\n",
            "57   43\n",
            "0.2   119\n",
            "57   43\n",
            "0.2   120\n",
            "57   43\n",
            "0.2   121\n",
            "58   44\n",
            "0.2   122\n",
            "59   45\n",
            "0.2   123\n",
            "60   46\n",
            "0.2   124\n",
            "60   46\n",
            "0.2   125\n",
            "61   47\n",
            "0.2   126\n",
            "62   48\n",
            "0.2   127\n",
            "63   49\n",
            "0.2   128\n",
            "63   49\n",
            "0.2   129\n",
            "64   50\n",
            "0.2   130\n",
            "65   51\n",
            "0.2   131\n",
            "65   51\n",
            "0.2   132\n",
            "66   51\n",
            "0.2   133\n",
            "67   52\n",
            "0.2   134\n",
            "68   53\n",
            "0.2   135\n",
            "68   53\n",
            "0.2   136\n",
            "69   53\n",
            "0.2   137\n",
            "70   53\n",
            "0.2   138\n",
            "70   53\n",
            "0.2   139\n",
            "71   53\n",
            "0.2   140\n",
            "72   54\n",
            "0.2   141\n",
            "72   54\n",
            "0.2   142\n",
            "73   55\n",
            "0.2   143\n",
            "74   56\n",
            "0.2   144\n",
            "74   56\n",
            "0.2   145\n",
            "75   57\n",
            "0.2   146\n",
            "76   58\n",
            "0.2   147\n",
            "76   58\n",
            "0.2   148\n",
            "76   58\n",
            "0.2   149\n",
            "76   58\n",
            "0.2   150\n",
            "77   59\n",
            "0.2   151\n",
            "78   60\n",
            "0.2   152\n",
            "79   61\n",
            "0.2   153\n",
            "80   62\n",
            "0.2   154\n",
            "80   62\n",
            "0.2   155\n",
            "81   63\n",
            "0.2   156\n",
            "81   63\n",
            "0.2   157\n",
            "82   64\n",
            "0.2   158\n",
            "82   64\n",
            "0.2   159\n",
            "83   65\n",
            "0.2   160\n",
            "84   66\n",
            "0.2   161\n",
            "84   66\n",
            "0.2   162\n",
            "85   67\n",
            "0.2   163\n",
            "85   67\n",
            "0.2   164\n",
            "86   68\n",
            "0.2   165\n",
            "87   69\n",
            "0.2   166\n",
            "87   69\n",
            "0.2   167\n",
            "88   69\n",
            "0.2   168\n",
            "88   69\n",
            "0.2   169\n",
            "89   70\n",
            "0.2   170\n",
            "90   71\n",
            "0.2   171\n",
            "90   71\n",
            "0.2   172\n",
            "91   72\n",
            "0.2   173\n",
            "92   73\n",
            "0.2   174\n",
            "92   73\n",
            "0.2   175\n",
            "93   74\n",
            "0.2   176\n",
            "94   74\n",
            "0.2   177\n",
            "95   75\n",
            "0.2   178\n",
            "95   75\n",
            "0.2   179\n",
            "95   75\n",
            "0.2   180\n",
            "95   75\n",
            "0.2   181\n",
            "96   76\n",
            "0.2   182\n",
            "96   76\n",
            "0.2   183\n",
            "97   77\n",
            "0.2   184\n",
            "97   77\n",
            "0.2   185\n",
            "98   77\n",
            "0.2   186\n",
            "99   78\n",
            "0.2   187\n",
            "99   78\n",
            "0.2   188\n",
            "100   79\n",
            "0.2   189\n",
            "101   80\n",
            "0.2   190\n",
            "101   80\n",
            "0.2   191\n",
            "101   80\n",
            "0.2   192\n",
            "102   81\n",
            "0.2   193\n",
            "102   81\n",
            "0.2   194\n",
            "102   81\n",
            "0.2   195\n",
            "103   82\n",
            "0.2   196\n",
            "104   82\n",
            "0.2   197\n",
            "105   83\n",
            "0.2   198\n",
            "106   83\n",
            "0.2   199\n",
            "107   84\n",
            "Epsilon value -  0.30000000000000004\n",
            "\n",
            "0.30000000000000004   0\n",
            "1   1\n",
            "0.30000000000000004   1\n",
            "2   1\n",
            "0.30000000000000004   2\n",
            "3   2\n",
            "0.30000000000000004   3\n",
            "3   2\n",
            "0.30000000000000004   4\n",
            "4   2\n",
            "0.30000000000000004   5\n",
            "5   3\n",
            "0.30000000000000004   6\n",
            "6   4\n",
            "0.30000000000000004   7\n",
            "6   4\n",
            "0.30000000000000004   8\n",
            "7   5\n",
            "0.30000000000000004   9\n",
            "7   5\n",
            "0.30000000000000004   10\n",
            "8   6\n",
            "0.30000000000000004   11\n",
            "9   7\n",
            "0.30000000000000004   12\n",
            "10   8\n",
            "0.30000000000000004   13\n",
            "11   8\n",
            "0.30000000000000004   14\n",
            "11   8\n",
            "0.30000000000000004   15\n",
            "11   8\n",
            "0.30000000000000004   16\n",
            "11   8\n",
            "0.30000000000000004   17\n",
            "11   8\n",
            "0.30000000000000004   18\n",
            "12   9\n",
            "0.30000000000000004   19\n",
            "12   9\n",
            "0.30000000000000004   20\n",
            "12   9\n",
            "0.30000000000000004   21\n",
            "13   10\n",
            "0.30000000000000004   22\n",
            "14   10\n",
            "0.30000000000000004   23\n",
            "15   11\n",
            "0.30000000000000004   24\n",
            "15   11\n",
            "0.30000000000000004   25\n",
            "15   11\n",
            "0.30000000000000004   26\n",
            "15   11\n",
            "0.30000000000000004   27\n",
            "15   11\n",
            "0.30000000000000004   28\n",
            "15   11\n",
            "0.30000000000000004   29\n",
            "16   12\n",
            "0.30000000000000004   30\n",
            "16   12\n",
            "0.30000000000000004   31\n",
            "16   12\n",
            "0.30000000000000004   32\n",
            "17   12\n",
            "0.30000000000000004   33\n",
            "18   12\n",
            "0.30000000000000004   34\n",
            "19   13\n",
            "0.30000000000000004   35\n",
            "19   13\n",
            "0.30000000000000004   36\n",
            "19   13\n",
            "0.30000000000000004   37\n",
            "20   14\n",
            "0.30000000000000004   38\n",
            "21   15\n",
            "0.30000000000000004   39\n",
            "22   15\n",
            "0.30000000000000004   40\n",
            "23   16\n",
            "0.30000000000000004   41\n",
            "24   17\n",
            "0.30000000000000004   42\n",
            "25   18\n",
            "0.30000000000000004   43\n",
            "26   19\n",
            "0.30000000000000004   44\n",
            "26   19\n",
            "0.30000000000000004   45\n",
            "26   19\n",
            "0.30000000000000004   46\n",
            "26   19\n",
            "0.30000000000000004   47\n",
            "26   19\n",
            "0.30000000000000004   48\n",
            "27   20\n",
            "0.30000000000000004   49\n",
            "27   20\n",
            "0.30000000000000004   50\n",
            "28   21\n",
            "0.30000000000000004   51\n",
            "29   22\n",
            "0.30000000000000004   52\n",
            "29   22\n",
            "0.30000000000000004   53\n",
            "29   22\n",
            "0.30000000000000004   54\n",
            "30   22\n",
            "0.30000000000000004   55\n",
            "31   23\n",
            "0.30000000000000004   56\n",
            "32   24\n",
            "0.30000000000000004   57\n",
            "33   25\n",
            "0.30000000000000004   58\n",
            "33   25\n",
            "0.30000000000000004   59\n",
            "34   26\n",
            "0.30000000000000004   60\n",
            "34   26\n",
            "0.30000000000000004   61\n",
            "35   27\n",
            "0.30000000000000004   62\n",
            "36   28\n",
            "0.30000000000000004   63\n",
            "37   29\n",
            "0.30000000000000004   64\n",
            "37   29\n",
            "0.30000000000000004   65\n",
            "37   29\n",
            "0.30000000000000004   66\n",
            "37   29\n",
            "0.30000000000000004   67\n",
            "38   30\n",
            "0.30000000000000004   68\n",
            "38   30\n",
            "0.30000000000000004   69\n",
            "38   30\n",
            "0.30000000000000004   70\n",
            "39   31\n",
            "0.30000000000000004   71\n",
            "40   32\n",
            "0.30000000000000004   72\n",
            "41   32\n",
            "0.30000000000000004   73\n",
            "41   32\n",
            "0.30000000000000004   74\n",
            "42   33\n",
            "0.30000000000000004   75\n",
            "42   33\n",
            "0.30000000000000004   76\n",
            "42   33\n",
            "0.30000000000000004   77\n",
            "43   34\n",
            "0.30000000000000004   78\n",
            "44   34\n",
            "0.30000000000000004   79\n",
            "45   34\n",
            "0.30000000000000004   80\n",
            "45   34\n",
            "0.30000000000000004   81\n",
            "45   34\n",
            "0.30000000000000004   82\n",
            "46   35\n",
            "0.30000000000000004   83\n",
            "47   35\n",
            "0.30000000000000004   84\n",
            "48   36\n",
            "0.30000000000000004   85\n",
            "48   36\n",
            "0.30000000000000004   86\n",
            "48   36\n",
            "0.30000000000000004   87\n",
            "48   36\n",
            "0.30000000000000004   88\n",
            "49   37\n",
            "0.30000000000000004   89\n",
            "50   38\n",
            "0.30000000000000004   90\n",
            "50   38\n",
            "0.30000000000000004   91\n",
            "51   39\n",
            "0.30000000000000004   92\n",
            "51   39\n",
            "0.30000000000000004   93\n",
            "51   39\n",
            "0.30000000000000004   94\n",
            "52   39\n",
            "0.30000000000000004   95\n",
            "53   40\n",
            "0.30000000000000004   96\n",
            "54   40\n",
            "0.30000000000000004   97\n",
            "55   41\n",
            "0.30000000000000004   98\n",
            "56   41\n",
            "0.30000000000000004   99\n",
            "57   42\n",
            "0.30000000000000004   100\n",
            "58   43\n",
            "0.30000000000000004   101\n",
            "58   43\n",
            "0.30000000000000004   102\n",
            "58   43\n",
            "0.30000000000000004   103\n",
            "59   44\n",
            "0.30000000000000004   104\n",
            "60   44\n",
            "0.30000000000000004   105\n",
            "61   44\n",
            "0.30000000000000004   106\n",
            "61   44\n",
            "0.30000000000000004   107\n",
            "61   44\n",
            "0.30000000000000004   108\n",
            "62   44\n",
            "0.30000000000000004   109\n",
            "63   44\n",
            "0.30000000000000004   110\n",
            "64   45\n",
            "0.30000000000000004   111\n",
            "64   45\n",
            "0.30000000000000004   112\n",
            "65   46\n",
            "0.30000000000000004   113\n",
            "66   47\n",
            "0.30000000000000004   114\n",
            "67   48\n",
            "0.30000000000000004   115\n",
            "68   49\n",
            "0.30000000000000004   116\n",
            "69   49\n",
            "0.30000000000000004   117\n",
            "69   49\n",
            "0.30000000000000004   118\n",
            "70   50\n",
            "0.30000000000000004   119\n",
            "70   50\n",
            "0.30000000000000004   120\n",
            "71   50\n",
            "0.30000000000000004   121\n",
            "72   51\n",
            "0.30000000000000004   122\n",
            "73   52\n",
            "0.30000000000000004   123\n",
            "74   53\n",
            "0.30000000000000004   124\n",
            "74   53\n",
            "0.30000000000000004   125\n",
            "75   54\n",
            "0.30000000000000004   126\n",
            "76   55\n",
            "0.30000000000000004   127\n",
            "77   56\n",
            "0.30000000000000004   128\n",
            "77   56\n",
            "0.30000000000000004   129\n",
            "78   57\n",
            "0.30000000000000004   130\n",
            "79   58\n",
            "0.30000000000000004   131\n",
            "79   58\n",
            "0.30000000000000004   132\n",
            "80   59\n",
            "0.30000000000000004   133\n",
            "81   60\n",
            "0.30000000000000004   134\n",
            "82   61\n",
            "0.30000000000000004   135\n",
            "83   61\n",
            "0.30000000000000004   136\n",
            "84   62\n",
            "0.30000000000000004   137\n",
            "85   62\n",
            "0.30000000000000004   138\n",
            "86   62\n",
            "0.30000000000000004   139\n",
            "87   62\n",
            "0.30000000000000004   140\n",
            "88   63\n",
            "0.30000000000000004   141\n",
            "89   63\n",
            "0.30000000000000004   142\n",
            "90   64\n",
            "0.30000000000000004   143\n",
            "91   65\n",
            "0.30000000000000004   144\n",
            "91   65\n",
            "0.30000000000000004   145\n",
            "92   66\n",
            "0.30000000000000004   146\n",
            "93   67\n",
            "0.30000000000000004   147\n",
            "93   67\n",
            "0.30000000000000004   148\n",
            "93   67\n",
            "0.30000000000000004   149\n",
            "94   67\n",
            "0.30000000000000004   150\n",
            "95   68\n",
            "0.30000000000000004   151\n",
            "96   69\n",
            "0.30000000000000004   152\n",
            "97   70\n",
            "0.30000000000000004   153\n",
            "98   71\n",
            "0.30000000000000004   154\n",
            "98   71\n",
            "0.30000000000000004   155\n",
            "99   72\n",
            "0.30000000000000004   156\n",
            "99   72\n",
            "0.30000000000000004   157\n",
            "100   73\n",
            "0.30000000000000004   158\n",
            "100   73\n",
            "0.30000000000000004   159\n",
            "101   74\n",
            "0.30000000000000004   160\n",
            "102   75\n",
            "0.30000000000000004   161\n",
            "102   75\n",
            "0.30000000000000004   162\n",
            "103   76\n",
            "0.30000000000000004   163\n",
            "104   76\n",
            "0.30000000000000004   164\n",
            "105   77\n",
            "0.30000000000000004   165\n",
            "106   78\n",
            "0.30000000000000004   166\n",
            "106   78\n",
            "0.30000000000000004   167\n",
            "107   78\n",
            "0.30000000000000004   168\n",
            "108   78\n",
            "0.30000000000000004   169\n",
            "109   79\n",
            "0.30000000000000004   170\n",
            "110   80\n",
            "0.30000000000000004   171\n",
            "110   80\n",
            "0.30000000000000004   172\n",
            "111   81\n",
            "0.30000000000000004   173\n",
            "112   82\n",
            "0.30000000000000004   174\n",
            "112   82\n",
            "0.30000000000000004   175\n",
            "113   83\n",
            "0.30000000000000004   176\n",
            "114   84\n",
            "0.30000000000000004   177\n",
            "115   85\n",
            "0.30000000000000004   178\n",
            "116   85\n",
            "0.30000000000000004   179\n",
            "117   85\n",
            "0.30000000000000004   180\n",
            "117   85\n",
            "0.30000000000000004   181\n",
            "118   86\n",
            "0.30000000000000004   182\n",
            "118   86\n",
            "0.30000000000000004   183\n",
            "119   87\n",
            "0.30000000000000004   184\n",
            "119   87\n",
            "0.30000000000000004   185\n",
            "120   88\n",
            "0.30000000000000004   186\n",
            "121   89\n",
            "0.30000000000000004   187\n",
            "121   89\n",
            "0.30000000000000004   188\n",
            "122   90\n",
            "0.30000000000000004   189\n",
            "123   91\n",
            "0.30000000000000004   190\n",
            "123   91\n",
            "0.30000000000000004   191\n",
            "123   91\n",
            "0.30000000000000004   192\n",
            "124   92\n",
            "0.30000000000000004   193\n",
            "124   92\n",
            "0.30000000000000004   194\n",
            "125   92\n",
            "0.30000000000000004   195\n",
            "126   93\n",
            "0.30000000000000004   196\n",
            "127   94\n",
            "0.30000000000000004   197\n",
            "128   95\n",
            "0.30000000000000004   198\n",
            "129   96\n",
            "0.30000000000000004   199\n",
            "130   97\n",
            "Epsilon value -  0.4\n",
            "\n",
            "0.4   0\n",
            "1   1\n",
            "0.4   1\n",
            "2   1\n",
            "0.4   2\n",
            "3   2\n",
            "0.4   3\n",
            "4   2\n",
            "0.4   4\n",
            "5   2\n",
            "0.4   5\n",
            "6   3\n",
            "0.4   6\n",
            "7   4\n",
            "0.4   7\n",
            "7   4\n",
            "0.4   8\n",
            "8   5\n",
            "0.4   9\n",
            "8   5\n",
            "0.4   10\n",
            "9   6\n",
            "0.4   11\n",
            "10   7\n",
            "0.4   12\n",
            "11   8\n",
            "0.4   13\n",
            "12   9\n",
            "0.4   14\n",
            "12   9\n",
            "0.4   15\n",
            "12   9\n",
            "0.4   16\n",
            "12   9\n",
            "0.4   17\n",
            "13   9\n",
            "0.4   18\n",
            "14   10\n",
            "0.4   19\n",
            "14   10\n",
            "0.4   20\n",
            "14   10\n",
            "0.4   21\n",
            "15   11\n",
            "0.4   22\n",
            "16   11\n",
            "0.4   23\n",
            "17   12\n",
            "0.4   24\n",
            "17   12\n",
            "0.4   25\n",
            "17   12\n",
            "0.4   26\n",
            "17   12\n",
            "0.4   27\n",
            "17   12\n",
            "0.4   28\n",
            "17   12\n",
            "0.4   29\n",
            "18   13\n",
            "0.4   30\n",
            "18   13\n",
            "0.4   31\n",
            "19   13\n",
            "0.4   32\n",
            "20   13\n",
            "0.4   33\n",
            "21   13\n",
            "0.4   34\n",
            "22   14\n",
            "0.4   35\n",
            "22   14\n",
            "0.4   36\n",
            "23   14\n",
            "0.4   37\n",
            "24   15\n",
            "0.4   38\n",
            "25   16\n",
            "0.4   39\n",
            "26   17\n",
            "0.4   40\n",
            "27   18\n",
            "0.4   41\n",
            "28   19\n",
            "0.4   42\n",
            "29   20\n",
            "0.4   43\n",
            "30   21\n",
            "0.4   44\n",
            "30   21\n",
            "0.4   45\n",
            "30   21\n",
            "0.4   46\n",
            "31   21\n",
            "0.4   47\n",
            "31   21\n",
            "0.4   48\n",
            "32   22\n",
            "0.4   49\n",
            "32   22\n",
            "0.4   50\n",
            "33   23\n",
            "0.4   51\n",
            "34   24\n",
            "0.4   52\n",
            "34   24\n",
            "0.4   53\n",
            "34   24\n",
            "0.4   54\n",
            "35   24\n",
            "0.4   55\n",
            "36   25\n",
            "0.4   56\n",
            "37   26\n",
            "0.4   57\n",
            "38   27\n",
            "0.4   58\n",
            "39   27\n",
            "0.4   59\n",
            "40   28\n",
            "0.4   60\n",
            "40   28\n",
            "0.4   61\n",
            "41   29\n",
            "0.4   62\n",
            "42   30\n",
            "0.4   63\n",
            "43   31\n",
            "0.4   64\n",
            "43   31\n",
            "0.4   65\n",
            "43   31\n",
            "0.4   66\n",
            "43   31\n",
            "0.4   67\n",
            "44   32\n",
            "0.4   68\n",
            "44   32\n",
            "0.4   69\n",
            "44   32\n",
            "0.4   70\n",
            "45   33\n",
            "0.4   71\n",
            "46   34\n",
            "0.4   72\n",
            "47   34\n",
            "0.4   73\n",
            "47   34\n",
            "0.4   74\n",
            "48   35\n",
            "0.4   75\n",
            "49   35\n",
            "0.4   76\n",
            "49   35\n",
            "0.4   77\n",
            "50   36\n",
            "0.4   78\n",
            "51   36\n",
            "0.4   79\n",
            "52   36\n",
            "0.4   80\n",
            "52   36\n",
            "0.4   81\n",
            "53   36\n",
            "0.4   82\n",
            "54   37\n",
            "0.4   83\n",
            "55   37\n",
            "0.4   84\n",
            "56   38\n",
            "0.4   85\n",
            "57   38\n",
            "0.4   86\n",
            "57   38\n",
            "0.4   87\n",
            "57   38\n",
            "0.4   88\n",
            "58   39\n",
            "0.4   89\n",
            "59   40\n",
            "0.4   90\n",
            "59   40\n",
            "0.4   91\n",
            "60   41\n",
            "0.4   92\n",
            "60   41\n",
            "0.4   93\n",
            "60   41\n",
            "0.4   94\n",
            "61   42\n",
            "0.4   95\n",
            "62   43\n",
            "0.4   96\n",
            "63   43\n",
            "0.4   97\n",
            "64   44\n",
            "0.4   98\n",
            "65   45\n",
            "0.4   99\n",
            "66   46\n",
            "0.4   100\n",
            "67   47\n",
            "0.4   101\n",
            "67   47\n",
            "0.4   102\n",
            "67   47\n",
            "0.4   103\n",
            "68   48\n",
            "0.4   104\n",
            "69   48\n",
            "0.4   105\n",
            "70   49\n",
            "0.4   106\n",
            "70   49\n",
            "0.4   107\n",
            "70   49\n",
            "0.4   108\n",
            "71   49\n",
            "0.4   109\n",
            "72   49\n",
            "0.4   110\n",
            "73   50\n",
            "0.4   111\n",
            "73   50\n",
            "0.4   112\n",
            "74   51\n",
            "0.4   113\n",
            "75   52\n",
            "0.4   114\n",
            "76   53\n",
            "0.4   115\n",
            "77   54\n",
            "0.4   116\n",
            "78   54\n",
            "0.4   117\n",
            "78   54\n",
            "0.4   118\n",
            "79   55\n",
            "0.4   119\n",
            "79   55\n",
            "0.4   120\n",
            "80   55\n",
            "0.4   121\n",
            "81   56\n",
            "0.4   122\n",
            "82   57\n",
            "0.4   123\n",
            "83   58\n",
            "0.4   124\n",
            "84   58\n",
            "0.4   125\n",
            "85   59\n",
            "0.4   126\n",
            "86   60\n",
            "0.4   127\n",
            "87   61\n",
            "0.4   128\n",
            "87   61\n",
            "0.4   129\n",
            "88   62\n",
            "0.4   130\n",
            "89   63\n",
            "0.4   131\n",
            "89   63\n",
            "0.4   132\n",
            "90   64\n",
            "0.4   133\n",
            "91   65\n",
            "0.4   134\n",
            "92   66\n",
            "0.4   135\n",
            "93   66\n",
            "0.4   136\n",
            "94   67\n",
            "0.4   137\n",
            "95   68\n",
            "0.4   138\n",
            "96   68\n",
            "0.4   139\n",
            "97   68\n",
            "0.4   140\n",
            "98   69\n",
            "0.4   141\n",
            "99   69\n",
            "0.4   142\n",
            "100   70\n",
            "0.4   143\n",
            "101   71\n",
            "0.4   144\n",
            "101   71\n",
            "0.4   145\n",
            "102   72\n",
            "0.4   146\n",
            "103   73\n",
            "0.4   147\n",
            "103   73\n",
            "0.4   148\n",
            "103   73\n",
            "0.4   149\n",
            "104   73\n",
            "0.4   150\n",
            "105   74\n",
            "0.4   151\n",
            "106   75\n",
            "0.4   152\n",
            "107   76\n",
            "0.4   153\n",
            "108   77\n",
            "0.4   154\n",
            "108   77\n",
            "0.4   155\n",
            "109   78\n",
            "0.4   156\n",
            "109   78\n",
            "0.4   157\n",
            "110   79\n",
            "0.4   158\n",
            "110   79\n",
            "0.4   159\n",
            "111   80\n",
            "0.4   160\n",
            "112   81\n",
            "0.4   161\n",
            "112   81\n",
            "0.4   162\n",
            "113   82\n",
            "0.4   163\n",
            "114   83\n",
            "0.4   164\n",
            "115   84\n",
            "0.4   165\n",
            "116   85\n",
            "0.4   166\n",
            "116   85\n",
            "0.4   167\n",
            "117   85\n",
            "0.4   168\n",
            "118   86\n",
            "0.4   169\n",
            "119   87\n",
            "0.4   170\n",
            "120   88\n",
            "0.4   171\n",
            "120   88\n",
            "0.4   172\n",
            "121   89\n",
            "0.4   173\n",
            "122   90\n",
            "0.4   174\n",
            "123   90\n",
            "0.4   175\n",
            "124   91\n",
            "0.4   176\n",
            "125   92\n",
            "0.4   177\n",
            "126   93\n",
            "0.4   178\n",
            "127   93\n",
            "0.4   179\n",
            "128   93\n",
            "0.4   180\n",
            "128   93\n",
            "0.4   181\n",
            "129   94\n",
            "0.4   182\n",
            "129   94\n",
            "0.4   183\n",
            "130   95\n",
            "0.4   184\n",
            "130   95\n",
            "0.4   185\n",
            "131   96\n",
            "0.4   186\n",
            "132   97\n",
            "0.4   187\n",
            "133   97\n",
            "0.4   188\n",
            "134   98\n",
            "0.4   189\n",
            "135   99\n",
            "0.4   190\n",
            "135   99\n",
            "0.4   191\n",
            "135   99\n",
            "0.4   192\n",
            "136   100\n",
            "0.4   193\n",
            "137   100\n",
            "0.4   194\n",
            "138   100\n",
            "0.4   195\n",
            "139   101\n",
            "0.4   196\n",
            "140   102\n",
            "0.4   197\n",
            "141   103\n",
            "0.4   198\n",
            "142   104\n",
            "0.4   199\n",
            "143   105\n",
            "Epsilon value -  0.5\n",
            "\n",
            "0.5   0\n",
            "1   1\n",
            "0.5   1\n",
            "2   1\n",
            "0.5   2\n",
            "3   2\n",
            "0.5   3\n",
            "4   2\n",
            "0.5   4\n",
            "5   2\n",
            "0.5   5\n",
            "6   3\n",
            "0.5   6\n",
            "7   4\n",
            "0.5   7\n",
            "7   4\n",
            "0.5   8\n",
            "8   5\n",
            "0.5   9\n",
            "8   5\n",
            "0.5   10\n",
            "9   6\n",
            "0.5   11\n",
            "10   7\n",
            "0.5   12\n",
            "11   8\n",
            "0.5   13\n",
            "12   9\n",
            "0.5   14\n",
            "12   9\n",
            "0.5   15\n",
            "12   9\n",
            "0.5   16\n",
            "12   9\n",
            "0.5   17\n",
            "13   9\n",
            "0.5   18\n",
            "14   10\n",
            "0.5   19\n",
            "14   10\n",
            "0.5   20\n",
            "14   10\n",
            "0.5   21\n",
            "15   11\n",
            "0.5   22\n",
            "16   11\n",
            "0.5   23\n",
            "17   12\n",
            "0.5   24\n",
            "17   12\n",
            "0.5   25\n",
            "17   12\n",
            "0.5   26\n",
            "17   12\n",
            "0.5   27\n",
            "17   12\n",
            "0.5   28\n",
            "17   12\n",
            "0.5   29\n",
            "18   13\n",
            "0.5   30\n",
            "18   13\n",
            "0.5   31\n",
            "19   13\n",
            "0.5   32\n",
            "20   14\n",
            "0.5   33\n",
            "21   14\n",
            "0.5   34\n",
            "22   15\n",
            "0.5   35\n",
            "22   15\n",
            "0.5   36\n",
            "23   15\n",
            "0.5   37\n",
            "24   16\n",
            "0.5   38\n",
            "25   17\n",
            "0.5   39\n",
            "26   18\n",
            "0.5   40\n",
            "27   19\n",
            "0.5   41\n",
            "28   20\n",
            "0.5   42\n",
            "29   21\n",
            "0.5   43\n",
            "30   22\n",
            "0.5   44\n",
            "30   22\n",
            "0.5   45\n",
            "30   22\n",
            "0.5   46\n",
            "31   22\n",
            "0.5   47\n",
            "31   22\n",
            "0.5   48\n",
            "32   23\n",
            "0.5   49\n",
            "32   23\n",
            "0.5   50\n",
            "33   24\n",
            "0.5   51\n",
            "34   25\n",
            "0.5   52\n",
            "34   25\n",
            "0.5   53\n",
            "34   25\n",
            "0.5   54\n",
            "35   26\n",
            "0.5   55\n",
            "36   27\n",
            "0.5   56\n",
            "37   28\n",
            "0.5   57\n",
            "38   29\n",
            "0.5   58\n",
            "39   29\n",
            "0.5   59\n",
            "40   30\n",
            "0.5   60\n",
            "40   30\n",
            "0.5   61\n",
            "41   31\n",
            "0.5   62\n",
            "42   32\n",
            "0.5   63\n",
            "43   33\n",
            "0.5   64\n",
            "43   33\n",
            "0.5   65\n",
            "43   33\n",
            "0.5   66\n",
            "43   33\n",
            "0.5   67\n",
            "44   34\n",
            "0.5   68\n",
            "44   34\n",
            "0.5   69\n",
            "44   34\n",
            "0.5   70\n",
            "45   35\n",
            "0.5   71\n",
            "46   36\n",
            "0.5   72\n",
            "47   36\n",
            "0.5   73\n",
            "47   36\n",
            "0.5   74\n",
            "48   37\n",
            "0.5   75\n",
            "49   37\n",
            "0.5   76\n",
            "49   37\n",
            "0.5   77\n",
            "50   38\n",
            "0.5   78\n",
            "51   39\n",
            "0.5   79\n",
            "52   39\n",
            "0.5   80\n",
            "52   39\n",
            "0.5   81\n",
            "53   39\n",
            "0.5   82\n",
            "54   40\n",
            "0.5   83\n",
            "55   40\n",
            "0.5   84\n",
            "56   41\n",
            "0.5   85\n",
            "57   41\n",
            "0.5   86\n",
            "57   41\n",
            "0.5   87\n",
            "57   41\n",
            "0.5   88\n",
            "58   42\n",
            "0.5   89\n",
            "59   43\n",
            "0.5   90\n",
            "59   43\n",
            "0.5   91\n",
            "60   44\n",
            "0.5   92\n",
            "60   44\n",
            "0.5   93\n",
            "60   44\n",
            "0.5   94\n",
            "61   45\n",
            "0.5   95\n",
            "62   46\n",
            "0.5   96\n",
            "63   47\n",
            "0.5   97\n",
            "64   48\n",
            "0.5   98\n",
            "65   49\n",
            "0.5   99\n",
            "66   50\n",
            "0.5   100\n",
            "67   51\n",
            "0.5   101\n",
            "67   51\n",
            "0.5   102\n",
            "67   51\n",
            "0.5   103\n",
            "68   52\n",
            "0.5   104\n",
            "69   52\n",
            "0.5   105\n",
            "70   53\n",
            "0.5   106\n",
            "70   53\n",
            "0.5   107\n",
            "70   53\n",
            "0.5   108\n",
            "71   53\n",
            "0.5   109\n",
            "72   53\n",
            "0.5   110\n",
            "73   54\n",
            "0.5   111\n",
            "73   54\n",
            "0.5   112\n",
            "74   55\n",
            "0.5   113\n",
            "75   56\n",
            "0.5   114\n",
            "76   57\n",
            "0.5   115\n",
            "77   58\n",
            "0.5   116\n",
            "78   58\n",
            "0.5   117\n",
            "78   58\n",
            "0.5   118\n",
            "79   59\n",
            "0.5   119\n",
            "79   59\n",
            "0.5   120\n",
            "80   59\n",
            "0.5   121\n",
            "81   60\n",
            "0.5   122\n",
            "82   61\n",
            "0.5   123\n",
            "83   62\n",
            "0.5   124\n",
            "84   62\n",
            "0.5   125\n",
            "85   63\n",
            "0.5   126\n",
            "86   64\n",
            "0.5   127\n",
            "87   64\n",
            "0.5   128\n",
            "87   64\n",
            "0.5   129\n",
            "88   65\n",
            "0.5   130\n",
            "89   66\n",
            "0.5   131\n",
            "89   66\n",
            "0.5   132\n",
            "90   67\n",
            "0.5   133\n",
            "91   68\n",
            "0.5   134\n",
            "92   69\n",
            "0.5   135\n",
            "93   69\n",
            "0.5   136\n",
            "94   70\n",
            "0.5   137\n",
            "95   71\n",
            "0.5   138\n",
            "96   72\n",
            "0.5   139\n",
            "97   73\n",
            "0.5   140\n",
            "98   74\n",
            "0.5   141\n",
            "99   75\n",
            "0.5   142\n",
            "100   76\n",
            "0.5   143\n",
            "101   77\n",
            "0.5   144\n",
            "101   77\n",
            "0.5   145\n",
            "102   78\n",
            "0.5   146\n",
            "103   79\n",
            "0.5   147\n",
            "103   79\n",
            "0.5   148\n",
            "103   79\n",
            "0.5   149\n",
            "104   79\n",
            "0.5   150\n",
            "105   80\n",
            "0.5   151\n",
            "106   81\n",
            "0.5   152\n",
            "107   82\n",
            "0.5   153\n",
            "108   83\n",
            "0.5   154\n",
            "108   83\n",
            "0.5   155\n",
            "109   84\n",
            "0.5   156\n",
            "109   84\n",
            "0.5   157\n",
            "110   85\n",
            "0.5   158\n",
            "110   85\n",
            "0.5   159\n",
            "111   86\n",
            "0.5   160\n",
            "112   87\n",
            "0.5   161\n",
            "112   87\n",
            "0.5   162\n",
            "113   88\n",
            "0.5   163\n",
            "114   89\n",
            "0.5   164\n",
            "115   90\n",
            "0.5   165\n",
            "116   91\n",
            "0.5   166\n",
            "116   91\n",
            "0.5   167\n",
            "117   91\n",
            "0.5   168\n",
            "118   92\n",
            "0.5   169\n",
            "119   93\n",
            "0.5   170\n",
            "120   94\n",
            "0.5   171\n",
            "120   94\n",
            "0.5   172\n",
            "121   95\n",
            "0.5   173\n",
            "122   96\n",
            "0.5   174\n",
            "123   96\n",
            "0.5   175\n",
            "124   97\n",
            "0.5   176\n",
            "125   98\n",
            "0.5   177\n",
            "126   99\n",
            "0.5   178\n",
            "127   99\n",
            "0.5   179\n",
            "128   99\n",
            "0.5   180\n",
            "128   99\n",
            "0.5   181\n",
            "129   100\n",
            "0.5   182\n",
            "129   100\n",
            "0.5   183\n",
            "130   101\n",
            "0.5   184\n",
            "130   101\n",
            "0.5   185\n",
            "131   102\n",
            "0.5   186\n",
            "132   103\n",
            "0.5   187\n",
            "133   103\n",
            "0.5   188\n",
            "134   104\n",
            "0.5   189\n",
            "135   105\n",
            "0.5   190\n",
            "135   105\n",
            "0.5   191\n",
            "135   105\n",
            "0.5   192\n",
            "136   106\n",
            "0.5   193\n",
            "137   106\n",
            "0.5   194\n",
            "138   106\n",
            "0.5   195\n",
            "139   107\n",
            "0.5   196\n",
            "140   108\n",
            "0.5   197\n",
            "141   109\n",
            "0.5   198\n",
            "142   110\n",
            "0.5   199\n",
            "143   111\n",
            "Epsilon value -  0.6\n",
            "\n",
            "0.6   0\n",
            "1   1\n",
            "0.6   1\n",
            "2   1\n",
            "0.6   2\n",
            "3   2\n",
            "0.6   3\n",
            "4   3\n",
            "0.6   4\n",
            "5   3\n",
            "0.6   5\n",
            "6   4\n",
            "0.6   6\n",
            "7   5\n",
            "0.6   7\n",
            "7   5\n",
            "0.6   8\n",
            "8   6\n",
            "0.6   9\n",
            "8   6\n",
            "0.6   10\n",
            "9   7\n",
            "0.6   11\n",
            "10   8\n",
            "0.6   12\n",
            "11   9\n",
            "0.6   13\n",
            "12   10\n",
            "0.6   14\n",
            "12   10\n",
            "0.6   15\n",
            "12   10\n",
            "0.6   16\n",
            "12   10\n",
            "0.6   17\n",
            "13   11\n",
            "0.6   18\n",
            "14   12\n",
            "0.6   19\n",
            "14   12\n",
            "0.6   20\n",
            "14   12\n",
            "0.6   21\n",
            "15   13\n",
            "0.6   22\n",
            "16   13\n",
            "0.6   23\n",
            "17   14\n",
            "0.6   24\n",
            "17   14\n",
            "0.6   25\n",
            "17   14\n",
            "0.6   26\n",
            "17   14\n",
            "0.6   27\n",
            "17   14\n",
            "0.6   28\n",
            "17   14\n",
            "0.6   29\n",
            "18   15\n",
            "0.6   30\n",
            "18   15\n",
            "0.6   31\n",
            "19   16\n",
            "0.6   32\n",
            "20   17\n",
            "0.6   33\n",
            "21   17\n",
            "0.6   34\n",
            "22   18\n",
            "0.6   35\n",
            "22   18\n",
            "0.6   36\n",
            "23   18\n",
            "0.6   37\n",
            "24   19\n",
            "0.6   38\n",
            "25   20\n",
            "0.6   39\n",
            "26   21\n",
            "0.6   40\n",
            "27   22\n",
            "0.6   41\n",
            "28   23\n",
            "0.6   42\n",
            "29   24\n",
            "0.6   43\n",
            "30   25\n",
            "0.6   44\n",
            "30   25\n",
            "0.6   45\n",
            "30   25\n",
            "0.6   46\n",
            "31   26\n",
            "0.6   47\n",
            "31   26\n",
            "0.6   48\n",
            "32   27\n",
            "0.6   49\n",
            "32   27\n",
            "0.6   50\n",
            "33   28\n",
            "0.6   51\n",
            "34   29\n",
            "0.6   52\n",
            "34   29\n",
            "0.6   53\n",
            "34   29\n",
            "0.6   54\n",
            "35   30\n",
            "0.6   55\n",
            "36   31\n",
            "0.6   56\n",
            "37   32\n",
            "0.6   57\n",
            "38   33\n",
            "0.6   58\n",
            "39   33\n",
            "0.6   59\n",
            "40   34\n",
            "0.6   60\n",
            "40   34\n",
            "0.6   61\n",
            "41   35\n",
            "0.6   62\n",
            "42   36\n",
            "0.6   63\n",
            "43   37\n",
            "0.6   64\n",
            "43   37\n",
            "0.6   65\n",
            "43   37\n",
            "0.6   66\n",
            "43   37\n",
            "0.6   67\n",
            "44   38\n",
            "0.6   68\n",
            "44   38\n",
            "0.6   69\n",
            "44   39\n",
            "0.6   70\n",
            "45   40\n",
            "0.6   71\n",
            "46   41\n",
            "0.6   72\n",
            "47   41\n",
            "0.6   73\n",
            "47   41\n",
            "0.6   74\n",
            "48   42\n",
            "0.6   75\n",
            "49   42\n",
            "0.6   76\n",
            "49   42\n",
            "0.6   77\n",
            "50   42\n",
            "0.6   78\n",
            "51   43\n",
            "0.6   79\n",
            "52   43\n",
            "0.6   80\n",
            "52   43\n",
            "0.6   81\n",
            "53   43\n",
            "0.6   82\n",
            "54   44\n",
            "0.6   83\n",
            "55   45\n",
            "0.6   84\n",
            "56   46\n",
            "0.6   85\n",
            "57   47\n",
            "0.6   86\n",
            "57   47\n",
            "0.6   87\n",
            "57   47\n",
            "0.6   88\n",
            "58   48\n",
            "0.6   89\n",
            "59   49\n",
            "0.6   90\n",
            "59   49\n",
            "0.6   91\n",
            "60   50\n",
            "0.6   92\n",
            "60   50\n",
            "0.6   93\n",
            "60   50\n",
            "0.6   94\n",
            "61   51\n",
            "0.6   95\n",
            "62   52\n",
            "0.6   96\n",
            "63   53\n",
            "0.6   97\n",
            "64   54\n",
            "0.6   98\n",
            "65   55\n",
            "0.6   99\n",
            "66   56\n",
            "0.6   100\n",
            "67   57\n",
            "0.6   101\n",
            "67   57\n",
            "0.6   102\n",
            "67   57\n",
            "0.6   103\n",
            "68   58\n",
            "0.6   104\n",
            "69   59\n",
            "0.6   105\n",
            "70   60\n",
            "0.6   106\n",
            "70   60\n",
            "0.6   107\n",
            "70   60\n",
            "0.6   108\n",
            "71   61\n",
            "0.6   109\n",
            "72   61\n",
            "0.6   110\n",
            "73   62\n",
            "0.6   111\n",
            "73   62\n",
            "0.6   112\n",
            "74   63\n",
            "0.6   113\n",
            "75   64\n",
            "0.6   114\n",
            "76   65\n",
            "0.6   115\n",
            "77   66\n",
            "0.6   116\n",
            "78   67\n",
            "0.6   117\n",
            "78   67\n",
            "0.6   118\n",
            "79   68\n",
            "0.6   119\n",
            "79   68\n",
            "0.6   120\n",
            "80   68\n",
            "0.6   121\n",
            "81   69\n",
            "0.6   122\n",
            "82   70\n",
            "0.6   123\n",
            "83   71\n",
            "0.6   124\n",
            "84   71\n",
            "0.6   125\n",
            "85   72\n",
            "0.6   126\n",
            "86   73\n",
            "0.6   127\n",
            "87   73\n",
            "0.6   128\n",
            "87   73\n",
            "0.6   129\n",
            "88   74\n",
            "0.6   130\n",
            "89   75\n",
            "0.6   131\n",
            "89   75\n",
            "0.6   132\n",
            "90   76\n",
            "0.6   133\n",
            "91   77\n",
            "0.6   134\n",
            "92   78\n",
            "0.6   135\n",
            "93   79\n",
            "0.6   136\n",
            "94   80\n",
            "0.6   137\n",
            "95   81\n",
            "0.6   138\n",
            "96   82\n",
            "0.6   139\n",
            "97   83\n",
            "0.6   140\n",
            "98   84\n",
            "0.6   141\n",
            "99   85\n",
            "0.6   142\n",
            "100   86\n",
            "0.6   143\n",
            "101   87\n",
            "0.6   144\n",
            "101   87\n",
            "0.6   145\n",
            "102   88\n",
            "0.6   146\n",
            "103   89\n",
            "0.6   147\n",
            "103   89\n",
            "0.6   148\n",
            "103   89\n",
            "0.6   149\n",
            "104   89\n",
            "0.6   150\n",
            "105   90\n",
            "0.6   151\n",
            "106   91\n",
            "0.6   152\n",
            "107   92\n",
            "0.6   153\n",
            "108   93\n",
            "0.6   154\n",
            "108   93\n",
            "0.6   155\n",
            "109   94\n",
            "0.6   156\n",
            "109   94\n",
            "0.6   157\n",
            "110   95\n",
            "0.6   158\n",
            "110   95\n",
            "0.6   159\n",
            "111   96\n",
            "0.6   160\n",
            "112   97\n",
            "0.6   161\n",
            "112   97\n",
            "0.6   162\n",
            "113   98\n",
            "0.6   163\n",
            "114   99\n",
            "0.6   164\n",
            "115   100\n",
            "0.6   165\n",
            "116   101\n",
            "0.6   166\n",
            "116   101\n",
            "0.6   167\n",
            "117   101\n",
            "0.6   168\n",
            "118   102\n",
            "0.6   169\n",
            "119   103\n",
            "0.6   170\n",
            "120   104\n",
            "0.6   171\n",
            "120   104\n",
            "0.6   172\n",
            "121   105\n",
            "0.6   173\n",
            "122   106\n",
            "0.6   174\n",
            "123   106\n",
            "0.6   175\n",
            "124   107\n",
            "0.6   176\n",
            "125   108\n",
            "0.6   177\n",
            "126   109\n",
            "0.6   178\n",
            "127   109\n",
            "0.6   179\n",
            "128   109\n",
            "0.6   180\n",
            "128   109\n",
            "0.6   181\n",
            "129   110\n",
            "0.6   182\n",
            "129   110\n",
            "0.6   183\n",
            "130   111\n",
            "0.6   184\n",
            "130   111\n",
            "0.6   185\n",
            "131   112\n",
            "0.6   186\n",
            "132   113\n",
            "0.6   187\n",
            "133   113\n",
            "0.6   188\n",
            "134   114\n",
            "0.6   189\n",
            "135   115\n",
            "0.6   190\n",
            "135   115\n",
            "0.6   191\n",
            "135   115\n",
            "0.6   192\n",
            "136   116\n",
            "0.6   193\n",
            "137   117\n",
            "0.6   194\n",
            "138   117\n",
            "0.6   195\n",
            "139   118\n",
            "0.6   196\n",
            "140   119\n",
            "0.6   197\n",
            "141   120\n",
            "0.6   198\n",
            "142   121\n",
            "0.6   199\n",
            "143   122\n",
            "Epsilon value -  0.7\n",
            "\n",
            "0.7   0\n",
            "1   1\n",
            "0.7   1\n",
            "2   1\n",
            "0.7   2\n",
            "3   2\n",
            "0.7   3\n",
            "4   3\n",
            "0.7   4\n",
            "5   3\n",
            "0.7   5\n",
            "6   4\n",
            "0.7   6\n",
            "7   5\n",
            "0.7   7\n",
            "7   5\n",
            "0.7   8\n",
            "8   6\n",
            "0.7   9\n",
            "8   6\n",
            "0.7   10\n",
            "9   7\n",
            "0.7   11\n",
            "10   8\n",
            "0.7   12\n",
            "11   9\n",
            "0.7   13\n",
            "12   10\n",
            "0.7   14\n",
            "12   10\n",
            "0.7   15\n",
            "12   10\n",
            "0.7   16\n",
            "12   10\n",
            "0.7   17\n",
            "13   11\n",
            "0.7   18\n",
            "14   12\n",
            "0.7   19\n",
            "14   12\n",
            "0.7   20\n",
            "14   13\n",
            "0.7   21\n",
            "15   14\n",
            "0.7   22\n",
            "16   15\n",
            "0.7   23\n",
            "17   16\n",
            "0.7   24\n",
            "17   16\n",
            "0.7   25\n",
            "17   16\n",
            "0.7   26\n",
            "17   16\n",
            "0.7   27\n",
            "17   16\n",
            "0.7   28\n",
            "17   16\n",
            "0.7   29\n",
            "18   17\n",
            "0.7   30\n",
            "18   17\n",
            "0.7   31\n",
            "19   18\n",
            "0.7   32\n",
            "20   19\n",
            "0.7   33\n",
            "21   19\n",
            "0.7   34\n",
            "22   20\n",
            "0.7   35\n",
            "22   20\n",
            "0.7   36\n",
            "23   20\n",
            "0.7   37\n",
            "24   21\n",
            "0.7   38\n",
            "25   22\n",
            "0.7   39\n",
            "26   23\n",
            "0.7   40\n",
            "27   24\n",
            "0.7   41\n",
            "28   25\n",
            "0.7   42\n",
            "29   26\n",
            "0.7   43\n",
            "30   27\n",
            "0.7   44\n",
            "30   27\n",
            "0.7   45\n",
            "30   27\n",
            "0.7   46\n",
            "31   28\n",
            "0.7   47\n",
            "31   28\n",
            "0.7   48\n",
            "32   29\n",
            "0.7   49\n",
            "32   29\n",
            "0.7   50\n",
            "33   30\n",
            "0.7   51\n",
            "34   31\n",
            "0.7   52\n",
            "34   31\n",
            "0.7   53\n",
            "34   31\n",
            "0.7   54\n",
            "35   32\n",
            "0.7   55\n",
            "36   33\n",
            "0.7   56\n",
            "37   34\n",
            "0.7   57\n",
            "38   35\n",
            "0.7   58\n",
            "39   35\n",
            "0.7   59\n",
            "40   36\n",
            "0.7   60\n",
            "40   36\n",
            "0.7   61\n",
            "41   37\n",
            "0.7   62\n",
            "42   38\n",
            "0.7   63\n",
            "43   39\n",
            "0.7   64\n",
            "43   39\n",
            "0.7   65\n",
            "43   40\n",
            "0.7   66\n",
            "43   40\n",
            "0.7   67\n",
            "44   41\n",
            "0.7   68\n",
            "44   41\n",
            "0.7   69\n",
            "44   42\n",
            "0.7   70\n",
            "45   43\n",
            "0.7   71\n",
            "46   44\n",
            "0.7   72\n",
            "47   45\n",
            "0.7   73\n",
            "47   45\n",
            "0.7   74\n",
            "48   46\n",
            "0.7   75\n",
            "49   46\n",
            "0.7   76\n",
            "49   46\n",
            "0.7   77\n",
            "50   46\n",
            "0.7   78\n",
            "51   47\n",
            "0.7   79\n",
            "52   48\n",
            "0.7   80\n",
            "52   48\n",
            "0.7   81\n",
            "53   48\n",
            "0.7   82\n",
            "54   49\n",
            "0.7   83\n",
            "55   50\n",
            "0.7   84\n",
            "56   51\n",
            "0.7   85\n",
            "57   52\n",
            "0.7   86\n",
            "57   52\n",
            "0.7   87\n",
            "57   52\n",
            "0.7   88\n",
            "58   53\n",
            "0.7   89\n",
            "59   54\n",
            "0.7   90\n",
            "59   54\n",
            "0.7   91\n",
            "60   55\n",
            "0.7   92\n",
            "60   55\n",
            "0.7   93\n",
            "60   55\n",
            "0.7   94\n",
            "61   56\n",
            "0.7   95\n",
            "62   57\n",
            "0.7   96\n",
            "63   58\n",
            "0.7   97\n",
            "64   59\n",
            "0.7   98\n",
            "65   60\n",
            "0.7   99\n",
            "66   61\n",
            "0.7   100\n",
            "67   62\n",
            "0.7   101\n",
            "67   62\n",
            "0.7   102\n",
            "67   62\n",
            "0.7   103\n",
            "68   63\n",
            "0.7   104\n",
            "69   64\n",
            "0.7   105\n",
            "70   65\n",
            "0.7   106\n",
            "70   65\n",
            "0.7   107\n",
            "70   66\n",
            "0.7   108\n",
            "71   67\n",
            "0.7   109\n",
            "72   67\n",
            "0.7   110\n",
            "73   68\n",
            "0.7   111\n",
            "73   68\n",
            "0.7   112\n",
            "74   69\n",
            "0.7   113\n",
            "75   70\n",
            "0.7   114\n",
            "76   71\n",
            "0.7   115\n",
            "77   72\n",
            "0.7   116\n",
            "78   73\n",
            "0.7   117\n",
            "78   73\n",
            "0.7   118\n",
            "79   74\n",
            "0.7   119\n",
            "79   74\n",
            "0.7   120\n",
            "80   74\n",
            "0.7   121\n",
            "81   75\n",
            "0.7   122\n",
            "82   76\n",
            "0.7   123\n",
            "83   77\n",
            "0.7   124\n",
            "84   77\n",
            "0.7   125\n",
            "85   78\n",
            "0.7   126\n",
            "86   79\n",
            "0.7   127\n",
            "87   79\n",
            "0.7   128\n",
            "87   79\n",
            "0.7   129\n",
            "88   80\n",
            "0.7   130\n",
            "89   81\n",
            "0.7   131\n",
            "89   81\n",
            "0.7   132\n",
            "90   82\n",
            "0.7   133\n",
            "91   83\n",
            "0.7   134\n",
            "92   84\n",
            "0.7   135\n",
            "93   85\n",
            "0.7   136\n",
            "94   86\n",
            "0.7   137\n",
            "95   87\n",
            "0.7   138\n",
            "96   88\n",
            "0.7   139\n",
            "97   89\n",
            "0.7   140\n",
            "98   90\n",
            "0.7   141\n",
            "99   91\n",
            "0.7   142\n",
            "100   92\n",
            "0.7   143\n",
            "101   93\n",
            "0.7   144\n",
            "101   93\n",
            "0.7   145\n",
            "102   94\n",
            "0.7   146\n",
            "103   95\n",
            "0.7   147\n",
            "103   95\n",
            "0.7   148\n",
            "103   95\n",
            "0.7   149\n",
            "104   95\n",
            "0.7   150\n",
            "105   96\n",
            "0.7   151\n",
            "106   97\n",
            "0.7   152\n",
            "107   98\n",
            "0.7   153\n",
            "108   99\n",
            "0.7   154\n",
            "108   99\n",
            "0.7   155\n",
            "109   100\n",
            "0.7   156\n",
            "109   100\n",
            "0.7   157\n",
            "110   101\n",
            "0.7   158\n",
            "110   101\n",
            "0.7   159\n",
            "111   102\n",
            "0.7   160\n",
            "112   103\n",
            "0.7   161\n",
            "112   103\n",
            "0.7   162\n",
            "113   104\n",
            "0.7   163\n",
            "114   105\n",
            "0.7   164\n",
            "115   106\n",
            "0.7   165\n",
            "116   107\n",
            "0.7   166\n",
            "116   107\n",
            "0.7   167\n",
            "117   107\n",
            "0.7   168\n",
            "118   108\n",
            "0.7   169\n",
            "119   109\n",
            "0.7   170\n",
            "120   110\n",
            "0.7   171\n",
            "120   110\n",
            "0.7   172\n",
            "121   111\n",
            "0.7   173\n",
            "122   112\n",
            "0.7   174\n",
            "123   113\n",
            "0.7   175\n",
            "124   114\n",
            "0.7   176\n",
            "125   115\n",
            "0.7   177\n",
            "126   116\n",
            "0.7   178\n",
            "127   116\n",
            "0.7   179\n",
            "128   117\n",
            "0.7   180\n",
            "128   117\n",
            "0.7   181\n",
            "129   118\n",
            "0.7   182\n",
            "129   118\n",
            "0.7   183\n",
            "130   119\n",
            "0.7   184\n",
            "130   119\n",
            "0.7   185\n",
            "131   120\n",
            "0.7   186\n",
            "132   121\n",
            "0.7   187\n",
            "133   121\n",
            "0.7   188\n",
            "134   122\n",
            "0.7   189\n",
            "135   123\n",
            "0.7   190\n",
            "135   123\n",
            "0.7   191\n",
            "135   123\n",
            "0.7   192\n",
            "136   124\n",
            "0.7   193\n",
            "137   125\n",
            "0.7   194\n",
            "138   125\n",
            "0.7   195\n",
            "139   126\n",
            "0.7   196\n",
            "140   127\n",
            "0.7   197\n",
            "141   128\n",
            "0.7   198\n",
            "142   129\n",
            "0.7   199\n",
            "143   130\n",
            "Epsilon value -  0.7999999999999999\n",
            "\n",
            "0.7999999999999999   0\n",
            "1   1\n",
            "0.7999999999999999   1\n",
            "2   1\n",
            "0.7999999999999999   2\n",
            "3   2\n",
            "0.7999999999999999   3\n",
            "4   3\n",
            "0.7999999999999999   4\n",
            "5   3\n",
            "0.7999999999999999   5\n",
            "6   4\n",
            "0.7999999999999999   6\n",
            "7   5\n",
            "0.7999999999999999   7\n",
            "7   5\n",
            "0.7999999999999999   8\n",
            "8   6\n",
            "0.7999999999999999   9\n",
            "8   7\n",
            "0.7999999999999999   10\n",
            "9   8\n",
            "0.7999999999999999   11\n",
            "10   9\n",
            "0.7999999999999999   12\n",
            "11   10\n",
            "0.7999999999999999   13\n",
            "12   11\n",
            "0.7999999999999999   14\n",
            "12   11\n",
            "0.7999999999999999   15\n",
            "12   11\n",
            "0.7999999999999999   16\n",
            "12   11\n",
            "0.7999999999999999   17\n",
            "13   12\n",
            "0.7999999999999999   18\n",
            "14   13\n",
            "0.7999999999999999   19\n",
            "14   13\n",
            "0.7999999999999999   20\n",
            "14   14\n",
            "0.7999999999999999   21\n",
            "15   15\n",
            "0.7999999999999999   22\n",
            "16   16\n",
            "0.7999999999999999   23\n",
            "17   17\n",
            "0.7999999999999999   24\n",
            "17   17\n",
            "0.7999999999999999   25\n",
            "17   17\n",
            "0.7999999999999999   26\n",
            "17   17\n",
            "0.7999999999999999   27\n",
            "17   17\n",
            "0.7999999999999999   28\n",
            "17   17\n",
            "0.7999999999999999   29\n",
            "18   18\n",
            "0.7999999999999999   30\n",
            "18   18\n",
            "0.7999999999999999   31\n",
            "19   19\n",
            "0.7999999999999999   32\n",
            "20   20\n",
            "0.7999999999999999   33\n",
            "21   20\n",
            "0.7999999999999999   34\n",
            "22   21\n",
            "0.7999999999999999   35\n",
            "22   21\n",
            "0.7999999999999999   36\n",
            "23   21\n",
            "0.7999999999999999   37\n",
            "24   22\n",
            "0.7999999999999999   38\n",
            "25   23\n",
            "0.7999999999999999   39\n",
            "26   24\n",
            "0.7999999999999999   40\n",
            "27   25\n",
            "0.7999999999999999   41\n",
            "28   26\n",
            "0.7999999999999999   42\n",
            "29   27\n",
            "0.7999999999999999   43\n",
            "30   28\n",
            "0.7999999999999999   44\n",
            "30   28\n",
            "0.7999999999999999   45\n",
            "30   29\n",
            "0.7999999999999999   46\n",
            "31   30\n",
            "0.7999999999999999   47\n",
            "31   30\n",
            "0.7999999999999999   48\n",
            "32   31\n",
            "0.7999999999999999   49\n",
            "32   31\n",
            "0.7999999999999999   50\n",
            "33   32\n",
            "0.7999999999999999   51\n",
            "34   33\n",
            "0.7999999999999999   52\n",
            "34   33\n",
            "0.7999999999999999   53\n",
            "34   33\n",
            "0.7999999999999999   54\n",
            "35   34\n",
            "0.7999999999999999   55\n",
            "36   35\n",
            "0.7999999999999999   56\n",
            "37   36\n",
            "0.7999999999999999   57\n",
            "38   37\n",
            "0.7999999999999999   58\n",
            "39   37\n",
            "0.7999999999999999   59\n",
            "40   38\n",
            "0.7999999999999999   60\n",
            "40   38\n",
            "0.7999999999999999   61\n",
            "41   39\n",
            "0.7999999999999999   62\n",
            "42   40\n",
            "0.7999999999999999   63\n",
            "43   41\n",
            "0.7999999999999999   64\n",
            "43   41\n",
            "0.7999999999999999   65\n",
            "43   42\n",
            "0.7999999999999999   66\n",
            "43   43\n",
            "0.7999999999999999   67\n",
            "44   44\n",
            "0.7999999999999999   68\n",
            "44   45\n",
            "0.7999999999999999   69\n",
            "44   46\n",
            "0.7999999999999999   70\n",
            "45   47\n",
            "0.7999999999999999   71\n",
            "46   48\n",
            "0.7999999999999999   72\n",
            "47   49\n",
            "0.7999999999999999   73\n",
            "47   49\n",
            "0.7999999999999999   74\n",
            "48   50\n",
            "0.7999999999999999   75\n",
            "49   50\n",
            "0.7999999999999999   76\n",
            "49   50\n",
            "0.7999999999999999   77\n",
            "50   50\n",
            "0.7999999999999999   78\n",
            "51   51\n",
            "0.7999999999999999   79\n",
            "52   52\n",
            "0.7999999999999999   80\n",
            "52   52\n",
            "0.7999999999999999   81\n",
            "53   52\n",
            "0.7999999999999999   82\n",
            "54   53\n",
            "0.7999999999999999   83\n",
            "55   54\n",
            "0.7999999999999999   84\n",
            "56   55\n",
            "0.7999999999999999   85\n",
            "57   56\n",
            "0.7999999999999999   86\n",
            "57   56\n",
            "0.7999999999999999   87\n",
            "57   56\n",
            "0.7999999999999999   88\n",
            "58   57\n",
            "0.7999999999999999   89\n",
            "59   58\n",
            "0.7999999999999999   90\n",
            "59   58\n",
            "0.7999999999999999   91\n",
            "60   59\n",
            "0.7999999999999999   92\n",
            "60   59\n",
            "0.7999999999999999   93\n",
            "60   59\n",
            "0.7999999999999999   94\n",
            "61   60\n",
            "0.7999999999999999   95\n",
            "62   61\n",
            "0.7999999999999999   96\n",
            "63   62\n",
            "0.7999999999999999   97\n",
            "64   63\n",
            "0.7999999999999999   98\n",
            "65   64\n",
            "0.7999999999999999   99\n",
            "66   65\n",
            "0.7999999999999999   100\n",
            "67   66\n",
            "0.7999999999999999   101\n",
            "67   66\n",
            "0.7999999999999999   102\n",
            "67   66\n",
            "0.7999999999999999   103\n",
            "68   67\n",
            "0.7999999999999999   104\n",
            "69   68\n",
            "0.7999999999999999   105\n",
            "70   69\n",
            "0.7999999999999999   106\n",
            "70   69\n",
            "0.7999999999999999   107\n",
            "70   70\n",
            "0.7999999999999999   108\n",
            "71   71\n",
            "0.7999999999999999   109\n",
            "72   71\n",
            "0.7999999999999999   110\n",
            "73   72\n",
            "0.7999999999999999   111\n",
            "73   72\n",
            "0.7999999999999999   112\n",
            "74   73\n",
            "0.7999999999999999   113\n",
            "75   74\n",
            "0.7999999999999999   114\n",
            "76   75\n",
            "0.7999999999999999   115\n",
            "77   76\n",
            "0.7999999999999999   116\n",
            "78   77\n",
            "0.7999999999999999   117\n",
            "78   77\n",
            "0.7999999999999999   118\n",
            "79   78\n",
            "0.7999999999999999   119\n",
            "79   78\n",
            "0.7999999999999999   120\n",
            "80   78\n",
            "0.7999999999999999   121\n",
            "81   79\n",
            "0.7999999999999999   122\n",
            "82   80\n",
            "0.7999999999999999   123\n",
            "83   81\n",
            "0.7999999999999999   124\n",
            "84   82\n",
            "0.7999999999999999   125\n",
            "85   83\n",
            "0.7999999999999999   126\n",
            "86   84\n",
            "0.7999999999999999   127\n",
            "87   84\n",
            "0.7999999999999999   128\n",
            "87   84\n",
            "0.7999999999999999   129\n",
            "88   85\n",
            "0.7999999999999999   130\n",
            "89   86\n",
            "0.7999999999999999   131\n",
            "89   86\n",
            "0.7999999999999999   132\n",
            "90   87\n",
            "0.7999999999999999   133\n",
            "91   88\n",
            "0.7999999999999999   134\n",
            "92   89\n",
            "0.7999999999999999   135\n",
            "93   90\n",
            "0.7999999999999999   136\n",
            "94   91\n",
            "0.7999999999999999   137\n",
            "95   92\n",
            "0.7999999999999999   138\n",
            "96   93\n",
            "0.7999999999999999   139\n",
            "97   94\n",
            "0.7999999999999999   140\n",
            "98   95\n",
            "0.7999999999999999   141\n",
            "99   96\n",
            "0.7999999999999999   142\n",
            "100   97\n",
            "0.7999999999999999   143\n",
            "101   98\n",
            "0.7999999999999999   144\n",
            "101   98\n",
            "0.7999999999999999   145\n",
            "102   99\n",
            "0.7999999999999999   146\n",
            "103   100\n",
            "0.7999999999999999   147\n",
            "103   100\n",
            "0.7999999999999999   148\n",
            "103   100\n",
            "0.7999999999999999   149\n",
            "104   100\n",
            "0.7999999999999999   150\n",
            "105   101\n",
            "0.7999999999999999   151\n",
            "106   102\n",
            "0.7999999999999999   152\n",
            "107   103\n",
            "0.7999999999999999   153\n",
            "108   104\n",
            "0.7999999999999999   154\n",
            "108   104\n",
            "0.7999999999999999   155\n",
            "109   105\n",
            "0.7999999999999999   156\n",
            "109   105\n",
            "0.7999999999999999   157\n",
            "110   106\n",
            "0.7999999999999999   158\n",
            "110   106\n",
            "0.7999999999999999   159\n",
            "111   107\n",
            "0.7999999999999999   160\n",
            "112   108\n",
            "0.7999999999999999   161\n",
            "112   108\n",
            "0.7999999999999999   162\n",
            "113   109\n",
            "0.7999999999999999   163\n",
            "114   110\n",
            "0.7999999999999999   164\n",
            "115   111\n",
            "0.7999999999999999   165\n",
            "116   112\n",
            "0.7999999999999999   166\n",
            "116   112\n",
            "0.7999999999999999   167\n",
            "117   112\n",
            "0.7999999999999999   168\n",
            "118   113\n",
            "0.7999999999999999   169\n",
            "119   114\n",
            "0.7999999999999999   170\n",
            "120   115\n",
            "0.7999999999999999   171\n",
            "120   115\n",
            "0.7999999999999999   172\n",
            "121   116\n",
            "0.7999999999999999   173\n",
            "122   117\n",
            "0.7999999999999999   174\n",
            "123   118\n",
            "0.7999999999999999   175\n",
            "124   119\n",
            "0.7999999999999999   176\n",
            "125   120\n",
            "0.7999999999999999   177\n",
            "126   121\n",
            "0.7999999999999999   178\n",
            "127   121\n",
            "0.7999999999999999   179\n",
            "128   122\n",
            "0.7999999999999999   180\n",
            "128   122\n",
            "0.7999999999999999   181\n",
            "129   123\n",
            "0.7999999999999999   182\n",
            "129   123\n",
            "0.7999999999999999   183\n",
            "130   124\n",
            "0.7999999999999999   184\n",
            "130   124\n",
            "0.7999999999999999   185\n",
            "131   125\n",
            "0.7999999999999999   186\n",
            "132   126\n",
            "0.7999999999999999   187\n",
            "133   126\n",
            "0.7999999999999999   188\n",
            "134   127\n",
            "0.7999999999999999   189\n",
            "135   128\n",
            "0.7999999999999999   190\n",
            "135   128\n",
            "0.7999999999999999   191\n",
            "135   128\n",
            "0.7999999999999999   192\n",
            "136   129\n",
            "0.7999999999999999   193\n",
            "137   130\n",
            "0.7999999999999999   194\n",
            "138   130\n",
            "0.7999999999999999   195\n",
            "139   131\n",
            "0.7999999999999999   196\n",
            "140   132\n",
            "0.7999999999999999   197\n",
            "141   133\n",
            "0.7999999999999999   198\n",
            "142   134\n",
            "0.7999999999999999   199\n",
            "143   135\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for key in result.keys():\n",
        "  print(key,' ',result[key])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HnIb04L5tvLw",
        "outputId": "d6cc6a26-95e1-4348-daed-731f79b07521"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.1   (60, 68)\n",
            "0.2   (84, 107)\n",
            "0.30000000000000004   (97, 130)\n",
            "0.4   (105, 143)\n",
            "0.5   (111, 143)\n",
            "0.6   (122, 143)\n",
            "0.7   (130, 143)\n",
            "0.7999999999999999   (135, 143)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "sfJGJlb903z4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "cifar-10"
      ],
      "metadata": {
        "id": "-eggW0dd04ix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.load_model(model_path)\n",
        "epsilon_end = 0.8\n",
        "#logits_model = tf.keras.Model(model.input,model.layers[-1].output)\n",
        "total_images = 200\n",
        "epsilon = 0.1\n",
        "while(epsilon <= epsilon_end):\n",
        "\n",
        "  fgsm_counter = 0\n",
        "  pgd_counter = 0\n",
        "  print(\"Epsilon value - \",epsilon)\n",
        "  print()\n",
        "  for image_index in range(total_images):\n",
        "  \n",
        "  \n",
        "    image = x_test[image_index]\n",
        "    image = img_to_array(image)\n",
        "    image = image.reshape(1, 32, 32, 3)\n",
        "    true_value = y_test[image_index]\n",
        "    original_prediction = make_prediction(model,image,true_value)\n",
        "    fgsm_sample = fast_gradient_method(model, image, epsilon, np.inf, targeted=False)\n",
        "    pgd_sample = projected_gradient_descent(model, image, epsilon, 0.01, 40, np.inf)\n",
        "    print(epsilon,' ',image_index)\n",
        "    fgsm_prediction = make_prediction(model , fgsm_sample , true_value)\n",
        "    pgd_prediction = make_prediction(model , pgd_sample , true_value)\n",
        "    pgd_counter+=pgd_prediction\n",
        "    fgsm_counter+=fgsm_prediction\n",
        "    print(pgd_counter,' ',fgsm_counter)\n",
        "  result[epsilon] = (fgsm_counter , pgd_counter)\n",
        "  epsilon+=0.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFbjyukVuf7n",
        "outputId": "e6bd1985-a1aa-4be1-caa4-45cccb92bdb3"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epsilon value -  0.1\n",
            "\n",
            "0.1   0\n",
            "0   0\n",
            "0.1   1\n",
            "0   0\n",
            "0.1   2\n",
            "0   0\n",
            "0.1   3\n",
            "0   0\n",
            "0.1   4\n",
            "0   0\n",
            "0.1   5\n",
            "0   0\n",
            "0.1   6\n",
            "0   0\n",
            "0.1   7\n",
            "0   0\n",
            "0.1   8\n",
            "0   0\n",
            "0.1   9\n",
            "1   0\n",
            "0.1   10\n",
            "2   1\n",
            "0.1   11\n",
            "2   1\n",
            "0.1   12\n",
            "3   2\n",
            "0.1   13\n",
            "3   2\n",
            "0.1   14\n",
            "3   2\n",
            "0.1   15\n",
            "4   3\n",
            "0.1   16\n",
            "4   3\n",
            "0.1   17\n",
            "4   3\n",
            "0.1   18\n",
            "4   3\n",
            "0.1   19\n",
            "4   3\n",
            "0.1   20\n",
            "5   4\n",
            "0.1   21\n",
            "5   4\n",
            "0.1   22\n",
            "5   4\n",
            "0.1   23\n",
            "5   4\n",
            "0.1   24\n",
            "6   5\n",
            "0.1   25\n",
            "7   6\n",
            "0.1   26\n",
            "8   6\n",
            "0.1   27\n",
            "8   6\n",
            "0.1   28\n",
            "8   6\n",
            "0.1   29\n",
            "8   6\n",
            "0.1   30\n",
            "8   6\n",
            "0.1   31\n",
            "8   6\n",
            "0.1   32\n",
            "8   6\n",
            "0.1   33\n",
            "8   6\n",
            "0.1   34\n",
            "8   6\n",
            "0.1   35\n",
            "8   6\n",
            "0.1   36\n",
            "8   6\n",
            "0.1   37\n",
            "9   7\n",
            "0.1   38\n",
            "9   7\n",
            "0.1   39\n",
            "9   7\n",
            "0.1   40\n",
            "10   8\n",
            "0.1   41\n",
            "10   8\n",
            "0.1   42\n",
            "10   8\n",
            "0.1   43\n",
            "10   8\n",
            "0.1   44\n",
            "10   8\n",
            "0.1   45\n",
            "10   8\n",
            "0.1   46\n",
            "11   9\n",
            "0.1   47\n",
            "11   9\n",
            "0.1   48\n",
            "11   9\n",
            "0.1   49\n",
            "11   9\n",
            "0.1   50\n",
            "11   9\n",
            "0.1   51\n",
            "11   9\n",
            "0.1   52\n",
            "11   9\n",
            "0.1   53\n",
            "12   10\n",
            "0.1   54\n",
            "12   10\n",
            "0.1   55\n",
            "12   10\n",
            "0.1   56\n",
            "12   10\n",
            "0.1   57\n",
            "13   11\n",
            "0.1   58\n",
            "14   12\n",
            "0.1   59\n",
            "15   13\n",
            "0.1   60\n",
            "15   13\n",
            "0.1   61\n",
            "15   13\n",
            "0.1   62\n",
            "15   13\n",
            "0.1   63\n",
            "16   13\n",
            "0.1   64\n",
            "16   13\n",
            "0.1   65\n",
            "16   13\n",
            "0.1   66\n",
            "16   13\n",
            "0.1   67\n",
            "16   13\n",
            "0.1   68\n",
            "17   13\n",
            "0.1   69\n",
            "17   13\n",
            "0.1   70\n",
            "17   13\n",
            "0.1   71\n",
            "17   13\n",
            "0.1   72\n",
            "17   13\n",
            "0.1   73\n",
            "17   13\n",
            "0.1   74\n",
            "17   13\n",
            "0.1   75\n",
            "17   13\n",
            "0.1   76\n",
            "17   13\n",
            "0.1   77\n",
            "17   13\n",
            "0.1   78\n",
            "18   14\n",
            "0.1   79\n",
            "18   14\n",
            "0.1   80\n",
            "18   14\n",
            "0.1   81\n",
            "18   14\n",
            "0.1   82\n",
            "18   14\n",
            "0.1   83\n",
            "18   14\n",
            "0.1   84\n",
            "19   14\n",
            "0.1   85\n",
            "20   14\n",
            "0.1   86\n",
            "21   15\n",
            "0.1   87\n",
            "21   15\n",
            "0.1   88\n",
            "21   15\n",
            "0.1   89\n",
            "21   15\n",
            "0.1   90\n",
            "22   15\n",
            "0.1   91\n",
            "22   15\n",
            "0.1   92\n",
            "22   15\n",
            "0.1   93\n",
            "22   15\n",
            "0.1   94\n",
            "22   15\n",
            "0.1   95\n",
            "23   16\n",
            "0.1   96\n",
            "23   16\n",
            "0.1   97\n",
            "24   17\n",
            "0.1   98\n",
            "24   17\n",
            "0.1   99\n",
            "24   17\n",
            "0.1   100\n",
            "24   17\n",
            "0.1   101\n",
            "24   17\n",
            "0.1   102\n",
            "24   17\n",
            "0.1   103\n",
            "24   17\n",
            "0.1   104\n",
            "24   17\n",
            "0.1   105\n",
            "24   17\n",
            "0.1   106\n",
            "24   17\n",
            "0.1   107\n",
            "24   17\n",
            "0.1   108\n",
            "24   17\n",
            "0.1   109\n",
            "24   17\n",
            "0.1   110\n",
            "24   17\n",
            "0.1   111\n",
            "24   17\n",
            "0.1   112\n",
            "25   18\n",
            "0.1   113\n",
            "25   18\n",
            "0.1   114\n",
            "25   18\n",
            "0.1   115\n",
            "25   18\n",
            "0.1   116\n",
            "25   18\n",
            "0.1   117\n",
            "26   18\n",
            "0.1   118\n",
            "27   19\n",
            "0.1   119\n",
            "27   19\n",
            "0.1   120\n",
            "27   19\n",
            "0.1   121\n",
            "27   19\n",
            "0.1   122\n",
            "27   19\n",
            "0.1   123\n",
            "27   19\n",
            "0.1   124\n",
            "27   19\n",
            "0.1   125\n",
            "28   20\n",
            "0.1   126\n",
            "28   20\n",
            "0.1   127\n",
            "28   20\n",
            "0.1   128\n",
            "28   20\n",
            "0.1   129\n",
            "29   21\n",
            "0.1   130\n",
            "29   21\n",
            "0.1   131\n",
            "29   21\n",
            "0.1   132\n",
            "29   21\n",
            "0.1   133\n",
            "29   21\n",
            "0.1   134\n",
            "30   22\n",
            "0.1   135\n",
            "30   22\n",
            "0.1   136\n",
            "30   22\n",
            "0.1   137\n",
            "30   22\n",
            "0.1   138\n",
            "30   22\n",
            "0.1   139\n",
            "30   22\n",
            "0.1   140\n",
            "30   22\n",
            "0.1   141\n",
            "30   22\n",
            "0.1   142\n",
            "30   22\n",
            "0.1   143\n",
            "31   23\n",
            "0.1   144\n",
            "31   23\n",
            "0.1   145\n",
            "31   23\n",
            "0.1   146\n",
            "31   23\n",
            "0.1   147\n",
            "32   24\n",
            "0.1   148\n",
            "32   24\n",
            "0.1   149\n",
            "32   24\n",
            "0.1   150\n",
            "32   24\n",
            "0.1   151\n",
            "33   25\n",
            "0.1   152\n",
            "33   25\n",
            "0.1   153\n",
            "33   25\n",
            "0.1   154\n",
            "33   25\n",
            "0.1   155\n",
            "34   26\n",
            "0.1   156\n",
            "34   26\n",
            "0.1   157\n",
            "34   26\n",
            "0.1   158\n",
            "34   26\n",
            "0.1   159\n",
            "34   26\n",
            "0.1   160\n",
            "35   27\n",
            "0.1   161\n",
            "35   27\n",
            "0.1   162\n",
            "36   28\n",
            "0.1   163\n",
            "36   28\n",
            "0.1   164\n",
            "37   29\n",
            "0.1   165\n",
            "38   30\n",
            "0.1   166\n",
            "38   30\n",
            "0.1   167\n",
            "38   30\n",
            "0.1   168\n",
            "38   30\n",
            "0.1   169\n",
            "38   30\n",
            "0.1   170\n",
            "38   30\n",
            "0.1   171\n",
            "38   30\n",
            "0.1   172\n",
            "39   30\n",
            "0.1   173\n",
            "39   30\n",
            "0.1   174\n",
            "39   30\n",
            "0.1   175\n",
            "39   30\n",
            "0.1   176\n",
            "39   30\n",
            "0.1   177\n",
            "39   30\n",
            "0.1   178\n",
            "40   31\n",
            "0.1   179\n",
            "40   31\n",
            "0.1   180\n",
            "41   31\n",
            "0.1   181\n",
            "41   31\n",
            "0.1   182\n",
            "41   31\n",
            "0.1   183\n",
            "42   32\n",
            "0.1   184\n",
            "42   32\n",
            "0.1   185\n",
            "42   32\n",
            "0.1   186\n",
            "42   32\n",
            "0.1   187\n",
            "42   32\n",
            "0.1   188\n",
            "43   33\n",
            "0.1   189\n",
            "43   33\n",
            "0.1   190\n",
            "43   33\n",
            "0.1   191\n",
            "43   33\n",
            "0.1   192\n",
            "44   33\n",
            "0.1   193\n",
            "44   33\n",
            "0.1   194\n",
            "44   33\n",
            "0.1   195\n",
            "44   33\n",
            "0.1   196\n",
            "44   33\n",
            "0.1   197\n",
            "44   33\n",
            "0.1   198\n",
            "44   33\n",
            "0.1   199\n",
            "44   33\n",
            "Epsilon value -  0.2\n",
            "\n",
            "0.2   0\n",
            "0   0\n",
            "0.2   1\n",
            "1   0\n",
            "0.2   2\n",
            "2   1\n",
            "0.2   3\n",
            "3   1\n",
            "0.2   4\n",
            "3   1\n",
            "0.2   5\n",
            "3   1\n",
            "0.2   6\n",
            "3   1\n",
            "0.2   7\n",
            "3   1\n",
            "0.2   8\n",
            "4   1\n",
            "0.2   9\n",
            "5   2\n",
            "0.2   10\n",
            "6   3\n",
            "0.2   11\n",
            "6   3\n",
            "0.2   12\n",
            "7   4\n",
            "0.2   13\n",
            "7   4\n",
            "0.2   14\n",
            "7   4\n",
            "0.2   15\n",
            "8   5\n",
            "0.2   16\n",
            "8   5\n",
            "0.2   17\n",
            "8   5\n",
            "0.2   18\n",
            "8   5\n",
            "0.2   19\n",
            "8   5\n",
            "0.2   20\n",
            "9   6\n",
            "0.2   21\n",
            "9   6\n",
            "0.2   22\n",
            "10   6\n",
            "0.2   23\n",
            "10   6\n",
            "0.2   24\n",
            "11   6\n",
            "0.2   25\n",
            "12   7\n",
            "0.2   26\n",
            "13   8\n",
            "0.2   27\n",
            "14   8\n",
            "0.2   28\n",
            "14   8\n",
            "0.2   29\n",
            "14   8\n",
            "0.2   30\n",
            "14   8\n",
            "0.2   31\n",
            "14   8\n",
            "0.2   32\n",
            "14   8\n",
            "0.2   33\n",
            "14   8\n",
            "0.2   34\n",
            "14   8\n",
            "0.2   35\n",
            "15   9\n",
            "0.2   36\n",
            "16   10\n",
            "0.2   37\n",
            "17   11\n",
            "0.2   38\n",
            "17   11\n",
            "0.2   39\n",
            "17   11\n",
            "0.2   40\n",
            "18   12\n",
            "0.2   41\n",
            "18   12\n",
            "0.2   42\n",
            "19   12\n",
            "0.2   43\n",
            "20   13\n",
            "0.2   44\n",
            "20   13\n",
            "0.2   45\n",
            "20   13\n",
            "0.2   46\n",
            "21   14\n",
            "0.2   47\n",
            "21   14\n",
            "0.2   48\n",
            "21   14\n",
            "0.2   49\n",
            "21   14\n",
            "0.2   50\n",
            "21   14\n",
            "0.2   51\n",
            "21   14\n",
            "0.2   52\n",
            "22   15\n",
            "0.2   53\n",
            "23   16\n",
            "0.2   54\n",
            "23   16\n",
            "0.2   55\n",
            "23   16\n",
            "0.2   56\n",
            "23   16\n",
            "0.2   57\n",
            "24   17\n",
            "0.2   58\n",
            "25   18\n",
            "0.2   59\n",
            "25   19\n",
            "0.2   60\n",
            "25   19\n",
            "0.2   61\n",
            "25   19\n",
            "0.2   62\n",
            "25   19\n",
            "0.2   63\n",
            "26   19\n",
            "0.2   64\n",
            "27   20\n",
            "0.2   65\n",
            "27   20\n",
            "0.2   66\n",
            "27   20\n",
            "0.2   67\n",
            "27   20\n",
            "0.2   68\n",
            "28   20\n",
            "0.2   69\n",
            "28   20\n",
            "0.2   70\n",
            "28   20\n",
            "0.2   71\n",
            "28   20\n",
            "0.2   72\n",
            "29   20\n",
            "0.2   73\n",
            "29   20\n",
            "0.2   74\n",
            "29   20\n",
            "0.2   75\n",
            "29   20\n",
            "0.2   76\n",
            "30   20\n",
            "0.2   77\n",
            "30   20\n",
            "0.2   78\n",
            "31   21\n",
            "0.2   79\n",
            "31   21\n",
            "0.2   80\n",
            "31   21\n",
            "0.2   81\n",
            "32   21\n",
            "0.2   82\n",
            "32   21\n",
            "0.2   83\n",
            "32   21\n",
            "0.2   84\n",
            "33   22\n",
            "0.2   85\n",
            "34   22\n",
            "0.2   86\n",
            "35   23\n",
            "0.2   87\n",
            "36   23\n",
            "0.2   88\n",
            "36   23\n",
            "0.2   89\n",
            "36   23\n",
            "0.2   90\n",
            "37   24\n",
            "0.2   91\n",
            "37   24\n",
            "0.2   92\n",
            "37   24\n",
            "0.2   93\n",
            "37   24\n",
            "0.2   94\n",
            "37   24\n",
            "0.2   95\n",
            "38   25\n",
            "0.2   96\n",
            "38   25\n",
            "0.2   97\n",
            "39   26\n",
            "0.2   98\n",
            "39   26\n",
            "0.2   99\n",
            "39   26\n",
            "0.2   100\n",
            "39   26\n",
            "0.2   101\n",
            "39   26\n",
            "0.2   102\n",
            "40   26\n",
            "0.2   103\n",
            "40   26\n",
            "0.2   104\n",
            "40   26\n",
            "0.2   105\n",
            "40   26\n",
            "0.2   106\n",
            "41   27\n",
            "0.2   107\n",
            "41   27\n",
            "0.2   108\n",
            "41   27\n",
            "0.2   109\n",
            "42   27\n",
            "0.2   110\n",
            "42   27\n",
            "0.2   111\n",
            "43   27\n",
            "0.2   112\n",
            "44   28\n",
            "0.2   113\n",
            "44   28\n",
            "0.2   114\n",
            "44   28\n",
            "0.2   115\n",
            "44   28\n",
            "0.2   116\n",
            "45   28\n",
            "0.2   117\n",
            "46   29\n",
            "0.2   118\n",
            "47   30\n",
            "0.2   119\n",
            "47   30\n",
            "0.2   120\n",
            "47   30\n",
            "0.2   121\n",
            "48   30\n",
            "0.2   122\n",
            "48   30\n",
            "0.2   123\n",
            "48   30\n",
            "0.2   124\n",
            "49   30\n",
            "0.2   125\n",
            "49   31\n",
            "0.2   126\n",
            "49   31\n",
            "0.2   127\n",
            "49   31\n",
            "0.2   128\n",
            "49   31\n",
            "0.2   129\n",
            "50   32\n",
            "0.2   130\n",
            "51   32\n",
            "0.2   131\n",
            "51   32\n",
            "0.2   132\n",
            "51   32\n",
            "0.2   133\n",
            "51   32\n",
            "0.2   134\n",
            "52   33\n",
            "0.2   135\n",
            "52   33\n",
            "0.2   136\n",
            "52   33\n",
            "0.2   137\n",
            "52   33\n",
            "0.2   138\n",
            "52   33\n",
            "0.2   139\n",
            "52   33\n",
            "0.2   140\n",
            "52   33\n",
            "0.2   141\n",
            "52   33\n",
            "0.2   142\n",
            "52   33\n",
            "0.2   143\n",
            "53   34\n",
            "0.2   144\n",
            "53   34\n",
            "0.2   145\n",
            "53   34\n",
            "0.2   146\n",
            "53   34\n",
            "0.2   147\n",
            "53   35\n",
            "0.2   148\n",
            "53   35\n",
            "0.2   149\n",
            "54   35\n",
            "0.2   150\n",
            "54   35\n",
            "0.2   151\n",
            "55   36\n",
            "0.2   152\n",
            "55   36\n",
            "0.2   153\n",
            "55   36\n",
            "0.2   154\n",
            "55   36\n",
            "0.2   155\n",
            "56   37\n",
            "0.2   156\n",
            "57   37\n",
            "0.2   157\n",
            "57   37\n",
            "0.2   158\n",
            "57   37\n",
            "0.2   159\n",
            "57   37\n",
            "0.2   160\n",
            "58   38\n",
            "0.2   161\n",
            "58   38\n",
            "0.2   162\n",
            "58   39\n",
            "0.2   163\n",
            "58   39\n",
            "0.2   164\n",
            "59   40\n",
            "0.2   165\n",
            "60   41\n",
            "0.2   166\n",
            "61   41\n",
            "0.2   167\n",
            "62   41\n",
            "0.2   168\n",
            "63   41\n",
            "0.2   169\n",
            "63   41\n",
            "0.2   170\n",
            "63   41\n",
            "0.2   171\n",
            "64   42\n",
            "0.2   172\n",
            "65   43\n",
            "0.2   173\n",
            "66   43\n",
            "0.2   174\n",
            "66   43\n",
            "0.2   175\n",
            "66   43\n",
            "0.2   176\n",
            "66   43\n",
            "0.2   177\n",
            "67   43\n",
            "0.2   178\n",
            "68   44\n",
            "0.2   179\n",
            "69   44\n",
            "0.2   180\n",
            "70   45\n",
            "0.2   181\n",
            "71   45\n",
            "0.2   182\n",
            "71   45\n",
            "0.2   183\n",
            "72   46\n",
            "0.2   184\n",
            "73   46\n",
            "0.2   185\n",
            "73   46\n",
            "0.2   186\n",
            "74   46\n",
            "0.2   187\n",
            "74   46\n",
            "0.2   188\n",
            "75   47\n",
            "0.2   189\n",
            "76   48\n",
            "0.2   190\n",
            "76   48\n",
            "0.2   191\n",
            "76   48\n",
            "0.2   192\n",
            "77   49\n",
            "0.2   193\n",
            "77   49\n",
            "0.2   194\n",
            "77   49\n",
            "0.2   195\n",
            "78   49\n",
            "0.2   196\n",
            "78   49\n",
            "0.2   197\n",
            "78   49\n",
            "0.2   198\n",
            "78   49\n",
            "0.2   199\n",
            "78   49\n",
            "Epsilon value -  0.30000000000000004\n",
            "\n",
            "0.30000000000000004   0\n",
            "0   0\n",
            "0.30000000000000004   1\n",
            "1   0\n",
            "0.30000000000000004   2\n",
            "2   1\n",
            "0.30000000000000004   3\n",
            "3   2\n",
            "0.30000000000000004   4\n",
            "4   2\n",
            "0.30000000000000004   5\n",
            "4   2\n",
            "0.30000000000000004   6\n",
            "4   2\n",
            "0.30000000000000004   7\n",
            "5   2\n",
            "0.30000000000000004   8\n",
            "6   2\n",
            "0.30000000000000004   9\n",
            "7   3\n",
            "0.30000000000000004   10\n",
            "8   4\n",
            "0.30000000000000004   11\n",
            "8   4\n",
            "0.30000000000000004   12\n",
            "9   5\n",
            "0.30000000000000004   13\n",
            "9   5\n",
            "0.30000000000000004   14\n",
            "10   5\n",
            "0.30000000000000004   15\n",
            "11   6\n",
            "0.30000000000000004   16\n",
            "11   6\n",
            "0.30000000000000004   17\n",
            "11   6\n",
            "0.30000000000000004   18\n",
            "12   6\n",
            "0.30000000000000004   19\n",
            "12   6\n",
            "0.30000000000000004   20\n",
            "13   7\n",
            "0.30000000000000004   21\n",
            "14   7\n",
            "0.30000000000000004   22\n",
            "15   7\n",
            "0.30000000000000004   23\n",
            "15   7\n",
            "0.30000000000000004   24\n",
            "16   7\n",
            "0.30000000000000004   25\n",
            "17   8\n",
            "0.30000000000000004   26\n",
            "18   9\n",
            "0.30000000000000004   27\n",
            "19   10\n",
            "0.30000000000000004   28\n",
            "19   10\n",
            "0.30000000000000004   29\n",
            "19   10\n",
            "0.30000000000000004   30\n",
            "19   10\n",
            "0.30000000000000004   31\n",
            "20   10\n",
            "0.30000000000000004   32\n",
            "20   10\n",
            "0.30000000000000004   33\n",
            "21   10\n",
            "0.30000000000000004   34\n",
            "21   10\n",
            "0.30000000000000004   35\n",
            "22   11\n",
            "0.30000000000000004   36\n",
            "23   12\n",
            "0.30000000000000004   37\n",
            "23   13\n",
            "0.30000000000000004   38\n",
            "23   13\n",
            "0.30000000000000004   39\n",
            "23   13\n",
            "0.30000000000000004   40\n",
            "24   14\n",
            "0.30000000000000004   41\n",
            "24   14\n",
            "0.30000000000000004   42\n",
            "25   15\n",
            "0.30000000000000004   43\n",
            "26   16\n",
            "0.30000000000000004   44\n",
            "27   16\n",
            "0.30000000000000004   45\n",
            "27   16\n",
            "0.30000000000000004   46\n",
            "28   17\n",
            "0.30000000000000004   47\n",
            "28   17\n",
            "0.30000000000000004   48\n",
            "28   17\n",
            "0.30000000000000004   49\n",
            "28   17\n",
            "0.30000000000000004   50\n",
            "29   17\n",
            "0.30000000000000004   51\n",
            "30   17\n",
            "0.30000000000000004   52\n",
            "31   18\n",
            "0.30000000000000004   53\n",
            "32   19\n",
            "0.30000000000000004   54\n",
            "32   19\n",
            "0.30000000000000004   55\n",
            "32   19\n",
            "0.30000000000000004   56\n",
            "32   19\n",
            "0.30000000000000004   57\n",
            "33   20\n",
            "0.30000000000000004   58\n",
            "34   21\n",
            "0.30000000000000004   59\n",
            "34   22\n",
            "0.30000000000000004   60\n",
            "34   22\n",
            "0.30000000000000004   61\n",
            "34   22\n",
            "0.30000000000000004   62\n",
            "34   22\n",
            "0.30000000000000004   63\n",
            "35   23\n",
            "0.30000000000000004   64\n",
            "36   24\n",
            "0.30000000000000004   65\n",
            "37   24\n",
            "0.30000000000000004   66\n",
            "37   24\n",
            "0.30000000000000004   67\n",
            "37   24\n",
            "0.30000000000000004   68\n",
            "38   25\n",
            "0.30000000000000004   69\n",
            "38   25\n",
            "0.30000000000000004   70\n",
            "38   25\n",
            "0.30000000000000004   71\n",
            "38   25\n",
            "0.30000000000000004   72\n",
            "39   25\n",
            "0.30000000000000004   73\n",
            "39   25\n",
            "0.30000000000000004   74\n",
            "40   25\n",
            "0.30000000000000004   75\n",
            "40   25\n",
            "0.30000000000000004   76\n",
            "41   25\n",
            "0.30000000000000004   77\n",
            "42   25\n",
            "0.30000000000000004   78\n",
            "43   26\n",
            "0.30000000000000004   79\n",
            "43   26\n",
            "0.30000000000000004   80\n",
            "43   26\n",
            "0.30000000000000004   81\n",
            "44   27\n",
            "0.30000000000000004   82\n",
            "44   27\n",
            "0.30000000000000004   83\n",
            "44   27\n",
            "0.30000000000000004   84\n",
            "45   28\n",
            "0.30000000000000004   85\n",
            "46   29\n",
            "0.30000000000000004   86\n",
            "47   30\n",
            "0.30000000000000004   87\n",
            "48   30\n",
            "0.30000000000000004   88\n",
            "48   30\n",
            "0.30000000000000004   89\n",
            "48   30\n",
            "0.30000000000000004   90\n",
            "49   31\n",
            "0.30000000000000004   91\n",
            "49   31\n",
            "0.30000000000000004   92\n",
            "50   31\n",
            "0.30000000000000004   93\n",
            "50   31\n",
            "0.30000000000000004   94\n",
            "51   31\n",
            "0.30000000000000004   95\n",
            "52   32\n",
            "0.30000000000000004   96\n",
            "53   32\n",
            "0.30000000000000004   97\n",
            "54   33\n",
            "0.30000000000000004   98\n",
            "54   33\n",
            "0.30000000000000004   99\n",
            "54   33\n",
            "0.30000000000000004   100\n",
            "55   33\n",
            "0.30000000000000004   101\n",
            "56   33\n",
            "0.30000000000000004   102\n",
            "57   34\n",
            "0.30000000000000004   103\n",
            "57   34\n",
            "0.30000000000000004   104\n",
            "57   34\n",
            "0.30000000000000004   105\n",
            "57   34\n",
            "0.30000000000000004   106\n",
            "58   35\n",
            "0.30000000000000004   107\n",
            "58   35\n",
            "0.30000000000000004   108\n",
            "58   35\n",
            "0.30000000000000004   109\n",
            "59   36\n",
            "0.30000000000000004   110\n",
            "59   36\n",
            "0.30000000000000004   111\n",
            "60   37\n",
            "0.30000000000000004   112\n",
            "61   38\n",
            "0.30000000000000004   113\n",
            "62   38\n",
            "0.30000000000000004   114\n",
            "62   38\n",
            "0.30000000000000004   115\n",
            "63   38\n",
            "0.30000000000000004   116\n",
            "64   39\n",
            "0.30000000000000004   117\n",
            "65   40\n",
            "0.30000000000000004   118\n",
            "66   41\n",
            "0.30000000000000004   119\n",
            "67   41\n",
            "0.30000000000000004   120\n",
            "67   41\n",
            "0.30000000000000004   121\n",
            "68   41\n",
            "0.30000000000000004   122\n",
            "68   41\n",
            "0.30000000000000004   123\n",
            "68   41\n",
            "0.30000000000000004   124\n",
            "69   41\n",
            "0.30000000000000004   125\n",
            "69   42\n",
            "0.30000000000000004   126\n",
            "69   42\n",
            "0.30000000000000004   127\n",
            "70   42\n",
            "0.30000000000000004   128\n",
            "70   42\n",
            "0.30000000000000004   129\n",
            "71   43\n",
            "0.30000000000000004   130\n",
            "72   44\n",
            "0.30000000000000004   131\n",
            "72   44\n",
            "0.30000000000000004   132\n",
            "72   44\n",
            "0.30000000000000004   133\n",
            "72   44\n",
            "0.30000000000000004   134\n",
            "73   45\n",
            "0.30000000000000004   135\n",
            "74   45\n",
            "0.30000000000000004   136\n",
            "74   45\n",
            "0.30000000000000004   137\n",
            "74   45\n",
            "0.30000000000000004   138\n",
            "74   45\n",
            "0.30000000000000004   139\n",
            "74   45\n",
            "0.30000000000000004   140\n",
            "75   45\n",
            "0.30000000000000004   141\n",
            "75   45\n",
            "0.30000000000000004   142\n",
            "75   45\n",
            "0.30000000000000004   143\n",
            "76   46\n",
            "0.30000000000000004   144\n",
            "76   46\n",
            "0.30000000000000004   145\n",
            "76   46\n",
            "0.30000000000000004   146\n",
            "77   46\n",
            "0.30000000000000004   147\n",
            "77   46\n",
            "0.30000000000000004   148\n",
            "77   46\n",
            "0.30000000000000004   149\n",
            "78   47\n",
            "0.30000000000000004   150\n",
            "78   47\n",
            "0.30000000000000004   151\n",
            "79   48\n",
            "0.30000000000000004   152\n",
            "79   48\n",
            "0.30000000000000004   153\n",
            "79   48\n",
            "0.30000000000000004   154\n",
            "80   48\n",
            "0.30000000000000004   155\n",
            "81   49\n",
            "0.30000000000000004   156\n",
            "82   49\n",
            "0.30000000000000004   157\n",
            "82   49\n",
            "0.30000000000000004   158\n",
            "83   49\n",
            "0.30000000000000004   159\n",
            "83   49\n",
            "0.30000000000000004   160\n",
            "84   50\n",
            "0.30000000000000004   161\n",
            "84   50\n",
            "0.30000000000000004   162\n",
            "84   51\n",
            "0.30000000000000004   163\n",
            "84   51\n",
            "0.30000000000000004   164\n",
            "85   52\n",
            "0.30000000000000004   165\n",
            "86   53\n",
            "0.30000000000000004   166\n",
            "87   53\n",
            "0.30000000000000004   167\n",
            "88   54\n",
            "0.30000000000000004   168\n",
            "89   54\n",
            "0.30000000000000004   169\n",
            "89   54\n",
            "0.30000000000000004   170\n",
            "89   54\n",
            "0.30000000000000004   171\n",
            "90   55\n",
            "0.30000000000000004   172\n",
            "91   56\n",
            "0.30000000000000004   173\n",
            "92   56\n",
            "0.30000000000000004   174\n",
            "92   56\n",
            "0.30000000000000004   175\n",
            "92   56\n",
            "0.30000000000000004   176\n",
            "92   56\n",
            "0.30000000000000004   177\n",
            "93   57\n",
            "0.30000000000000004   178\n",
            "94   58\n",
            "0.30000000000000004   179\n",
            "95   58\n",
            "0.30000000000000004   180\n",
            "96   59\n",
            "0.30000000000000004   181\n",
            "97   59\n",
            "0.30000000000000004   182\n",
            "97   59\n",
            "0.30000000000000004   183\n",
            "98   60\n",
            "0.30000000000000004   184\n",
            "99   60\n",
            "0.30000000000000004   185\n",
            "99   60\n",
            "0.30000000000000004   186\n",
            "100   60\n",
            "0.30000000000000004   187\n",
            "100   60\n",
            "0.30000000000000004   188\n",
            "101   61\n",
            "0.30000000000000004   189\n",
            "102   62\n",
            "0.30000000000000004   190\n",
            "102   62\n",
            "0.30000000000000004   191\n",
            "102   62\n",
            "0.30000000000000004   192\n",
            "103   63\n",
            "0.30000000000000004   193\n",
            "104   63\n",
            "0.30000000000000004   194\n",
            "104   63\n",
            "0.30000000000000004   195\n",
            "105   63\n",
            "0.30000000000000004   196\n",
            "105   63\n",
            "0.30000000000000004   197\n",
            "106   63\n",
            "0.30000000000000004   198\n",
            "107   63\n",
            "0.30000000000000004   199\n",
            "107   63\n",
            "Epsilon value -  0.4\n",
            "\n",
            "0.4   0\n",
            "1   0\n",
            "0.4   1\n",
            "2   0\n",
            "0.4   2\n",
            "3   1\n",
            "0.4   3\n",
            "4   2\n",
            "0.4   4\n",
            "5   2\n",
            "0.4   5\n",
            "5   2\n",
            "0.4   6\n",
            "5   2\n",
            "0.4   7\n",
            "6   2\n",
            "0.4   8\n",
            "7   2\n",
            "0.4   9\n",
            "8   3\n",
            "0.4   10\n",
            "9   4\n",
            "0.4   11\n",
            "9   4\n",
            "0.4   12\n",
            "10   5\n",
            "0.4   13\n",
            "10   5\n",
            "0.4   14\n",
            "11   5\n",
            "0.4   15\n",
            "12   6\n",
            "0.4   16\n",
            "12   6\n",
            "0.4   17\n",
            "13   6\n",
            "0.4   18\n",
            "14   6\n",
            "0.4   19\n",
            "14   6\n",
            "0.4   20\n",
            "15   7\n",
            "0.4   21\n",
            "16   7\n",
            "0.4   22\n",
            "17   8\n",
            "0.4   23\n",
            "17   8\n",
            "0.4   24\n",
            "18   8\n",
            "0.4   25\n",
            "19   9\n",
            "0.4   26\n",
            "20   10\n",
            "0.4   27\n",
            "21   11\n",
            "0.4   28\n",
            "21   11\n",
            "0.4   29\n",
            "21   11\n",
            "0.4   30\n",
            "21   11\n",
            "0.4   31\n",
            "22   11\n",
            "0.4   32\n",
            "22   11\n",
            "0.4   33\n",
            "23   11\n",
            "0.4   34\n",
            "23   11\n",
            "0.4   35\n",
            "24   12\n",
            "0.4   36\n",
            "25   13\n",
            "0.4   37\n",
            "25   14\n",
            "0.4   38\n",
            "25   14\n",
            "0.4   39\n",
            "25   14\n",
            "0.4   40\n",
            "26   15\n",
            "0.4   41\n",
            "26   15\n",
            "0.4   42\n",
            "27   16\n",
            "0.4   43\n",
            "28   17\n",
            "0.4   44\n",
            "29   17\n",
            "0.4   45\n",
            "29   17\n",
            "0.4   46\n",
            "30   18\n",
            "0.4   47\n",
            "30   18\n",
            "0.4   48\n",
            "30   18\n",
            "0.4   49\n",
            "30   18\n",
            "0.4   50\n",
            "31   18\n",
            "0.4   51\n",
            "32   18\n",
            "0.4   52\n",
            "33   19\n",
            "0.4   53\n",
            "34   20\n",
            "0.4   54\n",
            "34   20\n",
            "0.4   55\n",
            "34   20\n",
            "0.4   56\n",
            "34   20\n",
            "0.4   57\n",
            "35   21\n",
            "0.4   58\n",
            "36   22\n",
            "0.4   59\n",
            "36   23\n",
            "0.4   60\n",
            "36   23\n",
            "0.4   61\n",
            "36   23\n",
            "0.4   62\n",
            "36   23\n",
            "0.4   63\n",
            "37   24\n",
            "0.4   64\n",
            "38   25\n",
            "0.4   65\n",
            "39   25\n",
            "0.4   66\n",
            "39   25\n",
            "0.4   67\n",
            "39   25\n",
            "0.4   68\n",
            "40   26\n",
            "0.4   69\n",
            "40   26\n",
            "0.4   70\n",
            "40   26\n",
            "0.4   71\n",
            "40   26\n",
            "0.4   72\n",
            "41   26\n",
            "0.4   73\n",
            "41   26\n",
            "0.4   74\n",
            "42   26\n",
            "0.4   75\n",
            "42   26\n",
            "0.4   76\n",
            "43   26\n",
            "0.4   77\n",
            "44   26\n",
            "0.4   78\n",
            "45   27\n",
            "0.4   79\n",
            "45   27\n",
            "0.4   80\n",
            "45   27\n",
            "0.4   81\n",
            "46   28\n",
            "0.4   82\n",
            "46   28\n",
            "0.4   83\n",
            "46   28\n",
            "0.4   84\n",
            "47   29\n",
            "0.4   85\n",
            "48   30\n",
            "0.4   86\n",
            "49   31\n",
            "0.4   87\n",
            "50   31\n",
            "0.4   88\n",
            "50   31\n",
            "0.4   89\n",
            "50   31\n",
            "0.4   90\n",
            "51   32\n",
            "0.4   91\n",
            "52   32\n",
            "0.4   92\n",
            "53   32\n",
            "0.4   93\n",
            "53   32\n",
            "0.4   94\n",
            "54   32\n",
            "0.4   95\n",
            "55   33\n",
            "0.4   96\n",
            "56   34\n",
            "0.4   97\n",
            "57   35\n",
            "0.4   98\n",
            "57   35\n",
            "0.4   99\n",
            "57   35\n",
            "0.4   100\n",
            "58   35\n",
            "0.4   101\n",
            "59   35\n",
            "0.4   102\n",
            "60   36\n",
            "0.4   103\n",
            "60   36\n",
            "0.4   104\n",
            "60   36\n",
            "0.4   105\n",
            "60   36\n",
            "0.4   106\n",
            "61   37\n",
            "0.4   107\n",
            "61   37\n",
            "0.4   108\n",
            "61   37\n",
            "0.4   109\n",
            "62   38\n",
            "0.4   110\n",
            "62   38\n",
            "0.4   111\n",
            "63   39\n",
            "0.4   112\n",
            "64   40\n",
            "0.4   113\n",
            "65   40\n",
            "0.4   114\n",
            "65   40\n",
            "0.4   115\n",
            "66   40\n",
            "0.4   116\n",
            "67   41\n",
            "0.4   117\n",
            "68   42\n",
            "0.4   118\n",
            "69   43\n",
            "0.4   119\n",
            "70   43\n",
            "0.4   120\n",
            "70   43\n",
            "0.4   121\n",
            "71   43\n",
            "0.4   122\n",
            "71   43\n",
            "0.4   123\n",
            "71   43\n",
            "0.4   124\n",
            "72   44\n",
            "0.4   125\n",
            "72   44\n",
            "0.4   126\n",
            "72   44\n",
            "0.4   127\n",
            "73   44\n",
            "0.4   128\n",
            "73   44\n",
            "0.4   129\n",
            "74   45\n",
            "0.4   130\n",
            "75   46\n",
            "0.4   131\n",
            "75   46\n",
            "0.4   132\n",
            "75   46\n",
            "0.4   133\n",
            "75   46\n",
            "0.4   134\n",
            "76   47\n",
            "0.4   135\n",
            "77   47\n",
            "0.4   136\n",
            "77   47\n",
            "0.4   137\n",
            "77   47\n",
            "0.4   138\n",
            "78   47\n",
            "0.4   139\n",
            "78   47\n",
            "0.4   140\n",
            "79   48\n",
            "0.4   141\n",
            "80   48\n",
            "0.4   142\n",
            "80   48\n",
            "0.4   143\n",
            "81   49\n",
            "0.4   144\n",
            "81   49\n",
            "0.4   145\n",
            "82   49\n",
            "0.4   146\n",
            "83   49\n",
            "0.4   147\n",
            "83   49\n",
            "0.4   148\n",
            "83   49\n",
            "0.4   149\n",
            "84   50\n",
            "0.4   150\n",
            "84   50\n",
            "0.4   151\n",
            "85   51\n",
            "0.4   152\n",
            "85   51\n",
            "0.4   153\n",
            "86   51\n",
            "0.4   154\n",
            "87   51\n",
            "0.4   155\n",
            "88   52\n",
            "0.4   156\n",
            "89   52\n",
            "0.4   157\n",
            "89   52\n",
            "0.4   158\n",
            "90   52\n",
            "0.4   159\n",
            "90   52\n",
            "0.4   160\n",
            "91   53\n",
            "0.4   161\n",
            "91   53\n",
            "0.4   162\n",
            "91   54\n",
            "0.4   163\n",
            "91   54\n",
            "0.4   164\n",
            "92   55\n",
            "0.4   165\n",
            "93   56\n",
            "0.4   166\n",
            "94   56\n",
            "0.4   167\n",
            "95   57\n",
            "0.4   168\n",
            "96   58\n",
            "0.4   169\n",
            "96   58\n",
            "0.4   170\n",
            "96   58\n",
            "0.4   171\n",
            "97   59\n",
            "0.4   172\n",
            "98   60\n",
            "0.4   173\n",
            "99   61\n",
            "0.4   174\n",
            "100   61\n",
            "0.4   175\n",
            "100   61\n",
            "0.4   176\n",
            "100   61\n",
            "0.4   177\n",
            "101   62\n",
            "0.4   178\n",
            "102   63\n",
            "0.4   179\n",
            "103   64\n",
            "0.4   180\n",
            "104   65\n",
            "0.4   181\n",
            "105   66\n",
            "0.4   182\n",
            "105   66\n",
            "0.4   183\n",
            "106   67\n",
            "0.4   184\n",
            "107   67\n",
            "0.4   185\n",
            "107   67\n",
            "0.4   186\n",
            "108   68\n",
            "0.4   187\n",
            "108   68\n",
            "0.4   188\n",
            "109   69\n",
            "0.4   189\n",
            "110   70\n",
            "0.4   190\n",
            "110   70\n",
            "0.4   191\n",
            "110   70\n",
            "0.4   192\n",
            "111   71\n",
            "0.4   193\n",
            "112   71\n",
            "0.4   194\n",
            "112   71\n",
            "0.4   195\n",
            "113   72\n",
            "0.4   196\n",
            "113   72\n",
            "0.4   197\n",
            "114   72\n",
            "0.4   198\n",
            "115   72\n",
            "0.4   199\n",
            "115   72\n",
            "Epsilon value -  0.5\n",
            "\n",
            "0.5   0\n",
            "1   0\n",
            "0.5   1\n",
            "2   0\n",
            "0.5   2\n",
            "3   1\n",
            "0.5   3\n",
            "4   2\n",
            "0.5   4\n",
            "5   3\n",
            "0.5   5\n",
            "5   3\n",
            "0.5   6\n",
            "5   3\n",
            "0.5   7\n",
            "6   3\n",
            "0.5   8\n",
            "7   4\n",
            "0.5   9\n",
            "8   5\n",
            "0.5   10\n",
            "9   6\n",
            "0.5   11\n",
            "9   6\n",
            "0.5   12\n",
            "10   7\n",
            "0.5   13\n",
            "10   7\n",
            "0.5   14\n",
            "11   7\n",
            "0.5   15\n",
            "12   8\n",
            "0.5   16\n",
            "12   8\n",
            "0.5   17\n",
            "13   8\n",
            "0.5   18\n",
            "14   8\n",
            "0.5   19\n",
            "14   8\n",
            "0.5   20\n",
            "15   9\n",
            "0.5   21\n",
            "16   9\n",
            "0.5   22\n",
            "17   10\n",
            "0.5   23\n",
            "17   10\n",
            "0.5   24\n",
            "18   10\n",
            "0.5   25\n",
            "19   11\n",
            "0.5   26\n",
            "20   12\n",
            "0.5   27\n",
            "21   13\n",
            "0.5   28\n",
            "21   13\n",
            "0.5   29\n",
            "21   13\n",
            "0.5   30\n",
            "21   13\n",
            "0.5   31\n",
            "22   14\n",
            "0.5   32\n",
            "22   14\n",
            "0.5   33\n",
            "23   14\n",
            "0.5   34\n",
            "23   14\n",
            "0.5   35\n",
            "24   15\n",
            "0.5   36\n",
            "25   16\n",
            "0.5   37\n",
            "25   17\n",
            "0.5   38\n",
            "25   17\n",
            "0.5   39\n",
            "25   17\n",
            "0.5   40\n",
            "26   18\n",
            "0.5   41\n",
            "26   18\n",
            "0.5   42\n",
            "27   19\n",
            "0.5   43\n",
            "28   20\n",
            "0.5   44\n",
            "29   20\n",
            "0.5   45\n",
            "29   20\n",
            "0.5   46\n",
            "30   21\n",
            "0.5   47\n",
            "30   21\n",
            "0.5   48\n",
            "30   21\n",
            "0.5   49\n",
            "30   21\n",
            "0.5   50\n",
            "31   21\n",
            "0.5   51\n",
            "32   21\n",
            "0.5   52\n",
            "33   22\n",
            "0.5   53\n",
            "34   23\n",
            "0.5   54\n",
            "34   23\n",
            "0.5   55\n",
            "34   23\n",
            "0.5   56\n",
            "34   23\n",
            "0.5   57\n",
            "35   24\n",
            "0.5   58\n",
            "36   25\n",
            "0.5   59\n",
            "36   26\n",
            "0.5   60\n",
            "36   26\n",
            "0.5   61\n",
            "36   26\n",
            "0.5   62\n",
            "36   26\n",
            "0.5   63\n",
            "37   27\n",
            "0.5   64\n",
            "38   28\n",
            "0.5   65\n",
            "39   29\n",
            "0.5   66\n",
            "39   29\n",
            "0.5   67\n",
            "39   29\n",
            "0.5   68\n",
            "40   30\n",
            "0.5   69\n",
            "40   30\n",
            "0.5   70\n",
            "40   30\n",
            "0.5   71\n",
            "40   30\n",
            "0.5   72\n",
            "41   30\n",
            "0.5   73\n",
            "41   30\n",
            "0.5   74\n",
            "42   30\n",
            "0.5   75\n",
            "42   30\n",
            "0.5   76\n",
            "43   30\n",
            "0.5   77\n",
            "44   30\n",
            "0.5   78\n",
            "45   31\n",
            "0.5   79\n",
            "45   31\n",
            "0.5   80\n",
            "45   31\n",
            "0.5   81\n",
            "46   32\n",
            "0.5   82\n",
            "46   32\n",
            "0.5   83\n",
            "46   32\n",
            "0.5   84\n",
            "47   33\n",
            "0.5   85\n",
            "48   34\n",
            "0.5   86\n",
            "49   35\n",
            "0.5   87\n",
            "50   35\n",
            "0.5   88\n",
            "50   35\n",
            "0.5   89\n",
            "50   35\n",
            "0.5   90\n",
            "51   36\n",
            "0.5   91\n",
            "52   36\n",
            "0.5   92\n",
            "53   36\n",
            "0.5   93\n",
            "53   36\n",
            "0.5   94\n",
            "54   36\n",
            "0.5   95\n",
            "55   37\n",
            "0.5   96\n",
            "56   38\n",
            "0.5   97\n",
            "57   39\n",
            "0.5   98\n",
            "57   39\n",
            "0.5   99\n",
            "57   39\n",
            "0.5   100\n",
            "58   39\n",
            "0.5   101\n",
            "59   39\n",
            "0.5   102\n",
            "60   40\n",
            "0.5   103\n",
            "60   40\n",
            "0.5   104\n",
            "60   40\n",
            "0.5   105\n",
            "60   40\n",
            "0.5   106\n",
            "61   41\n",
            "0.5   107\n",
            "61   41\n",
            "0.5   108\n",
            "61   41\n",
            "0.5   109\n",
            "62   42\n",
            "0.5   110\n",
            "62   42\n",
            "0.5   111\n",
            "63   43\n",
            "0.5   112\n",
            "64   44\n",
            "0.5   113\n",
            "65   44\n",
            "0.5   114\n",
            "65   44\n",
            "0.5   115\n",
            "66   44\n",
            "0.5   116\n",
            "67   45\n",
            "0.5   117\n",
            "68   46\n",
            "0.5   118\n",
            "69   47\n",
            "0.5   119\n",
            "70   47\n",
            "0.5   120\n",
            "70   47\n",
            "0.5   121\n",
            "71   47\n",
            "0.5   122\n",
            "71   47\n",
            "0.5   123\n",
            "71   47\n",
            "0.5   124\n",
            "72   48\n",
            "0.5   125\n",
            "72   48\n",
            "0.5   126\n",
            "72   48\n",
            "0.5   127\n",
            "73   48\n",
            "0.5   128\n",
            "73   48\n",
            "0.5   129\n",
            "74   49\n",
            "0.5   130\n",
            "75   50\n",
            "0.5   131\n",
            "75   50\n",
            "0.5   132\n",
            "75   50\n",
            "0.5   133\n",
            "75   50\n",
            "0.5   134\n",
            "76   51\n",
            "0.5   135\n",
            "77   51\n",
            "0.5   136\n",
            "77   51\n",
            "0.5   137\n",
            "77   51\n",
            "0.5   138\n",
            "78   51\n",
            "0.5   139\n",
            "78   51\n",
            "0.5   140\n",
            "79   52\n",
            "0.5   141\n",
            "80   52\n",
            "0.5   142\n",
            "80   52\n",
            "0.5   143\n",
            "81   53\n",
            "0.5   144\n",
            "81   53\n",
            "0.5   145\n",
            "82   53\n",
            "0.5   146\n",
            "83   53\n",
            "0.5   147\n",
            "83   53\n",
            "0.5   148\n",
            "83   53\n",
            "0.5   149\n",
            "84   54\n",
            "0.5   150\n",
            "84   54\n",
            "0.5   151\n",
            "85   55\n",
            "0.5   152\n",
            "85   55\n",
            "0.5   153\n",
            "86   55\n",
            "0.5   154\n",
            "87   55\n",
            "0.5   155\n",
            "88   56\n",
            "0.5   156\n",
            "89   57\n",
            "0.5   157\n",
            "89   57\n",
            "0.5   158\n",
            "90   57\n",
            "0.5   159\n",
            "90   57\n",
            "0.5   160\n",
            "91   58\n",
            "0.5   161\n",
            "91   58\n",
            "0.5   162\n",
            "91   59\n",
            "0.5   163\n",
            "91   59\n",
            "0.5   164\n",
            "92   60\n",
            "0.5   165\n",
            "93   61\n",
            "0.5   166\n",
            "94   62\n",
            "0.5   167\n",
            "95   63\n",
            "0.5   168\n",
            "96   64\n",
            "0.5   169\n",
            "96   64\n",
            "0.5   170\n",
            "96   64\n",
            "0.5   171\n",
            "97   65\n",
            "0.5   172\n",
            "98   66\n",
            "0.5   173\n",
            "99   67\n",
            "0.5   174\n",
            "100   68\n",
            "0.5   175\n",
            "100   68\n",
            "0.5   176\n",
            "100   68\n",
            "0.5   177\n",
            "101   69\n",
            "0.5   178\n",
            "102   70\n",
            "0.5   179\n",
            "103   71\n",
            "0.5   180\n",
            "104   72\n",
            "0.5   181\n",
            "105   73\n",
            "0.5   182\n",
            "105   73\n",
            "0.5   183\n",
            "106   74\n",
            "0.5   184\n",
            "107   74\n",
            "0.5   185\n",
            "107   74\n",
            "0.5   186\n",
            "108   75\n",
            "0.5   187\n",
            "108   75\n",
            "0.5   188\n",
            "109   76\n",
            "0.5   189\n",
            "110   77\n",
            "0.5   190\n",
            "110   77\n",
            "0.5   191\n",
            "110   77\n",
            "0.5   192\n",
            "111   78\n",
            "0.5   193\n",
            "112   79\n",
            "0.5   194\n",
            "112   79\n",
            "0.5   195\n",
            "113   80\n",
            "0.5   196\n",
            "113   80\n",
            "0.5   197\n",
            "114   80\n",
            "0.5   198\n",
            "115   80\n",
            "0.5   199\n",
            "115   80\n",
            "Epsilon value -  0.6\n",
            "\n",
            "0.6   0\n",
            "1   0\n",
            "0.6   1\n",
            "2   0\n",
            "0.6   2\n",
            "3   1\n",
            "0.6   3\n",
            "4   2\n",
            "0.6   4\n",
            "5   3\n",
            "0.6   5\n",
            "5   3\n",
            "0.6   6\n",
            "5   3\n",
            "0.6   7\n",
            "6   3\n",
            "0.6   8\n",
            "7   4\n",
            "0.6   9\n",
            "8   5\n",
            "0.6   10\n",
            "9   6\n",
            "0.6   11\n",
            "9   6\n",
            "0.6   12\n",
            "10   7\n",
            "0.6   13\n",
            "10   7\n",
            "0.6   14\n",
            "11   8\n",
            "0.6   15\n",
            "12   9\n",
            "0.6   16\n",
            "12   9\n",
            "0.6   17\n",
            "13   10\n",
            "0.6   18\n",
            "14   10\n",
            "0.6   19\n",
            "14   10\n",
            "0.6   20\n",
            "15   11\n",
            "0.6   21\n",
            "16   11\n",
            "0.6   22\n",
            "17   12\n",
            "0.6   23\n",
            "17   12\n",
            "0.6   24\n",
            "18   12\n",
            "0.6   25\n",
            "19   13\n",
            "0.6   26\n",
            "20   14\n",
            "0.6   27\n",
            "21   15\n",
            "0.6   28\n",
            "21   15\n",
            "0.6   29\n",
            "21   15\n",
            "0.6   30\n",
            "21   15\n",
            "0.6   31\n",
            "22   16\n",
            "0.6   32\n",
            "22   16\n",
            "0.6   33\n",
            "23   16\n",
            "0.6   34\n",
            "23   16\n",
            "0.6   35\n",
            "24   17\n",
            "0.6   36\n",
            "25   18\n",
            "0.6   37\n",
            "25   19\n",
            "0.6   38\n",
            "25   19\n",
            "0.6   39\n",
            "25   19\n",
            "0.6   40\n",
            "26   20\n",
            "0.6   41\n",
            "26   20\n",
            "0.6   42\n",
            "27   21\n",
            "0.6   43\n",
            "28   22\n",
            "0.6   44\n",
            "29   22\n",
            "0.6   45\n",
            "29   22\n",
            "0.6   46\n",
            "30   23\n",
            "0.6   47\n",
            "30   23\n",
            "0.6   48\n",
            "30   23\n",
            "0.6   49\n",
            "30   23\n",
            "0.6   50\n",
            "31   23\n",
            "0.6   51\n",
            "32   23\n",
            "0.6   52\n",
            "33   24\n",
            "0.6   53\n",
            "34   25\n",
            "0.6   54\n",
            "34   25\n",
            "0.6   55\n",
            "34   25\n",
            "0.6   56\n",
            "34   25\n",
            "0.6   57\n",
            "35   26\n",
            "0.6   58\n",
            "36   27\n",
            "0.6   59\n",
            "36   28\n",
            "0.6   60\n",
            "36   28\n",
            "0.6   61\n",
            "36   28\n",
            "0.6   62\n",
            "36   28\n",
            "0.6   63\n",
            "37   29\n",
            "0.6   64\n",
            "38   30\n",
            "0.6   65\n",
            "39   31\n",
            "0.6   66\n",
            "39   31\n",
            "0.6   67\n",
            "39   31\n",
            "0.6   68\n",
            "40   32\n",
            "0.6   69\n",
            "40   32\n",
            "0.6   70\n",
            "40   32\n",
            "0.6   71\n",
            "40   32\n",
            "0.6   72\n",
            "41   32\n",
            "0.6   73\n",
            "41   32\n",
            "0.6   74\n",
            "42   33\n",
            "0.6   75\n",
            "42   33\n",
            "0.6   76\n",
            "43   33\n",
            "0.6   77\n",
            "44   33\n",
            "0.6   78\n",
            "45   34\n",
            "0.6   79\n",
            "45   34\n",
            "0.6   80\n",
            "45   34\n",
            "0.6   81\n",
            "46   35\n",
            "0.6   82\n",
            "46   35\n",
            "0.6   83\n",
            "46   35\n",
            "0.6   84\n",
            "47   36\n",
            "0.6   85\n",
            "48   37\n",
            "0.6   86\n",
            "49   38\n",
            "0.6   87\n",
            "50   38\n",
            "0.6   88\n",
            "50   38\n",
            "0.6   89\n",
            "50   38\n",
            "0.6   90\n",
            "51   39\n",
            "0.6   91\n",
            "52   39\n",
            "0.6   92\n",
            "53   40\n",
            "0.6   93\n",
            "53   40\n",
            "0.6   94\n",
            "54   40\n",
            "0.6   95\n",
            "55   41\n",
            "0.6   96\n",
            "56   42\n",
            "0.6   97\n",
            "57   43\n",
            "0.6   98\n",
            "57   43\n",
            "0.6   99\n",
            "57   43\n",
            "0.6   100\n",
            "58   43\n",
            "0.6   101\n",
            "59   43\n",
            "0.6   102\n",
            "60   44\n",
            "0.6   103\n",
            "60   44\n",
            "0.6   104\n",
            "60   44\n",
            "0.6   105\n",
            "60   44\n",
            "0.6   106\n",
            "61   45\n",
            "0.6   107\n",
            "61   45\n",
            "0.6   108\n",
            "61   45\n",
            "0.6   109\n",
            "62   46\n",
            "0.6   110\n",
            "62   47\n",
            "0.6   111\n",
            "63   48\n",
            "0.6   112\n",
            "64   49\n",
            "0.6   113\n",
            "65   49\n",
            "0.6   114\n",
            "65   49\n",
            "0.6   115\n",
            "66   49\n",
            "0.6   116\n",
            "67   50\n",
            "0.6   117\n",
            "68   51\n",
            "0.6   118\n",
            "69   52\n",
            "0.6   119\n",
            "70   53\n",
            "0.6   120\n",
            "70   53\n",
            "0.6   121\n",
            "71   54\n",
            "0.6   122\n",
            "71   54\n",
            "0.6   123\n",
            "71   54\n",
            "0.6   124\n",
            "72   55\n",
            "0.6   125\n",
            "72   56\n",
            "0.6   126\n",
            "72   56\n",
            "0.6   127\n",
            "73   56\n",
            "0.6   128\n",
            "73   56\n",
            "0.6   129\n",
            "74   57\n",
            "0.6   130\n",
            "75   58\n",
            "0.6   131\n",
            "75   58\n",
            "0.6   132\n",
            "75   58\n",
            "0.6   133\n",
            "75   58\n",
            "0.6   134\n",
            "76   59\n",
            "0.6   135\n",
            "77   59\n",
            "0.6   136\n",
            "77   59\n",
            "0.6   137\n",
            "77   59\n",
            "0.6   138\n",
            "78   59\n",
            "0.6   139\n",
            "78   59\n",
            "0.6   140\n",
            "79   60\n",
            "0.6   141\n",
            "80   61\n",
            "0.6   142\n",
            "80   61\n",
            "0.6   143\n",
            "81   62\n",
            "0.6   144\n",
            "81   62\n",
            "0.6   145\n",
            "82   62\n",
            "0.6   146\n",
            "83   62\n",
            "0.6   147\n",
            "83   62\n",
            "0.6   148\n",
            "83   62\n",
            "0.6   149\n",
            "84   63\n",
            "0.6   150\n",
            "84   63\n",
            "0.6   151\n",
            "85   64\n",
            "0.6   152\n",
            "85   64\n",
            "0.6   153\n",
            "86   64\n",
            "0.6   154\n",
            "87   64\n",
            "0.6   155\n",
            "88   65\n",
            "0.6   156\n",
            "89   66\n",
            "0.6   157\n",
            "89   66\n",
            "0.6   158\n",
            "90   66\n",
            "0.6   159\n",
            "90   66\n",
            "0.6   160\n",
            "91   67\n",
            "0.6   161\n",
            "91   67\n",
            "0.6   162\n",
            "91   68\n",
            "0.6   163\n",
            "91   68\n",
            "0.6   164\n",
            "92   69\n",
            "0.6   165\n",
            "93   70\n",
            "0.6   166\n",
            "94   71\n",
            "0.6   167\n",
            "95   72\n",
            "0.6   168\n",
            "96   73\n",
            "0.6   169\n",
            "96   73\n",
            "0.6   170\n",
            "96   73\n",
            "0.6   171\n",
            "97   74\n",
            "0.6   172\n",
            "98   75\n",
            "0.6   173\n",
            "99   76\n",
            "0.6   174\n",
            "100   77\n",
            "0.6   175\n",
            "100   77\n",
            "0.6   176\n",
            "100   77\n",
            "0.6   177\n",
            "101   78\n",
            "0.6   178\n",
            "102   79\n",
            "0.6   179\n",
            "103   80\n",
            "0.6   180\n",
            "104   81\n",
            "0.6   181\n",
            "105   82\n",
            "0.6   182\n",
            "105   82\n",
            "0.6   183\n",
            "106   83\n",
            "0.6   184\n",
            "107   83\n",
            "0.6   185\n",
            "107   83\n",
            "0.6   186\n",
            "108   84\n",
            "0.6   187\n",
            "108   84\n",
            "0.6   188\n",
            "109   85\n",
            "0.6   189\n",
            "110   86\n",
            "0.6   190\n",
            "110   86\n",
            "0.6   191\n",
            "110   86\n",
            "0.6   192\n",
            "111   87\n",
            "0.6   193\n",
            "112   88\n",
            "0.6   194\n",
            "112   88\n",
            "0.6   195\n",
            "113   89\n",
            "0.6   196\n",
            "113   89\n",
            "0.6   197\n",
            "114   89\n",
            "0.6   198\n",
            "115   90\n",
            "0.6   199\n",
            "115   90\n",
            "Epsilon value -  0.7\n",
            "\n",
            "0.7   0\n",
            "1   0\n",
            "0.7   1\n",
            "2   0\n",
            "0.7   2\n",
            "3   1\n",
            "0.7   3\n",
            "4   2\n",
            "0.7   4\n",
            "5   3\n",
            "0.7   5\n",
            "5   3\n",
            "0.7   6\n",
            "5   3\n",
            "0.7   7\n",
            "6   4\n",
            "0.7   8\n",
            "7   5\n",
            "0.7   9\n",
            "8   6\n",
            "0.7   10\n",
            "9   7\n",
            "0.7   11\n",
            "9   7\n",
            "0.7   12\n",
            "10   8\n",
            "0.7   13\n",
            "10   8\n",
            "0.7   14\n",
            "11   9\n",
            "0.7   15\n",
            "12   10\n",
            "0.7   16\n",
            "12   10\n",
            "0.7   17\n",
            "13   11\n",
            "0.7   18\n",
            "14   11\n",
            "0.7   19\n",
            "14   11\n",
            "0.7   20\n",
            "15   12\n",
            "0.7   21\n",
            "16   12\n",
            "0.7   22\n",
            "17   13\n",
            "0.7   23\n",
            "17   13\n",
            "0.7   24\n",
            "18   13\n",
            "0.7   25\n",
            "19   14\n",
            "0.7   26\n",
            "20   15\n",
            "0.7   27\n",
            "21   16\n",
            "0.7   28\n",
            "21   16\n",
            "0.7   29\n",
            "21   16\n",
            "0.7   30\n",
            "21   16\n",
            "0.7   31\n",
            "22   17\n",
            "0.7   32\n",
            "22   17\n",
            "0.7   33\n",
            "23   18\n",
            "0.7   34\n",
            "23   18\n",
            "0.7   35\n",
            "24   19\n",
            "0.7   36\n",
            "25   20\n",
            "0.7   37\n",
            "25   21\n",
            "0.7   38\n",
            "25   21\n",
            "0.7   39\n",
            "25   21\n",
            "0.7   40\n",
            "26   22\n",
            "0.7   41\n",
            "26   22\n",
            "0.7   42\n",
            "27   23\n",
            "0.7   43\n",
            "28   24\n",
            "0.7   44\n",
            "29   24\n",
            "0.7   45\n",
            "29   24\n",
            "0.7   46\n",
            "30   25\n",
            "0.7   47\n",
            "30   25\n",
            "0.7   48\n",
            "30   25\n",
            "0.7   49\n",
            "30   25\n",
            "0.7   50\n",
            "31   25\n",
            "0.7   51\n",
            "32   25\n",
            "0.7   52\n",
            "33   26\n",
            "0.7   53\n",
            "34   27\n",
            "0.7   54\n",
            "34   27\n",
            "0.7   55\n",
            "34   27\n",
            "0.7   56\n",
            "34   27\n",
            "0.7   57\n",
            "35   28\n",
            "0.7   58\n",
            "36   29\n",
            "0.7   59\n",
            "36   30\n",
            "0.7   60\n",
            "36   30\n",
            "0.7   61\n",
            "36   30\n",
            "0.7   62\n",
            "36   30\n",
            "0.7   63\n",
            "37   31\n",
            "0.7   64\n",
            "38   32\n",
            "0.7   65\n",
            "39   33\n",
            "0.7   66\n",
            "39   33\n",
            "0.7   67\n",
            "39   33\n",
            "0.7   68\n",
            "40   34\n",
            "0.7   69\n",
            "40   34\n",
            "0.7   70\n",
            "40   34\n",
            "0.7   71\n",
            "40   34\n",
            "0.7   72\n",
            "41   35\n",
            "0.7   73\n",
            "41   35\n",
            "0.7   74\n",
            "42   36\n",
            "0.7   75\n",
            "42   36\n",
            "0.7   76\n",
            "43   36\n",
            "0.7   77\n",
            "44   36\n",
            "0.7   78\n",
            "45   37\n",
            "0.7   79\n",
            "45   37\n",
            "0.7   80\n",
            "45   37\n",
            "0.7   81\n",
            "46   38\n",
            "0.7   82\n",
            "46   38\n",
            "0.7   83\n",
            "46   38\n",
            "0.7   84\n",
            "47   39\n",
            "0.7   85\n",
            "48   40\n",
            "0.7   86\n",
            "49   41\n",
            "0.7   87\n",
            "50   41\n",
            "0.7   88\n",
            "50   41\n",
            "0.7   89\n",
            "50   41\n",
            "0.7   90\n",
            "51   42\n",
            "0.7   91\n",
            "52   42\n",
            "0.7   92\n",
            "53   43\n",
            "0.7   93\n",
            "53   43\n",
            "0.7   94\n",
            "54   43\n",
            "0.7   95\n",
            "55   44\n",
            "0.7   96\n",
            "56   45\n",
            "0.7   97\n",
            "57   46\n",
            "0.7   98\n",
            "57   46\n",
            "0.7   99\n",
            "57   46\n",
            "0.7   100\n",
            "58   46\n",
            "0.7   101\n",
            "59   46\n",
            "0.7   102\n",
            "60   47\n",
            "0.7   103\n",
            "60   47\n",
            "0.7   104\n",
            "60   47\n",
            "0.7   105\n",
            "60   47\n",
            "0.7   106\n",
            "61   48\n",
            "0.7   107\n",
            "61   48\n",
            "0.7   108\n",
            "61   48\n",
            "0.7   109\n",
            "62   49\n",
            "0.7   110\n",
            "62   50\n",
            "0.7   111\n",
            "63   51\n",
            "0.7   112\n",
            "64   52\n",
            "0.7   113\n",
            "65   52\n",
            "0.7   114\n",
            "65   52\n",
            "0.7   115\n",
            "66   53\n",
            "0.7   116\n",
            "67   54\n",
            "0.7   117\n",
            "68   55\n",
            "0.7   118\n",
            "69   56\n",
            "0.7   119\n",
            "70   57\n",
            "0.7   120\n",
            "70   57\n",
            "0.7   121\n",
            "71   57\n",
            "0.7   122\n",
            "71   57\n",
            "0.7   123\n",
            "71   57\n",
            "0.7   124\n",
            "72   58\n",
            "0.7   125\n",
            "72   59\n",
            "0.7   126\n",
            "72   59\n",
            "0.7   127\n",
            "73   60\n",
            "0.7   128\n",
            "73   60\n",
            "0.7   129\n",
            "74   61\n",
            "0.7   130\n",
            "75   62\n",
            "0.7   131\n",
            "75   62\n",
            "0.7   132\n",
            "75   62\n",
            "0.7   133\n",
            "75   62\n",
            "0.7   134\n",
            "76   63\n",
            "0.7   135\n",
            "77   63\n",
            "0.7   136\n",
            "77   63\n",
            "0.7   137\n",
            "77   63\n",
            "0.7   138\n",
            "78   64\n",
            "0.7   139\n",
            "78   64\n",
            "0.7   140\n",
            "79   65\n",
            "0.7   141\n",
            "80   66\n",
            "0.7   142\n",
            "80   66\n",
            "0.7   143\n",
            "81   67\n",
            "0.7   144\n",
            "81   67\n",
            "0.7   145\n",
            "82   67\n",
            "0.7   146\n",
            "83   67\n",
            "0.7   147\n",
            "83   67\n",
            "0.7   148\n",
            "83   67\n",
            "0.7   149\n",
            "84   68\n",
            "0.7   150\n",
            "84   68\n",
            "0.7   151\n",
            "85   69\n",
            "0.7   152\n",
            "85   69\n",
            "0.7   153\n",
            "86   69\n",
            "0.7   154\n",
            "87   69\n",
            "0.7   155\n",
            "88   70\n",
            "0.7   156\n",
            "89   71\n",
            "0.7   157\n",
            "89   71\n",
            "0.7   158\n",
            "90   72\n",
            "0.7   159\n",
            "90   72\n",
            "0.7   160\n",
            "91   73\n",
            "0.7   161\n",
            "91   73\n",
            "0.7   162\n",
            "91   74\n",
            "0.7   163\n",
            "91   74\n",
            "0.7   164\n",
            "92   75\n",
            "0.7   165\n",
            "93   76\n",
            "0.7   166\n",
            "94   77\n",
            "0.7   167\n",
            "95   78\n",
            "0.7   168\n",
            "96   79\n",
            "0.7   169\n",
            "96   79\n",
            "0.7   170\n",
            "96   79\n",
            "0.7   171\n",
            "97   80\n",
            "0.7   172\n",
            "98   81\n",
            "0.7   173\n",
            "99   82\n",
            "0.7   174\n",
            "100   83\n",
            "0.7   175\n",
            "100   83\n",
            "0.7   176\n",
            "100   83\n",
            "0.7   177\n",
            "101   84\n",
            "0.7   178\n",
            "102   85\n",
            "0.7   179\n",
            "103   86\n",
            "0.7   180\n",
            "104   87\n",
            "0.7   181\n",
            "105   88\n",
            "0.7   182\n",
            "105   88\n",
            "0.7   183\n",
            "106   89\n",
            "0.7   184\n",
            "107   89\n",
            "0.7   185\n",
            "107   89\n",
            "0.7   186\n",
            "108   90\n",
            "0.7   187\n",
            "108   90\n",
            "0.7   188\n",
            "109   91\n",
            "0.7   189\n",
            "110   92\n",
            "0.7   190\n",
            "110   92\n",
            "0.7   191\n",
            "110   92\n",
            "0.7   192\n",
            "111   93\n",
            "0.7   193\n",
            "112   94\n",
            "0.7   194\n",
            "112   94\n",
            "0.7   195\n",
            "113   95\n",
            "0.7   196\n",
            "113   95\n",
            "0.7   197\n",
            "114   95\n",
            "0.7   198\n",
            "115   96\n",
            "0.7   199\n",
            "115   96\n",
            "Epsilon value -  0.7999999999999999\n",
            "\n",
            "0.7999999999999999   0\n",
            "1   0\n",
            "0.7999999999999999   1\n",
            "2   0\n",
            "0.7999999999999999   2\n",
            "3   1\n",
            "0.7999999999999999   3\n",
            "4   2\n",
            "0.7999999999999999   4\n",
            "5   3\n",
            "0.7999999999999999   5\n",
            "5   3\n",
            "0.7999999999999999   6\n",
            "5   3\n",
            "0.7999999999999999   7\n",
            "6   4\n",
            "0.7999999999999999   8\n",
            "7   5\n",
            "0.7999999999999999   9\n",
            "8   6\n",
            "0.7999999999999999   10\n",
            "9   7\n",
            "0.7999999999999999   11\n",
            "9   7\n",
            "0.7999999999999999   12\n",
            "10   8\n",
            "0.7999999999999999   13\n",
            "10   8\n",
            "0.7999999999999999   14\n",
            "11   9\n",
            "0.7999999999999999   15\n",
            "12   10\n",
            "0.7999999999999999   16\n",
            "12   10\n",
            "0.7999999999999999   17\n",
            "13   11\n",
            "0.7999999999999999   18\n",
            "14   11\n",
            "0.7999999999999999   19\n",
            "14   11\n",
            "0.7999999999999999   20\n",
            "15   12\n",
            "0.7999999999999999   21\n",
            "16   12\n",
            "0.7999999999999999   22\n",
            "17   13\n",
            "0.7999999999999999   23\n",
            "17   13\n",
            "0.7999999999999999   24\n",
            "18   13\n",
            "0.7999999999999999   25\n",
            "19   14\n",
            "0.7999999999999999   26\n",
            "20   15\n",
            "0.7999999999999999   27\n",
            "21   16\n",
            "0.7999999999999999   28\n",
            "21   16\n",
            "0.7999999999999999   29\n",
            "21   16\n",
            "0.7999999999999999   30\n",
            "21   16\n",
            "0.7999999999999999   31\n",
            "22   17\n",
            "0.7999999999999999   32\n",
            "22   17\n",
            "0.7999999999999999   33\n",
            "23   18\n",
            "0.7999999999999999   34\n",
            "23   18\n",
            "0.7999999999999999   35\n",
            "24   19\n",
            "0.7999999999999999   36\n",
            "25   20\n",
            "0.7999999999999999   37\n",
            "25   21\n",
            "0.7999999999999999   38\n",
            "25   21\n",
            "0.7999999999999999   39\n",
            "25   21\n",
            "0.7999999999999999   40\n",
            "26   22\n",
            "0.7999999999999999   41\n",
            "26   22\n",
            "0.7999999999999999   42\n",
            "27   23\n",
            "0.7999999999999999   43\n",
            "28   24\n",
            "0.7999999999999999   44\n",
            "29   24\n",
            "0.7999999999999999   45\n",
            "29   24\n",
            "0.7999999999999999   46\n",
            "30   25\n",
            "0.7999999999999999   47\n",
            "30   25\n",
            "0.7999999999999999   48\n",
            "30   25\n",
            "0.7999999999999999   49\n",
            "30   25\n",
            "0.7999999999999999   50\n",
            "31   25\n",
            "0.7999999999999999   51\n",
            "32   26\n",
            "0.7999999999999999   52\n",
            "33   27\n",
            "0.7999999999999999   53\n",
            "34   28\n",
            "0.7999999999999999   54\n",
            "34   28\n",
            "0.7999999999999999   55\n",
            "34   28\n",
            "0.7999999999999999   56\n",
            "34   28\n",
            "0.7999999999999999   57\n",
            "35   29\n",
            "0.7999999999999999   58\n",
            "36   30\n",
            "0.7999999999999999   59\n",
            "36   31\n",
            "0.7999999999999999   60\n",
            "36   31\n",
            "0.7999999999999999   61\n",
            "36   31\n",
            "0.7999999999999999   62\n",
            "36   31\n",
            "0.7999999999999999   63\n",
            "37   32\n",
            "0.7999999999999999   64\n",
            "38   33\n",
            "0.7999999999999999   65\n",
            "39   34\n",
            "0.7999999999999999   66\n",
            "39   34\n",
            "0.7999999999999999   67\n",
            "39   34\n",
            "0.7999999999999999   68\n",
            "40   35\n",
            "0.7999999999999999   69\n",
            "40   35\n",
            "0.7999999999999999   70\n",
            "40   35\n",
            "0.7999999999999999   71\n",
            "40   35\n",
            "0.7999999999999999   72\n",
            "41   36\n",
            "0.7999999999999999   73\n",
            "41   36\n",
            "0.7999999999999999   74\n",
            "42   37\n",
            "0.7999999999999999   75\n",
            "42   37\n",
            "0.7999999999999999   76\n",
            "43   38\n",
            "0.7999999999999999   77\n",
            "44   38\n",
            "0.7999999999999999   78\n",
            "45   39\n",
            "0.7999999999999999   79\n",
            "45   39\n",
            "0.7999999999999999   80\n",
            "45   39\n",
            "0.7999999999999999   81\n",
            "46   40\n",
            "0.7999999999999999   82\n",
            "46   40\n",
            "0.7999999999999999   83\n",
            "46   40\n",
            "0.7999999999999999   84\n",
            "47   41\n",
            "0.7999999999999999   85\n",
            "48   42\n",
            "0.7999999999999999   86\n",
            "49   43\n",
            "0.7999999999999999   87\n",
            "50   43\n",
            "0.7999999999999999   88\n",
            "50   43\n",
            "0.7999999999999999   89\n",
            "50   43\n",
            "0.7999999999999999   90\n",
            "51   44\n",
            "0.7999999999999999   91\n",
            "52   45\n",
            "0.7999999999999999   92\n",
            "53   46\n",
            "0.7999999999999999   93\n",
            "53   46\n",
            "0.7999999999999999   94\n",
            "54   46\n",
            "0.7999999999999999   95\n",
            "55   47\n",
            "0.7999999999999999   96\n",
            "56   48\n",
            "0.7999999999999999   97\n",
            "57   49\n",
            "0.7999999999999999   98\n",
            "57   49\n",
            "0.7999999999999999   99\n",
            "57   49\n",
            "0.7999999999999999   100\n",
            "58   50\n",
            "0.7999999999999999   101\n",
            "59   51\n",
            "0.7999999999999999   102\n",
            "60   52\n",
            "0.7999999999999999   103\n",
            "60   52\n",
            "0.7999999999999999   104\n",
            "60   53\n",
            "0.7999999999999999   105\n",
            "60   53\n",
            "0.7999999999999999   106\n",
            "61   54\n",
            "0.7999999999999999   107\n",
            "61   54\n",
            "0.7999999999999999   108\n",
            "61   54\n",
            "0.7999999999999999   109\n",
            "62   55\n",
            "0.7999999999999999   110\n",
            "62   56\n",
            "0.7999999999999999   111\n",
            "63   57\n",
            "0.7999999999999999   112\n",
            "64   58\n",
            "0.7999999999999999   113\n",
            "65   58\n",
            "0.7999999999999999   114\n",
            "65   58\n",
            "0.7999999999999999   115\n",
            "66   59\n",
            "0.7999999999999999   116\n",
            "67   60\n",
            "0.7999999999999999   117\n",
            "68   61\n",
            "0.7999999999999999   118\n",
            "69   62\n",
            "0.7999999999999999   119\n",
            "70   63\n",
            "0.7999999999999999   120\n",
            "70   63\n",
            "0.7999999999999999   121\n",
            "71   63\n",
            "0.7999999999999999   122\n",
            "71   63\n",
            "0.7999999999999999   123\n",
            "71   63\n",
            "0.7999999999999999   124\n",
            "72   64\n",
            "0.7999999999999999   125\n",
            "72   65\n",
            "0.7999999999999999   126\n",
            "72   65\n",
            "0.7999999999999999   127\n",
            "73   66\n",
            "0.7999999999999999   128\n",
            "73   66\n",
            "0.7999999999999999   129\n",
            "74   67\n",
            "0.7999999999999999   130\n",
            "75   68\n",
            "0.7999999999999999   131\n",
            "75   68\n",
            "0.7999999999999999   132\n",
            "75   68\n",
            "0.7999999999999999   133\n",
            "75   68\n",
            "0.7999999999999999   134\n",
            "76   69\n",
            "0.7999999999999999   135\n",
            "77   69\n",
            "0.7999999999999999   136\n",
            "77   69\n",
            "0.7999999999999999   137\n",
            "77   69\n",
            "0.7999999999999999   138\n",
            "78   70\n",
            "0.7999999999999999   139\n",
            "78   70\n",
            "0.7999999999999999   140\n",
            "79   71\n",
            "0.7999999999999999   141\n",
            "80   72\n",
            "0.7999999999999999   142\n",
            "80   72\n",
            "0.7999999999999999   143\n",
            "81   73\n",
            "0.7999999999999999   144\n",
            "81   73\n",
            "0.7999999999999999   145\n",
            "82   73\n",
            "0.7999999999999999   146\n",
            "83   73\n",
            "0.7999999999999999   147\n",
            "83   73\n",
            "0.7999999999999999   148\n",
            "83   73\n",
            "0.7999999999999999   149\n",
            "84   74\n",
            "0.7999999999999999   150\n",
            "84   74\n",
            "0.7999999999999999   151\n",
            "85   75\n",
            "0.7999999999999999   152\n",
            "85   75\n",
            "0.7999999999999999   153\n",
            "86   75\n",
            "0.7999999999999999   154\n",
            "87   75\n",
            "0.7999999999999999   155\n",
            "88   76\n",
            "0.7999999999999999   156\n",
            "89   77\n",
            "0.7999999999999999   157\n",
            "89   77\n",
            "0.7999999999999999   158\n",
            "90   78\n",
            "0.7999999999999999   159\n",
            "90   78\n",
            "0.7999999999999999   160\n",
            "91   79\n",
            "0.7999999999999999   161\n",
            "91   79\n",
            "0.7999999999999999   162\n",
            "91   80\n",
            "0.7999999999999999   163\n",
            "91   80\n",
            "0.7999999999999999   164\n",
            "92   81\n",
            "0.7999999999999999   165\n",
            "93   82\n",
            "0.7999999999999999   166\n",
            "94   83\n",
            "0.7999999999999999   167\n",
            "95   84\n",
            "0.7999999999999999   168\n",
            "96   85\n",
            "0.7999999999999999   169\n",
            "96   85\n",
            "0.7999999999999999   170\n",
            "96   85\n",
            "0.7999999999999999   171\n",
            "97   86\n",
            "0.7999999999999999   172\n",
            "98   87\n",
            "0.7999999999999999   173\n",
            "99   88\n",
            "0.7999999999999999   174\n",
            "100   89\n",
            "0.7999999999999999   175\n",
            "100   89\n",
            "0.7999999999999999   176\n",
            "100   89\n",
            "0.7999999999999999   177\n",
            "101   90\n",
            "0.7999999999999999   178\n",
            "102   91\n",
            "0.7999999999999999   179\n",
            "103   92\n",
            "0.7999999999999999   180\n",
            "104   93\n",
            "0.7999999999999999   181\n",
            "105   94\n",
            "0.7999999999999999   182\n",
            "105   94\n",
            "0.7999999999999999   183\n",
            "106   95\n",
            "0.7999999999999999   184\n",
            "107   95\n",
            "0.7999999999999999   185\n",
            "107   95\n",
            "0.7999999999999999   186\n",
            "108   96\n",
            "0.7999999999999999   187\n",
            "108   96\n",
            "0.7999999999999999   188\n",
            "109   97\n",
            "0.7999999999999999   189\n",
            "110   98\n",
            "0.7999999999999999   190\n",
            "110   98\n",
            "0.7999999999999999   191\n",
            "110   98\n",
            "0.7999999999999999   192\n",
            "111   99\n",
            "0.7999999999999999   193\n",
            "112   100\n",
            "0.7999999999999999   194\n",
            "112   100\n",
            "0.7999999999999999   195\n",
            "113   101\n",
            "0.7999999999999999   196\n",
            "113   101\n",
            "0.7999999999999999   197\n",
            "114   101\n",
            "0.7999999999999999   198\n",
            "115   102\n",
            "0.7999999999999999   199\n",
            "115   102\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for key in result.keys():\n",
        "  print(key,' ',result[key])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqfPLIoq1PxA",
        "outputId": "f7f0629f-420f-4d75-d636-e6b32619288e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.1   (33, 44)\n",
            "0.2   (49, 78)\n",
            "0.30000000000000004   (63, 107)\n",
            "0.4   (72, 115)\n",
            "0.5   (80, 115)\n",
            "0.6   (90, 115)\n",
            "0.7   (96, 115)\n",
            "0.7999999999999999   (102, 115)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "koOoiXzx6OsD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}