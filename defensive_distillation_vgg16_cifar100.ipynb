{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "defensive_distillation_vgg16_cifar100.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPRc4AGWKvKtmiWI4WMesEO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kirath2205/Final-Year-Project/blob/main/defensive_distillation_vgg16_cifar100.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "c9Vpz-objd13"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Input, UpSampling2D, Flatten, BatchNormalization, Dense, Dropout, GlobalAveragePooling2D , UpSampling3D , Lambda , Conv2D ,MaxPooling2D\n",
        "from tensorflow.keras import optimizers\n",
        "from keras.datasets import cifar100,cifar10,fashion_mnist\n",
        "import tensorflow as tf\n",
        "from keras.utils import np_utils\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from skimage.transform import resize\n",
        "from keras import applications\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50 , preprocess_input , decode_predictions\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras import regularizers\n",
        "tf.keras.backend.clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "device_lib.list_local_devices()\n",
        "tf.test.gpu_device_name()\n"
      ],
      "metadata": {
        "id": "FNOkdguAen-8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c21e2425-5599-4226-90fd-ad119909ef62"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Distiller(keras.Model):\n",
        "    def __init__(self, student, teacher):\n",
        "        super(Distiller, self).__init__()\n",
        "        self.teacher = teacher\n",
        "        self.student = student\n",
        "\n",
        "    def compile(\n",
        "        self,\n",
        "        optimizer,\n",
        "        metrics,\n",
        "        student_loss_fn,\n",
        "        distillation_loss_fn,\n",
        "        alpha=0.2,\n",
        "        temperature=20,\n",
        "    ):\n",
        "        \"\"\" Configure the distiller.\n",
        "\n",
        "        Args:\n",
        "            optimizer: Keras optimizer for the student weights\n",
        "            metrics: Keras metrics for evaluation\n",
        "            student_loss_fn: Loss function of difference between student\n",
        "                predictions and ground-truth\n",
        "            distillation_loss_fn: Loss function of difference between soft\n",
        "                student predictions and soft teacher predictions\n",
        "            alpha: weight to student_loss_fn and 1-alpha to distillation_loss_fn\n",
        "            temperature: Temperature for softening probability distributions.\n",
        "                Larger temperature gives softer distributions.\n",
        "        \"\"\"\n",
        "        super(Distiller, self).compile(optimizer=optimizer, metrics=metrics)\n",
        "        self.student_loss_fn = student_loss_fn\n",
        "        self.distillation_loss_fn = distillation_loss_fn\n",
        "        self.alpha = alpha\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def train_step(self, data):\n",
        "        # Unpack data\n",
        "        x, y = data\n",
        "\n",
        "        # Forward pass of teacher\n",
        "        teacher_predictions = self.teacher(x, training=False)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Forward pass of student\n",
        "            student_predictions = self.student(x, training=True)\n",
        "\n",
        "            # Compute losses\n",
        "            student_loss = self.student_loss_fn(y, student_predictions)\n",
        "            distillation_loss = self.distillation_loss_fn(\n",
        "                tf.nn.softmax(teacher_predictions / self.temperature, axis=1),\n",
        "                tf.nn.softmax(student_predictions / self.temperature, axis=1),\n",
        "            )\n",
        "            loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n",
        "\n",
        "        # Compute gradients\n",
        "        trainable_vars = self.student.trainable_variables\n",
        "        gradients = tape.gradient(loss, trainable_vars)\n",
        "\n",
        "        # Update weights\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "\n",
        "        # Update the metrics configured in `compile()`.\n",
        "        self.compiled_metrics.update_state(y, student_predictions)\n",
        "\n",
        "        # Return a dict of performance\n",
        "        results = {m.name: m.result() for m in self.metrics}\n",
        "        results.update(\n",
        "            {\"student_loss\": student_loss, \"distillation_loss\": distillation_loss}\n",
        "        )\n",
        "        return results\n",
        "\n",
        "    def test_step(self, data):\n",
        "        # Unpack the data\n",
        "        x, y = data\n",
        "\n",
        "        # Compute predictions\n",
        "        y_prediction = self.student(x, training=False)\n",
        "\n",
        "        # Calculate the loss\n",
        "        student_loss = self.student_loss_fn(y, y_prediction)\n",
        "\n",
        "        # Update the metrics.\n",
        "        self.compiled_metrics.update_state(y, y_prediction)\n",
        "\n",
        "        # Return a dict of performance\n",
        "        results = {m.name: m.result() for m in self.metrics}\n",
        "        results.update({\"student_loss\": student_loss})\n",
        "        return results\n",
        "\n",
        "    def call(self, inputs, *args, **kwargs):\n",
        "       return self.student(inputs)"
      ],
      "metadata": {
        "id": "3vClqBLTkEoT"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_random_eraser(p=0.5, s_l=0.02, s_h=0.4, r_1=0.3, r_2=1/0.3, v_l=0, v_h=255, pixel_level=False):\n",
        "    def eraser(input_img):\n",
        "        img_h, img_w, img_c = input_img.shape\n",
        "        p_1 = np.random.rand()\n",
        "\n",
        "        if p_1 > p:\n",
        "            return input_img\n",
        "\n",
        "        while True:\n",
        "            s = np.random.uniform(s_l, s_h) * img_h * img_w\n",
        "            r = np.random.uniform(r_1, r_2)\n",
        "            w = int(np.sqrt(s / r))\n",
        "            h = int(np.sqrt(s * r))\n",
        "            left = np.random.randint(0, img_w)\n",
        "            top = np.random.randint(0, img_h)\n",
        "\n",
        "            if left + w <= img_w and top + h <= img_h:\n",
        "                break\n",
        "\n",
        "        if pixel_level:\n",
        "            c = np.random.uniform(v_l, v_h, (h, w, img_c))\n",
        "        else:\n",
        "            c = np.random.uniform(v_l, v_h)\n",
        "\n",
        "        input_img[top:top + h, left:left + w, :] = c\n",
        "\n",
        "        return input_img\n",
        "\n",
        "    return eraser"
      ],
      "metadata": {
        "id": "_4ac45bEkIq9"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def select_dataset(index=1): #1 for cifar10 , 2 for cifar100 , 3 for fashion mnist\n",
        "  if(index == 1):\n",
        "    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "    channel = 3\n",
        "    num_classes = 10\n",
        "  if(index == 2):\n",
        "    (x_train, y_train), (x_test, y_test) = cifar100.load_data()\n",
        "    num_classes = 100\n",
        "    channel = 3\n",
        "  if(index == 3):\n",
        "    (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "    x_train = x_train.reshape((60000, 28, 28, 1))\n",
        "    x_test =  x_test.reshape((10000, 28, 28, 1))\n",
        "    num_classes = 10\n",
        "    y_train = np_utils.to_categorical(y_train, num_classes)\n",
        "    y_test = np_utils.to_categorical(y_test, num_classes)\n",
        "    channel = 1\n",
        "    return (x_train , y_train , x_test , y_test , num_classes , channel)\n",
        "\n",
        "  #Pre-process the data\n",
        "  x_train = preprocess_input(x_train)\n",
        "  x_test = preprocess_input(x_test)\n",
        "  datagen = ImageDataGenerator(preprocessing_function=get_random_eraser(v_l=0, v_h=1, pixel_level=True))\n",
        "  datagen.fit(x_train)\n",
        "  y_train = np_utils.to_categorical(y_train, num_classes)\n",
        "  y_test = np_utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "  return (x_train , y_train , x_test , y_test , num_classes , channel , datagen)"
      ],
      "metadata": {
        "id": "Ds4lOH1qmZpq"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = 1\n",
        "x_train , y_train , x_test , y_test , num_classes ,channel , datagen = select_dataset(index)\n",
        "model_name = 'Desktop/Trained_models/defensive_distillation_keras_vgg16_cifar10'\n",
        "model_path = 'Desktop/Trained_models/defensive_distillation_keras_vgg16_cifar10.h5'"
      ],
      "metadata": {
        "id": "8TbyWALmnNDC"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = 2\n",
        "x_train , y_train , x_test , y_test , num_classes ,channel , datagen = select_dataset(index)\n",
        "model_name = 'Desktop/Trained_models/defensive_distillation_keras_vgg16_cifar100'\n",
        "model_path = 'Desktop/Trained_models/defensive_distillation_keras_vgg16_cifar100.h5'"
      ],
      "metadata": {
        "id": "2lRB-7LZncsd"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp = 200\n",
        "weight_decay = 0.0005\n",
        "if(index==1 or index==2):\n",
        "  teacher = keras.Sequential(\n",
        "      [\n",
        "        layers.UpSampling2D((5,5)),\n",
        "        layers.Conv2D(64,3,padding=\"same\",kernel_regularizer=regularizers.l2(weight_decay),input_shape = (160,160,3)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.ReLU(),\n",
        "        layers.Conv2D(64,3,padding=\"same\",kernel_regularizer=regularizers.l2(weight_decay)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.ReLU(),\n",
        "        layers.MaxPooling2D((2,2)),\n",
        "        layers.Dropout(0.25),\n",
        "        layers.Conv2D(128,3,padding=\"same\",kernel_regularizer=regularizers.l2(weight_decay)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.ReLU(),\n",
        "        layers.Conv2D(128,3,padding=\"same\",kernel_regularizer=regularizers.l2(weight_decay)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.ReLU(),\n",
        "        layers.MaxPooling2D((2,2)),\n",
        "        layers.Dropout(0.25),\n",
        "        layers.Conv2D(256,3,padding=\"same\",kernel_regularizer=regularizers.l2(weight_decay)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.ReLU(),\n",
        "        layers.Conv2D(256,3,padding=\"same\",kernel_regularizer=regularizers.l2(weight_decay)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.ReLU(),\n",
        "        layers.Conv2D(256,3,padding=\"same\",kernel_regularizer=regularizers.l2(weight_decay)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.ReLU(),\n",
        "        layers.MaxPooling2D((2,2)),\n",
        "        layers.Dropout(0.25),\n",
        "        layers.Conv2D(512,3,padding=\"same\",kernel_regularizer=regularizers.l2(weight_decay)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.ReLU(),\n",
        "        layers.Conv2D(512,3,padding=\"same\",kernel_regularizer=regularizers.l2(weight_decay)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.ReLU(),\n",
        "        layers.Conv2D(512,3,padding=\"same\",kernel_regularizer=regularizers.l2(weight_decay)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.ReLU(),\n",
        "        layers.MaxPooling2D((2,2)),\n",
        "        layers.Dropout(0.25),\n",
        "        layers.Conv2D(512,3,padding=\"same\",kernel_regularizer=regularizers.l2(weight_decay)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.ReLU(),\n",
        "        layers.Conv2D(512,3,padding=\"same\",kernel_regularizer=regularizers.l2(weight_decay)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.ReLU(),\n",
        "        layers.Conv2D(512,3,padding=\"same\",kernel_regularizer=regularizers.l2(weight_decay)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.ReLU(),\n",
        "        layers.MaxPooling2D((2,2)),\n",
        "        layers.Dropout(0.25),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(512,kernel_regularizer=regularizers.l2(weight_decay)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.ReLU(),\n",
        "        layers.Dropout(0.25),\n",
        "        layers.Dense(num_classes),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Lambda((lambda x: x / temp)),\n",
        "      ],\n",
        "      name=\"teacher\",\n",
        "  )\n",
        "\n",
        "# Create the student\n",
        "  student = keras.Sequential(\n",
        "      [\n",
        "        layers.UpSampling2D((5,5)),\n",
        "        layers.Conv2D(64,3,padding=\"same\",kernel_regularizer=regularizers.l2(weight_decay),input_shape = (160,160,3)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.ReLU(),\n",
        "        layers.Conv2D(64,3,padding=\"same\",kernel_regularizer=regularizers.l2(weight_decay)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.ReLU(),\n",
        "        layers.MaxPooling2D((2,2)),\n",
        "        layers.Dropout(0.25),\n",
        "        layers.Conv2D(128,3,padding=\"same\",kernel_regularizer=regularizers.l2(weight_decay)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.ReLU(),\n",
        "        layers.Conv2D(128,3,padding=\"same\",kernel_regularizer=regularizers.l2(weight_decay)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.ReLU(),\n",
        "        layers.MaxPooling2D((2,2)),\n",
        "        layers.Dropout(0.25),\n",
        "        layers.Conv2D(256,3,padding=\"same\",kernel_regularizer=regularizers.l2(weight_decay)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.ReLU(),\n",
        "        layers.Conv2D(256,3,padding=\"same\",kernel_regularizer=regularizers.l2(weight_decay)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.ReLU(),\n",
        "        layers.Conv2D(256,3,padding=\"same\",kernel_regularizer=regularizers.l2(weight_decay)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.ReLU(),\n",
        "        layers.MaxPooling2D((2,2)),\n",
        "        layers.Dropout(0.25),\n",
        "        layers.Conv2D(512,3,padding=\"same\",kernel_regularizer=regularizers.l2(weight_decay)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.ReLU(),\n",
        "        layers.Conv2D(512,3,padding=\"same\",kernel_regularizer=regularizers.l2(weight_decay)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.ReLU(),\n",
        "        layers.Conv2D(512,3,padding=\"same\",kernel_regularizer=regularizers.l2(weight_decay)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.ReLU(),\n",
        "        layers.MaxPooling2D((2,2)),\n",
        "        layers.Dropout(0.25),\n",
        "        layers.Conv2D(512,3,padding=\"same\",kernel_regularizer=regularizers.l2(weight_decay)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.ReLU(),\n",
        "        layers.Conv2D(512,3,padding=\"same\",kernel_regularizer=regularizers.l2(weight_decay)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.ReLU(),\n",
        "        layers.Conv2D(512,3,padding=\"same\",kernel_regularizer=regularizers.l2(weight_decay)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.ReLU(),\n",
        "        layers.MaxPooling2D((2,2)),\n",
        "        layers.Dropout(0.25),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(512,kernel_regularizer=regularizers.l2(weight_decay)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.ReLU(),\n",
        "        layers.Dropout(0.25),\n",
        "        layers.Dense(num_classes),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Lambda((lambda x: x / temp)),\n",
        "      ],\n",
        "      name=\"student\",\n",
        "  )\n"
      ],
      "metadata": {
        "id": "cMXAsAZXnp9y"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 250\n",
        "learning_rate = 0.1\n",
        "lr_decay = 1e-6\n",
        "lr_drop = 20\n",
        "callbacks = [ \n",
        "    tf.keras.callbacks.ModelCheckpoint(model_path, monitor='val_categorical_accuracy', verbose=1, save_best_only=True),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau( factor = 0.7, patience = 5, min_lr = 0.000001, verbose = 1,monitor='val_loss' )\n",
        "  ]\n",
        "batch_size = 32\n",
        "teacher.compile(\n",
        "    optimizer=keras.optimizers.SGD(lr=learning_rate, decay=lr_decay, momentum=0.9, nesterov=True),\n",
        "    loss=keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[keras.metrics.CategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "# Train and evaluate teacher on data.\n",
        "history_teacher = teacher.fit_generator(datagen.flow(x_train, y_train,\n",
        "                                  batch_size=batch_size),\n",
        "                                  steps_per_epoch=x_train.shape[0] // batch_size,\n",
        "                                  epochs=epochs,\n",
        "                                  validation_data=(x_test, y_test),callbacks=callbacks)\n",
        "teacher.evaluate(x_test, y_test)"
      ],
      "metadata": {
        "id": "pQUorMgFnyxf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dabfdfd0-23a3-4afb-ce78-a6f667bad1a5"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 5.3532 - categorical_accuracy: 0.0890\n",
            "Epoch 00001: val_categorical_accuracy improved from -inf to 0.05350, saving model to Desktop/Trained_models/defensive_distillation_keras_vgg16_cifar100.h5\n",
            "1562/1562 [==============================] - 179s 114ms/step - loss: 5.3532 - categorical_accuracy: 0.0890 - val_loss: 4.7103 - val_categorical_accuracy: 0.0535 - lr: 0.1000\n",
            "Epoch 2/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.6314 - categorical_accuracy: 0.1717\n",
            "Epoch 00002: val_categorical_accuracy improved from 0.05350 to 0.22450, saving model to Desktop/Trained_models/defensive_distillation_keras_vgg16_cifar100.h5\n",
            "1562/1562 [==============================] - 179s 115ms/step - loss: 4.6314 - categorical_accuracy: 0.1717 - val_loss: 4.6023 - val_categorical_accuracy: 0.2245 - lr: 0.1000\n",
            "Epoch 3/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.5984 - categorical_accuracy: 0.2262\n",
            "Epoch 00003: val_categorical_accuracy did not improve from 0.22450\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.5984 - categorical_accuracy: 0.2262 - val_loss: 4.5970 - val_categorical_accuracy: 0.2023 - lr: 0.1000\n",
            "Epoch 4/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.5949 - categorical_accuracy: 0.2541\n",
            "Epoch 00004: val_categorical_accuracy did not improve from 0.22450\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.5949 - categorical_accuracy: 0.2541 - val_loss: 4.5937 - val_categorical_accuracy: 0.2120 - lr: 0.1000\n",
            "Epoch 5/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.5929 - categorical_accuracy: 0.2654\n",
            "Epoch 00005: val_categorical_accuracy did not improve from 0.22450\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.5929 - categorical_accuracy: 0.2654 - val_loss: 4.5947 - val_categorical_accuracy: 0.1368 - lr: 0.1000\n",
            "Epoch 6/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.5909 - categorical_accuracy: 0.2712\n",
            "Epoch 00006: val_categorical_accuracy improved from 0.22450 to 0.23060, saving model to Desktop/Trained_models/defensive_distillation_keras_vgg16_cifar100.h5\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.5909 - categorical_accuracy: 0.2712 - val_loss: 4.5882 - val_categorical_accuracy: 0.2306 - lr: 0.1000\n",
            "Epoch 7/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.5890 - categorical_accuracy: 0.2748\n",
            "Epoch 00007: val_categorical_accuracy did not improve from 0.23060\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.5890 - categorical_accuracy: 0.2748 - val_loss: 4.5824 - val_categorical_accuracy: 0.2247 - lr: 0.1000\n",
            "Epoch 8/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.5872 - categorical_accuracy: 0.2716\n",
            "Epoch 00008: val_categorical_accuracy improved from 0.23060 to 0.26560, saving model to Desktop/Trained_models/defensive_distillation_keras_vgg16_cifar100.h5\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.5872 - categorical_accuracy: 0.2716 - val_loss: 4.5786 - val_categorical_accuracy: 0.2656 - lr: 0.1000\n",
            "Epoch 9/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.5852 - categorical_accuracy: 0.2726\n",
            "Epoch 00009: val_categorical_accuracy did not improve from 0.26560\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.5852 - categorical_accuracy: 0.2726 - val_loss: 4.5843 - val_categorical_accuracy: 0.2116 - lr: 0.1000\n",
            "Epoch 10/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.5832 - categorical_accuracy: 0.2763\n",
            "Epoch 00010: val_categorical_accuracy did not improve from 0.26560\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.5832 - categorical_accuracy: 0.2763 - val_loss: 4.5816 - val_categorical_accuracy: 0.2052 - lr: 0.1000\n",
            "Epoch 11/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.5814 - categorical_accuracy: 0.2709\n",
            "Epoch 00011: val_categorical_accuracy did not improve from 0.26560\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.5814 - categorical_accuracy: 0.2709 - val_loss: 4.5762 - val_categorical_accuracy: 0.2227 - lr: 0.1000\n",
            "Epoch 12/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.5793 - categorical_accuracy: 0.2722\n",
            "Epoch 00012: val_categorical_accuracy did not improve from 0.26560\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.5793 - categorical_accuracy: 0.2722 - val_loss: 4.5739 - val_categorical_accuracy: 0.2285 - lr: 0.1000\n",
            "Epoch 13/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.5774 - categorical_accuracy: 0.2747\n",
            "Epoch 00013: val_categorical_accuracy improved from 0.26560 to 0.30810, saving model to Desktop/Trained_models/defensive_distillation_keras_vgg16_cifar100.h5\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.5774 - categorical_accuracy: 0.2747 - val_loss: 4.5680 - val_categorical_accuracy: 0.3081 - lr: 0.1000\n",
            "Epoch 14/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.5756 - categorical_accuracy: 0.2715\n",
            "Epoch 00014: val_categorical_accuracy did not improve from 0.30810\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.5756 - categorical_accuracy: 0.2715 - val_loss: 4.5677 - val_categorical_accuracy: 0.2693 - lr: 0.1000\n",
            "Epoch 15/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.5735 - categorical_accuracy: 0.2718\n",
            "Epoch 00015: val_categorical_accuracy did not improve from 0.30810\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.5735 - categorical_accuracy: 0.2718 - val_loss: 4.5720 - val_categorical_accuracy: 0.1666 - lr: 0.1000\n",
            "Epoch 16/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.5716 - categorical_accuracy: 0.2756\n",
            "Epoch 00016: val_categorical_accuracy did not improve from 0.30810\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.5716 - categorical_accuracy: 0.2756 - val_loss: 4.5615 - val_categorical_accuracy: 0.2904 - lr: 0.1000\n",
            "Epoch 17/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.5698 - categorical_accuracy: 0.2693\n",
            "Epoch 00017: val_categorical_accuracy did not improve from 0.30810\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.5698 - categorical_accuracy: 0.2693 - val_loss: 4.5534 - val_categorical_accuracy: 0.2867 - lr: 0.1000\n",
            "Epoch 18/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.5676 - categorical_accuracy: 0.2723\n",
            "Epoch 00018: val_categorical_accuracy did not improve from 0.30810\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.5676 - categorical_accuracy: 0.2723 - val_loss: 4.5694 - val_categorical_accuracy: 0.2536 - lr: 0.1000\n",
            "Epoch 19/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.5657 - categorical_accuracy: 0.2716\n",
            "Epoch 00019: val_categorical_accuracy did not improve from 0.30810\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.5657 - categorical_accuracy: 0.2716 - val_loss: 4.5473 - val_categorical_accuracy: 0.3058 - lr: 0.1000\n",
            "Epoch 20/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.5639 - categorical_accuracy: 0.2720\n",
            "Epoch 00020: val_categorical_accuracy did not improve from 0.30810\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.5639 - categorical_accuracy: 0.2720 - val_loss: 4.5679 - val_categorical_accuracy: 0.2056 - lr: 0.1000\n",
            "Epoch 21/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.5621 - categorical_accuracy: 0.2721\n",
            "Epoch 00021: val_categorical_accuracy did not improve from 0.30810\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.5621 - categorical_accuracy: 0.2721 - val_loss: 4.5476 - val_categorical_accuracy: 0.2544 - lr: 0.1000\n",
            "Epoch 22/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.5602 - categorical_accuracy: 0.2707\n",
            "Epoch 00022: val_categorical_accuracy did not improve from 0.30810\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.5602 - categorical_accuracy: 0.2707 - val_loss: 4.5507 - val_categorical_accuracy: 0.2348 - lr: 0.1000\n",
            "Epoch 23/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.5586 - categorical_accuracy: 0.2701\n",
            "Epoch 00023: val_categorical_accuracy improved from 0.30810 to 0.32890, saving model to Desktop/Trained_models/defensive_distillation_keras_vgg16_cifar100.h5\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.5586 - categorical_accuracy: 0.2701 - val_loss: 4.5383 - val_categorical_accuracy: 0.3289 - lr: 0.1000\n",
            "Epoch 24/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.5563 - categorical_accuracy: 0.2738\n",
            "Epoch 00024: val_categorical_accuracy did not improve from 0.32890\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.5563 - categorical_accuracy: 0.2738 - val_loss: 4.5499 - val_categorical_accuracy: 0.2588 - lr: 0.1000\n",
            "Epoch 25/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.5548 - categorical_accuracy: 0.2711\n",
            "Epoch 00025: val_categorical_accuracy did not improve from 0.32890\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.5548 - categorical_accuracy: 0.2711 - val_loss: 4.5422 - val_categorical_accuracy: 0.2475 - lr: 0.1000\n",
            "Epoch 26/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.5532 - categorical_accuracy: 0.2685\n",
            "Epoch 00026: val_categorical_accuracy did not improve from 0.32890\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.5532 - categorical_accuracy: 0.2685 - val_loss: 4.5407 - val_categorical_accuracy: 0.2401 - lr: 0.1000\n",
            "Epoch 27/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.5510 - categorical_accuracy: 0.2714\n",
            "Epoch 00027: val_categorical_accuracy did not improve from 0.32890\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.5510 - categorical_accuracy: 0.2714 - val_loss: 4.5587 - val_categorical_accuracy: 0.1996 - lr: 0.1000\n",
            "Epoch 28/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.5495 - categorical_accuracy: 0.2712\n",
            "Epoch 00028: val_categorical_accuracy did not improve from 0.32890\n",
            "\n",
            "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.07000000104308128.\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.5495 - categorical_accuracy: 0.2712 - val_loss: 4.5390 - val_categorical_accuracy: 0.2890 - lr: 0.1000\n",
            "Epoch 29/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.5386 - categorical_accuracy: 0.2914\n",
            "Epoch 00029: val_categorical_accuracy did not improve from 0.32890\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.5386 - categorical_accuracy: 0.2914 - val_loss: 4.5276 - val_categorical_accuracy: 0.2376 - lr: 0.0700\n",
            "Epoch 30/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.5363 - categorical_accuracy: 0.2913\n",
            "Epoch 00030: val_categorical_accuracy did not improve from 0.32890\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.5363 - categorical_accuracy: 0.2913 - val_loss: 4.5075 - val_categorical_accuracy: 0.3014 - lr: 0.0700\n",
            "Epoch 31/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.5351 - categorical_accuracy: 0.2920\n",
            "Epoch 00031: val_categorical_accuracy did not improve from 0.32890\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.5351 - categorical_accuracy: 0.2920 - val_loss: 4.5469 - val_categorical_accuracy: 0.1923 - lr: 0.0700\n",
            "Epoch 32/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.5337 - categorical_accuracy: 0.2939\n",
            "Epoch 00032: val_categorical_accuracy improved from 0.32890 to 0.34720, saving model to Desktop/Trained_models/defensive_distillation_keras_vgg16_cifar100.h5\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.5337 - categorical_accuracy: 0.2939 - val_loss: 4.5028 - val_categorical_accuracy: 0.3472 - lr: 0.0700\n",
            "Epoch 33/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.5324 - categorical_accuracy: 0.2914\n",
            "Epoch 00033: val_categorical_accuracy did not improve from 0.34720\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.5324 - categorical_accuracy: 0.2914 - val_loss: 4.5257 - val_categorical_accuracy: 0.2677 - lr: 0.0700\n",
            "Epoch 34/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.5307 - categorical_accuracy: 0.2955\n",
            "Epoch 00034: val_categorical_accuracy did not improve from 0.34720\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.5307 - categorical_accuracy: 0.2955 - val_loss: 4.4985 - val_categorical_accuracy: 0.3115 - lr: 0.0700\n",
            "Epoch 35/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.5296 - categorical_accuracy: 0.2948\n",
            "Epoch 00035: val_categorical_accuracy did not improve from 0.34720\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.5296 - categorical_accuracy: 0.2948 - val_loss: 4.5274 - val_categorical_accuracy: 0.1723 - lr: 0.0700\n",
            "Epoch 36/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.5288 - categorical_accuracy: 0.2942\n",
            "Epoch 00036: val_categorical_accuracy did not improve from 0.34720\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.5288 - categorical_accuracy: 0.2942 - val_loss: 4.5192 - val_categorical_accuracy: 0.3273 - lr: 0.0700\n",
            "Epoch 37/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.5269 - categorical_accuracy: 0.2939\n",
            "Epoch 00037: val_categorical_accuracy did not improve from 0.34720\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.5269 - categorical_accuracy: 0.2939 - val_loss: 4.5294 - val_categorical_accuracy: 0.2302 - lr: 0.0700\n",
            "Epoch 38/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.5257 - categorical_accuracy: 0.2959\n",
            "Epoch 00038: val_categorical_accuracy did not improve from 0.34720\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.5257 - categorical_accuracy: 0.2959 - val_loss: 4.5079 - val_categorical_accuracy: 0.1986 - lr: 0.0700\n",
            "Epoch 39/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.5252 - categorical_accuracy: 0.2937\n",
            "Epoch 00039: val_categorical_accuracy did not improve from 0.34720\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.5252 - categorical_accuracy: 0.2937 - val_loss: 4.4861 - val_categorical_accuracy: 0.2958 - lr: 0.0700\n",
            "Epoch 40/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.5240 - categorical_accuracy: 0.2929\n",
            "Epoch 00040: val_categorical_accuracy improved from 0.34720 to 0.37650, saving model to Desktop/Trained_models/defensive_distillation_keras_vgg16_cifar100.h5\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.5240 - categorical_accuracy: 0.2929 - val_loss: 4.4742 - val_categorical_accuracy: 0.3765 - lr: 0.0700\n",
            "Epoch 41/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.5227 - categorical_accuracy: 0.2940\n",
            "Epoch 00041: val_categorical_accuracy did not improve from 0.37650\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.5227 - categorical_accuracy: 0.2940 - val_loss: 4.4933 - val_categorical_accuracy: 0.3563 - lr: 0.0700\n",
            "Epoch 42/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.5218 - categorical_accuracy: 0.2931\n",
            "Epoch 00042: val_categorical_accuracy did not improve from 0.37650\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.5218 - categorical_accuracy: 0.2931 - val_loss: 4.5074 - val_categorical_accuracy: 0.2293 - lr: 0.0700\n",
            "Epoch 43/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.5213 - categorical_accuracy: 0.2925\n",
            "Epoch 00043: val_categorical_accuracy did not improve from 0.37650\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.5213 - categorical_accuracy: 0.2925 - val_loss: 4.4916 - val_categorical_accuracy: 0.3065 - lr: 0.0700\n",
            "Epoch 44/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.5206 - categorical_accuracy: 0.2927\n",
            "Epoch 00044: val_categorical_accuracy did not improve from 0.37650\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.5206 - categorical_accuracy: 0.2927 - val_loss: 4.4808 - val_categorical_accuracy: 0.3430 - lr: 0.0700\n",
            "Epoch 45/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.5207 - categorical_accuracy: 0.2905\n",
            "Epoch 00045: val_categorical_accuracy did not improve from 0.37650\n",
            "\n",
            "Epoch 00045: ReduceLROnPlateau reducing learning rate to 0.04900000020861625.\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.5207 - categorical_accuracy: 0.2905 - val_loss: 4.5052 - val_categorical_accuracy: 0.2533 - lr: 0.0700\n",
            "Epoch 46/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.5056 - categorical_accuracy: 0.3084\n",
            "Epoch 00046: val_categorical_accuracy improved from 0.37650 to 0.37740, saving model to Desktop/Trained_models/defensive_distillation_keras_vgg16_cifar100.h5\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.5056 - categorical_accuracy: 0.3084 - val_loss: 4.4398 - val_categorical_accuracy: 0.3774 - lr: 0.0490\n",
            "Epoch 47/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.5039 - categorical_accuracy: 0.3071\n",
            "Epoch 00047: val_categorical_accuracy did not improve from 0.37740\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.5039 - categorical_accuracy: 0.3071 - val_loss: 4.4753 - val_categorical_accuracy: 0.3163 - lr: 0.0490\n",
            "Epoch 48/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.5034 - categorical_accuracy: 0.3085\n",
            "Epoch 00048: val_categorical_accuracy did not improve from 0.37740\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.5034 - categorical_accuracy: 0.3085 - val_loss: 4.4443 - val_categorical_accuracy: 0.3580 - lr: 0.0490\n",
            "Epoch 49/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.5036 - categorical_accuracy: 0.3087\n",
            "Epoch 00049: val_categorical_accuracy did not improve from 0.37740\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.5036 - categorical_accuracy: 0.3087 - val_loss: 4.4735 - val_categorical_accuracy: 0.3346 - lr: 0.0490\n",
            "Epoch 50/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.5029 - categorical_accuracy: 0.3069\n",
            "Epoch 00050: val_categorical_accuracy did not improve from 0.37740\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.5029 - categorical_accuracy: 0.3069 - val_loss: 4.4707 - val_categorical_accuracy: 0.3020 - lr: 0.0490\n",
            "Epoch 51/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.5042 - categorical_accuracy: 0.3047\n",
            "Epoch 00051: val_categorical_accuracy did not improve from 0.37740\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.5042 - categorical_accuracy: 0.3047 - val_loss: 4.4286 - val_categorical_accuracy: 0.3594 - lr: 0.0490\n",
            "Epoch 52/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.5053 - categorical_accuracy: 0.2996\n",
            "Epoch 00052: val_categorical_accuracy did not improve from 0.37740\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.5053 - categorical_accuracy: 0.2996 - val_loss: 4.4397 - val_categorical_accuracy: 0.3460 - lr: 0.0490\n",
            "Epoch 53/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.5060 - categorical_accuracy: 0.3011\n",
            "Epoch 00053: val_categorical_accuracy did not improve from 0.37740\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.5060 - categorical_accuracy: 0.3011 - val_loss: 4.4781 - val_categorical_accuracy: 0.2369 - lr: 0.0490\n",
            "Epoch 54/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.5053 - categorical_accuracy: 0.3026\n",
            "Epoch 00054: val_categorical_accuracy did not improve from 0.37740\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.5053 - categorical_accuracy: 0.3026 - val_loss: 4.4284 - val_categorical_accuracy: 0.3280 - lr: 0.0490\n",
            "Epoch 55/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.5059 - categorical_accuracy: 0.2975\n",
            "Epoch 00055: val_categorical_accuracy did not improve from 0.37740\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.5059 - categorical_accuracy: 0.2975 - val_loss: 4.4913 - val_categorical_accuracy: 0.2031 - lr: 0.0490\n",
            "Epoch 56/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.5073 - categorical_accuracy: 0.2963\n",
            "Epoch 00056: val_categorical_accuracy did not improve from 0.37740\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.5073 - categorical_accuracy: 0.2963 - val_loss: 4.5245 - val_categorical_accuracy: 0.2073 - lr: 0.0490\n",
            "Epoch 57/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.5076 - categorical_accuracy: 0.2943\n",
            "Epoch 00057: val_categorical_accuracy did not improve from 0.37740\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.5076 - categorical_accuracy: 0.2943 - val_loss: 4.4223 - val_categorical_accuracy: 0.2660 - lr: 0.0490\n",
            "Epoch 58/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.5080 - categorical_accuracy: 0.2919\n",
            "Epoch 00058: val_categorical_accuracy did not improve from 0.37740\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.5080 - categorical_accuracy: 0.2919 - val_loss: 4.4688 - val_categorical_accuracy: 0.2256 - lr: 0.0490\n",
            "Epoch 59/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.5091 - categorical_accuracy: 0.2946\n",
            "Epoch 00059: val_categorical_accuracy did not improve from 0.37740\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.5091 - categorical_accuracy: 0.2946 - val_loss: 4.4586 - val_categorical_accuracy: 0.2941 - lr: 0.0490\n",
            "Epoch 60/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.5094 - categorical_accuracy: 0.2925\n",
            "Epoch 00060: val_categorical_accuracy did not improve from 0.37740\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.5094 - categorical_accuracy: 0.2925 - val_loss: 4.4925 - val_categorical_accuracy: 0.2500 - lr: 0.0490\n",
            "Epoch 61/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.5086 - categorical_accuracy: 0.2913\n",
            "Epoch 00061: val_categorical_accuracy did not improve from 0.37740\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.5086 - categorical_accuracy: 0.2913 - val_loss: 4.4314 - val_categorical_accuracy: 0.3100 - lr: 0.0490\n",
            "Epoch 62/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.5092 - categorical_accuracy: 0.2893\n",
            "Epoch 00062: val_categorical_accuracy improved from 0.37740 to 0.37810, saving model to Desktop/Trained_models/defensive_distillation_keras_vgg16_cifar100.h5\n",
            "\n",
            "Epoch 00062: ReduceLROnPlateau reducing learning rate to 0.03429999910295009.\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.5092 - categorical_accuracy: 0.2893 - val_loss: 4.4258 - val_categorical_accuracy: 0.3781 - lr: 0.0490\n",
            "Epoch 63/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.4907 - categorical_accuracy: 0.3085\n",
            "Epoch 00063: val_categorical_accuracy did not improve from 0.37810\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.4907 - categorical_accuracy: 0.3085 - val_loss: 4.4052 - val_categorical_accuracy: 0.3619 - lr: 0.0343\n",
            "Epoch 64/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.4859 - categorical_accuracy: 0.3098\n",
            "Epoch 00064: val_categorical_accuracy improved from 0.37810 to 0.39380, saving model to Desktop/Trained_models/defensive_distillation_keras_vgg16_cifar100.h5\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.4859 - categorical_accuracy: 0.3098 - val_loss: 4.3707 - val_categorical_accuracy: 0.3938 - lr: 0.0343\n",
            "Epoch 65/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.4864 - categorical_accuracy: 0.3078\n",
            "Epoch 00065: val_categorical_accuracy did not improve from 0.39380\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.4864 - categorical_accuracy: 0.3078 - val_loss: 4.4563 - val_categorical_accuracy: 0.2417 - lr: 0.0343\n",
            "Epoch 66/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.4873 - categorical_accuracy: 0.3077\n",
            "Epoch 00066: val_categorical_accuracy did not improve from 0.39380\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.4873 - categorical_accuracy: 0.3077 - val_loss: 4.4147 - val_categorical_accuracy: 0.3259 - lr: 0.0343\n",
            "Epoch 67/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.4878 - categorical_accuracy: 0.3077\n",
            "Epoch 00067: val_categorical_accuracy did not improve from 0.39380\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.4878 - categorical_accuracy: 0.3077 - val_loss: 4.4498 - val_categorical_accuracy: 0.2868 - lr: 0.0343\n",
            "Epoch 68/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.4873 - categorical_accuracy: 0.3081\n",
            "Epoch 00068: val_categorical_accuracy did not improve from 0.39380\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.4873 - categorical_accuracy: 0.3081 - val_loss: 4.3931 - val_categorical_accuracy: 0.2829 - lr: 0.0343\n",
            "Epoch 69/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.4883 - categorical_accuracy: 0.3072\n",
            "Epoch 00069: val_categorical_accuracy did not improve from 0.39380\n",
            "\n",
            "Epoch 00069: ReduceLROnPlateau reducing learning rate to 0.024009999632835385.\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.4883 - categorical_accuracy: 0.3072 - val_loss: 4.4107 - val_categorical_accuracy: 0.3223 - lr: 0.0343\n",
            "Epoch 70/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.4699 - categorical_accuracy: 0.3275\n",
            "Epoch 00070: val_categorical_accuracy did not improve from 0.39380\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.4699 - categorical_accuracy: 0.3275 - val_loss: 4.3921 - val_categorical_accuracy: 0.3378 - lr: 0.0240\n",
            "Epoch 71/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.4647 - categorical_accuracy: 0.3275\n",
            "Epoch 00071: val_categorical_accuracy improved from 0.39380 to 0.44940, saving model to Desktop/Trained_models/defensive_distillation_keras_vgg16_cifar100.h5\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.4647 - categorical_accuracy: 0.3275 - val_loss: 4.2989 - val_categorical_accuracy: 0.4494 - lr: 0.0240\n",
            "Epoch 72/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.4644 - categorical_accuracy: 0.3270\n",
            "Epoch 00072: val_categorical_accuracy did not improve from 0.44940\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.4644 - categorical_accuracy: 0.3270 - val_loss: 4.4253 - val_categorical_accuracy: 0.3605 - lr: 0.0240\n",
            "Epoch 73/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.4646 - categorical_accuracy: 0.3278\n",
            "Epoch 00073: val_categorical_accuracy did not improve from 0.44940\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.4646 - categorical_accuracy: 0.3278 - val_loss: 4.3354 - val_categorical_accuracy: 0.4441 - lr: 0.0240\n",
            "Epoch 74/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.4655 - categorical_accuracy: 0.3285\n",
            "Epoch 00074: val_categorical_accuracy did not improve from 0.44940\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.4655 - categorical_accuracy: 0.3285 - val_loss: 4.4015 - val_categorical_accuracy: 0.3596 - lr: 0.0240\n",
            "Epoch 75/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.4643 - categorical_accuracy: 0.3300\n",
            "Epoch 00075: val_categorical_accuracy did not improve from 0.44940\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.4643 - categorical_accuracy: 0.3300 - val_loss: 4.3470 - val_categorical_accuracy: 0.3655 - lr: 0.0240\n",
            "Epoch 76/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.4650 - categorical_accuracy: 0.3302\n",
            "Epoch 00076: val_categorical_accuracy did not improve from 0.44940\n",
            "\n",
            "Epoch 00076: ReduceLROnPlateau reducing learning rate to 0.01680699922144413.\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.4650 - categorical_accuracy: 0.3302 - val_loss: 4.3062 - val_categorical_accuracy: 0.4358 - lr: 0.0240\n",
            "Epoch 77/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.4487 - categorical_accuracy: 0.3504\n",
            "Epoch 00077: val_categorical_accuracy improved from 0.44940 to 0.47340, saving model to Desktop/Trained_models/defensive_distillation_keras_vgg16_cifar100.h5\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.4487 - categorical_accuracy: 0.3504 - val_loss: 4.2910 - val_categorical_accuracy: 0.4734 - lr: 0.0168\n",
            "Epoch 78/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.4414 - categorical_accuracy: 0.3542\n",
            "Epoch 00078: val_categorical_accuracy did not improve from 0.47340\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.4414 - categorical_accuracy: 0.3542 - val_loss: 4.3142 - val_categorical_accuracy: 0.4566 - lr: 0.0168\n",
            "Epoch 79/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.4428 - categorical_accuracy: 0.3497\n",
            "Epoch 00079: val_categorical_accuracy did not improve from 0.47340\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.4428 - categorical_accuracy: 0.3497 - val_loss: 4.3028 - val_categorical_accuracy: 0.4646 - lr: 0.0168\n",
            "Epoch 80/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.4414 - categorical_accuracy: 0.3562\n",
            "Epoch 00080: val_categorical_accuracy improved from 0.47340 to 0.48760, saving model to Desktop/Trained_models/defensive_distillation_keras_vgg16_cifar100.h5\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.4414 - categorical_accuracy: 0.3562 - val_loss: 4.2248 - val_categorical_accuracy: 0.4876 - lr: 0.0168\n",
            "Epoch 81/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.4422 - categorical_accuracy: 0.3534\n",
            "Epoch 00081: val_categorical_accuracy did not improve from 0.48760\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.4422 - categorical_accuracy: 0.3534 - val_loss: 4.2891 - val_categorical_accuracy: 0.4470 - lr: 0.0168\n",
            "Epoch 82/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.4412 - categorical_accuracy: 0.3558\n",
            "Epoch 00082: val_categorical_accuracy did not improve from 0.48760\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.4412 - categorical_accuracy: 0.3558 - val_loss: 4.3339 - val_categorical_accuracy: 0.3999 - lr: 0.0168\n",
            "Epoch 83/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.4413 - categorical_accuracy: 0.3553\n",
            "Epoch 00083: val_categorical_accuracy did not improve from 0.48760\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.4413 - categorical_accuracy: 0.3553 - val_loss: 4.3269 - val_categorical_accuracy: 0.4509 - lr: 0.0168\n",
            "Epoch 84/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.4425 - categorical_accuracy: 0.3513\n",
            "Epoch 00084: val_categorical_accuracy did not improve from 0.48760\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.4425 - categorical_accuracy: 0.3513 - val_loss: 4.3079 - val_categorical_accuracy: 0.4352 - lr: 0.0168\n",
            "Epoch 85/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.4409 - categorical_accuracy: 0.3589\n",
            "Epoch 00085: val_categorical_accuracy improved from 0.48760 to 0.49320, saving model to Desktop/Trained_models/defensive_distillation_keras_vgg16_cifar100.h5\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.4409 - categorical_accuracy: 0.3589 - val_loss: 4.2240 - val_categorical_accuracy: 0.4932 - lr: 0.0168\n",
            "Epoch 86/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.4428 - categorical_accuracy: 0.3551\n",
            "Epoch 00086: val_categorical_accuracy did not improve from 0.49320\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.4428 - categorical_accuracy: 0.3551 - val_loss: 4.2670 - val_categorical_accuracy: 0.4598 - lr: 0.0168\n",
            "Epoch 87/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.4417 - categorical_accuracy: 0.3559\n",
            "Epoch 00087: val_categorical_accuracy did not improve from 0.49320\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.4417 - categorical_accuracy: 0.3559 - val_loss: 4.3113 - val_categorical_accuracy: 0.4430 - lr: 0.0168\n",
            "Epoch 88/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.4418 - categorical_accuracy: 0.3573\n",
            "Epoch 00088: val_categorical_accuracy did not improve from 0.49320\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.4418 - categorical_accuracy: 0.3573 - val_loss: 4.2750 - val_categorical_accuracy: 0.4437 - lr: 0.0168\n",
            "Epoch 89/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.4412 - categorical_accuracy: 0.3565\n",
            "Epoch 00089: val_categorical_accuracy did not improve from 0.49320\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.4412 - categorical_accuracy: 0.3565 - val_loss: 4.2412 - val_categorical_accuracy: 0.4725 - lr: 0.0168\n",
            "Epoch 90/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.4412 - categorical_accuracy: 0.3568\n",
            "Epoch 00090: val_categorical_accuracy did not improve from 0.49320\n",
            "\n",
            "Epoch 00090: ReduceLROnPlateau reducing learning rate to 0.01176489945501089.\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.4412 - categorical_accuracy: 0.3568 - val_loss: 4.2321 - val_categorical_accuracy: 0.4853 - lr: 0.0168\n",
            "Epoch 91/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.4263 - categorical_accuracy: 0.3777\n",
            "Epoch 00091: val_categorical_accuracy improved from 0.49320 to 0.54480, saving model to Desktop/Trained_models/defensive_distillation_keras_vgg16_cifar100.h5\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.4263 - categorical_accuracy: 0.3777 - val_loss: 4.2039 - val_categorical_accuracy: 0.5448 - lr: 0.0118\n",
            "Epoch 92/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.4182 - categorical_accuracy: 0.3833\n",
            "Epoch 00092: val_categorical_accuracy did not improve from 0.54480\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.4182 - categorical_accuracy: 0.3833 - val_loss: 4.2464 - val_categorical_accuracy: 0.5225 - lr: 0.0118\n",
            "Epoch 93/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.4168 - categorical_accuracy: 0.3799\n",
            "Epoch 00093: val_categorical_accuracy did not improve from 0.54480\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.4168 - categorical_accuracy: 0.3799 - val_loss: 4.1729 - val_categorical_accuracy: 0.4993 - lr: 0.0118\n",
            "Epoch 94/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.4175 - categorical_accuracy: 0.3753\n",
            "Epoch 00094: val_categorical_accuracy did not improve from 0.54480\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.4175 - categorical_accuracy: 0.3753 - val_loss: 4.2504 - val_categorical_accuracy: 0.4823 - lr: 0.0118\n",
            "Epoch 95/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.4162 - categorical_accuracy: 0.3835\n",
            "Epoch 00095: val_categorical_accuracy did not improve from 0.54480\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.4162 - categorical_accuracy: 0.3835 - val_loss: 4.2253 - val_categorical_accuracy: 0.5170 - lr: 0.0118\n",
            "Epoch 96/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.4148 - categorical_accuracy: 0.3821\n",
            "Epoch 00096: val_categorical_accuracy did not improve from 0.54480\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.4148 - categorical_accuracy: 0.3821 - val_loss: 4.2877 - val_categorical_accuracy: 0.4581 - lr: 0.0118\n",
            "Epoch 97/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.4163 - categorical_accuracy: 0.3823\n",
            "Epoch 00097: val_categorical_accuracy did not improve from 0.54480\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.4163 - categorical_accuracy: 0.3823 - val_loss: 4.2736 - val_categorical_accuracy: 0.4534 - lr: 0.0118\n",
            "Epoch 98/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.4158 - categorical_accuracy: 0.3846\n",
            "Epoch 00098: val_categorical_accuracy did not improve from 0.54480\n",
            "\n",
            "Epoch 00098: ReduceLROnPlateau reducing learning rate to 0.008235429879277945.\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.4158 - categorical_accuracy: 0.3846 - val_loss: 4.1812 - val_categorical_accuracy: 0.5234 - lr: 0.0118\n",
            "Epoch 99/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.4047 - categorical_accuracy: 0.3975\n",
            "Epoch 00099: val_categorical_accuracy improved from 0.54480 to 0.58710, saving model to Desktop/Trained_models/defensive_distillation_keras_vgg16_cifar100.h5\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.4047 - categorical_accuracy: 0.3975 - val_loss: 4.1694 - val_categorical_accuracy: 0.5871 - lr: 0.0082\n",
            "Epoch 100/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.3967 - categorical_accuracy: 0.4108\n",
            "Epoch 00100: val_categorical_accuracy did not improve from 0.58710\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.3967 - categorical_accuracy: 0.4108 - val_loss: 4.1713 - val_categorical_accuracy: 0.5800 - lr: 0.0082\n",
            "Epoch 101/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.3944 - categorical_accuracy: 0.4075\n",
            "Epoch 00101: val_categorical_accuracy did not improve from 0.58710\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.3944 - categorical_accuracy: 0.4075 - val_loss: 4.1344 - val_categorical_accuracy: 0.5773 - lr: 0.0082\n",
            "Epoch 102/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.3938 - categorical_accuracy: 0.4091\n",
            "Epoch 00102: val_categorical_accuracy did not improve from 0.58710\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.3938 - categorical_accuracy: 0.4091 - val_loss: 4.1950 - val_categorical_accuracy: 0.5403 - lr: 0.0082\n",
            "Epoch 103/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.3926 - categorical_accuracy: 0.4074\n",
            "Epoch 00103: val_categorical_accuracy did not improve from 0.58710\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.3926 - categorical_accuracy: 0.4074 - val_loss: 4.1925 - val_categorical_accuracy: 0.5393 - lr: 0.0082\n",
            "Epoch 104/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.3924 - categorical_accuracy: 0.4094\n",
            "Epoch 00104: val_categorical_accuracy did not improve from 0.58710\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.3924 - categorical_accuracy: 0.4094 - val_loss: 4.1926 - val_categorical_accuracy: 0.4879 - lr: 0.0082\n",
            "Epoch 105/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.3931 - categorical_accuracy: 0.4064\n",
            "Epoch 00105: val_categorical_accuracy did not improve from 0.58710\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.3931 - categorical_accuracy: 0.4064 - val_loss: 4.1828 - val_categorical_accuracy: 0.5360 - lr: 0.0082\n",
            "Epoch 106/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.3932 - categorical_accuracy: 0.4070\n",
            "Epoch 00106: val_categorical_accuracy did not improve from 0.58710\n",
            "\n",
            "Epoch 00106: ReduceLROnPlateau reducing learning rate to 0.005764801241457462.\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.3932 - categorical_accuracy: 0.4070 - val_loss: 4.1521 - val_categorical_accuracy: 0.5489 - lr: 0.0082\n",
            "Epoch 107/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.3826 - categorical_accuracy: 0.4273\n",
            "Epoch 00107: val_categorical_accuracy improved from 0.58710 to 0.59340, saving model to Desktop/Trained_models/defensive_distillation_keras_vgg16_cifar100.h5\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.3826 - categorical_accuracy: 0.4273 - val_loss: 4.1516 - val_categorical_accuracy: 0.5934 - lr: 0.0058\n",
            "Epoch 108/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.3764 - categorical_accuracy: 0.4349\n",
            "Epoch 00108: val_categorical_accuracy did not improve from 0.59340\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.3764 - categorical_accuracy: 0.4349 - val_loss: 4.1580 - val_categorical_accuracy: 0.5875 - lr: 0.0058\n",
            "Epoch 109/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.3720 - categorical_accuracy: 0.4420\n",
            "Epoch 00109: val_categorical_accuracy improved from 0.59340 to 0.61120, saving model to Desktop/Trained_models/defensive_distillation_keras_vgg16_cifar100.h5\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.3720 - categorical_accuracy: 0.4420 - val_loss: 4.1194 - val_categorical_accuracy: 0.6112 - lr: 0.0058\n",
            "Epoch 110/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.3718 - categorical_accuracy: 0.4360\n",
            "Epoch 00110: val_categorical_accuracy did not improve from 0.61120\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.3718 - categorical_accuracy: 0.4360 - val_loss: 4.1439 - val_categorical_accuracy: 0.5896 - lr: 0.0058\n",
            "Epoch 111/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.3720 - categorical_accuracy: 0.4345\n",
            "Epoch 00111: val_categorical_accuracy did not improve from 0.61120\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.3720 - categorical_accuracy: 0.4345 - val_loss: 4.1194 - val_categorical_accuracy: 0.5972 - lr: 0.0058\n",
            "Epoch 112/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.3708 - categorical_accuracy: 0.4339\n",
            "Epoch 00112: val_categorical_accuracy did not improve from 0.61120\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.3708 - categorical_accuracy: 0.4339 - val_loss: 4.1224 - val_categorical_accuracy: 0.6066 - lr: 0.0058\n",
            "Epoch 113/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.3695 - categorical_accuracy: 0.4374\n",
            "Epoch 00113: val_categorical_accuracy did not improve from 0.61120\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.3695 - categorical_accuracy: 0.4374 - val_loss: 4.1978 - val_categorical_accuracy: 0.5564 - lr: 0.0058\n",
            "Epoch 114/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.3696 - categorical_accuracy: 0.4355\n",
            "Epoch 00114: val_categorical_accuracy did not improve from 0.61120\n",
            "\n",
            "Epoch 00114: ReduceLROnPlateau reducing learning rate to 0.004035360738635063.\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.3696 - categorical_accuracy: 0.4355 - val_loss: 4.2103 - val_categorical_accuracy: 0.5370 - lr: 0.0058\n",
            "Epoch 115/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.3622 - categorical_accuracy: 0.4515\n",
            "Epoch 00115: val_categorical_accuracy improved from 0.61120 to 0.62530, saving model to Desktop/Trained_models/defensive_distillation_keras_vgg16_cifar100.h5\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.3622 - categorical_accuracy: 0.4515 - val_loss: 4.1075 - val_categorical_accuracy: 0.6253 - lr: 0.0040\n",
            "Epoch 116/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.3578 - categorical_accuracy: 0.4606\n",
            "Epoch 00116: val_categorical_accuracy improved from 0.62530 to 0.64520, saving model to Desktop/Trained_models/defensive_distillation_keras_vgg16_cifar100.h5\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.3578 - categorical_accuracy: 0.4606 - val_loss: 4.0900 - val_categorical_accuracy: 0.6452 - lr: 0.0040\n",
            "Epoch 117/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.3556 - categorical_accuracy: 0.4591\n",
            "Epoch 00117: val_categorical_accuracy did not improve from 0.64520\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.3556 - categorical_accuracy: 0.4591 - val_loss: 4.1260 - val_categorical_accuracy: 0.6216 - lr: 0.0040\n",
            "Epoch 118/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.3529 - categorical_accuracy: 0.4665\n",
            "Epoch 00118: val_categorical_accuracy did not improve from 0.64520\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.3529 - categorical_accuracy: 0.4665 - val_loss: 4.1369 - val_categorical_accuracy: 0.6182 - lr: 0.0040\n",
            "Epoch 119/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.3515 - categorical_accuracy: 0.4661\n",
            "Epoch 00119: val_categorical_accuracy did not improve from 0.64520\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.3515 - categorical_accuracy: 0.4661 - val_loss: 4.1124 - val_categorical_accuracy: 0.6041 - lr: 0.0040\n",
            "Epoch 120/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.3510 - categorical_accuracy: 0.4636\n",
            "Epoch 00120: val_categorical_accuracy did not improve from 0.64520\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.3510 - categorical_accuracy: 0.4636 - val_loss: 4.0812 - val_categorical_accuracy: 0.6224 - lr: 0.0040\n",
            "Epoch 121/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.3501 - categorical_accuracy: 0.4609\n",
            "Epoch 00121: val_categorical_accuracy did not improve from 0.64520\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.3501 - categorical_accuracy: 0.4609 - val_loss: 4.1290 - val_categorical_accuracy: 0.6011 - lr: 0.0040\n",
            "Epoch 122/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.3498 - categorical_accuracy: 0.4622\n",
            "Epoch 00122: val_categorical_accuracy did not improve from 0.64520\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.3498 - categorical_accuracy: 0.4622 - val_loss: 4.0819 - val_categorical_accuracy: 0.6119 - lr: 0.0040\n",
            "Epoch 123/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.3496 - categorical_accuracy: 0.4615\n",
            "Epoch 00123: val_categorical_accuracy did not improve from 0.64520\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.3496 - categorical_accuracy: 0.4615 - val_loss: 4.1075 - val_categorical_accuracy: 0.6069 - lr: 0.0040\n",
            "Epoch 124/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.3489 - categorical_accuracy: 0.4586\n",
            "Epoch 00124: val_categorical_accuracy did not improve from 0.64520\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.3489 - categorical_accuracy: 0.4586 - val_loss: 4.1075 - val_categorical_accuracy: 0.5903 - lr: 0.0040\n",
            "Epoch 125/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.3486 - categorical_accuracy: 0.4578\n",
            "Epoch 00125: val_categorical_accuracy did not improve from 0.64520\n",
            "\n",
            "Epoch 00125: ReduceLROnPlateau reducing learning rate to 0.0028247524518519636.\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.3486 - categorical_accuracy: 0.4578 - val_loss: 4.1462 - val_categorical_accuracy: 0.5866 - lr: 0.0040\n",
            "Epoch 126/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.3427 - categorical_accuracy: 0.4774\n",
            "Epoch 00126: val_categorical_accuracy did not improve from 0.64520\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.3427 - categorical_accuracy: 0.4774 - val_loss: 4.0866 - val_categorical_accuracy: 0.6392 - lr: 0.0028\n",
            "Epoch 127/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.3389 - categorical_accuracy: 0.4887\n",
            "Epoch 00127: val_categorical_accuracy improved from 0.64520 to 0.65480, saving model to Desktop/Trained_models/defensive_distillation_keras_vgg16_cifar100.h5\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.3389 - categorical_accuracy: 0.4887 - val_loss: 4.0953 - val_categorical_accuracy: 0.6548 - lr: 0.0028\n",
            "Epoch 128/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.3377 - categorical_accuracy: 0.4859\n",
            "Epoch 00128: val_categorical_accuracy did not improve from 0.65480\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.3377 - categorical_accuracy: 0.4859 - val_loss: 4.0707 - val_categorical_accuracy: 0.6544 - lr: 0.0028\n",
            "Epoch 129/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.3352 - categorical_accuracy: 0.4898\n",
            "Epoch 00129: val_categorical_accuracy did not improve from 0.65480\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.3352 - categorical_accuracy: 0.4898 - val_loss: 4.0976 - val_categorical_accuracy: 0.6426 - lr: 0.0028\n",
            "Epoch 130/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.3335 - categorical_accuracy: 0.4878\n",
            "Epoch 00130: val_categorical_accuracy did not improve from 0.65480\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.3335 - categorical_accuracy: 0.4878 - val_loss: 4.0756 - val_categorical_accuracy: 0.6466 - lr: 0.0028\n",
            "Epoch 131/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.3341 - categorical_accuracy: 0.4864\n",
            "Epoch 00131: val_categorical_accuracy did not improve from 0.65480\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.3341 - categorical_accuracy: 0.4864 - val_loss: 4.1196 - val_categorical_accuracy: 0.5921 - lr: 0.0028\n",
            "Epoch 132/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.3332 - categorical_accuracy: 0.4854\n",
            "Epoch 00132: val_categorical_accuracy did not improve from 0.65480\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.3332 - categorical_accuracy: 0.4854 - val_loss: 4.1018 - val_categorical_accuracy: 0.6334 - lr: 0.0028\n",
            "Epoch 133/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.3328 - categorical_accuracy: 0.4825\n",
            "Epoch 00133: val_categorical_accuracy did not improve from 0.65480\n",
            "\n",
            "Epoch 00133: ReduceLROnPlateau reducing learning rate to 0.0019773266511037943.\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.3328 - categorical_accuracy: 0.4825 - val_loss: 4.0868 - val_categorical_accuracy: 0.6440 - lr: 0.0028\n",
            "Epoch 134/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.3279 - categorical_accuracy: 0.4988\n",
            "Epoch 00134: val_categorical_accuracy improved from 0.65480 to 0.66190, saving model to Desktop/Trained_models/defensive_distillation_keras_vgg16_cifar100.h5\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.3279 - categorical_accuracy: 0.4988 - val_loss: 4.0645 - val_categorical_accuracy: 0.6619 - lr: 0.0020\n",
            "Epoch 135/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.3252 - categorical_accuracy: 0.5102\n",
            "Epoch 00135: val_categorical_accuracy improved from 0.66190 to 0.66710, saving model to Desktop/Trained_models/defensive_distillation_keras_vgg16_cifar100.h5\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.3252 - categorical_accuracy: 0.5102 - val_loss: 4.0844 - val_categorical_accuracy: 0.6671 - lr: 0.0020\n",
            "Epoch 136/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.3235 - categorical_accuracy: 0.5129\n",
            "Epoch 00136: val_categorical_accuracy did not improve from 0.66710\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.3235 - categorical_accuracy: 0.5129 - val_loss: 4.0974 - val_categorical_accuracy: 0.6637 - lr: 0.0020\n",
            "Epoch 137/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.3232 - categorical_accuracy: 0.5112\n",
            "Epoch 00137: val_categorical_accuracy improved from 0.66710 to 0.66960, saving model to Desktop/Trained_models/defensive_distillation_keras_vgg16_cifar100.h5\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.3232 - categorical_accuracy: 0.5112 - val_loss: 4.0737 - val_categorical_accuracy: 0.6696 - lr: 0.0020\n",
            "Epoch 138/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.3213 - categorical_accuracy: 0.5110\n",
            "Epoch 00138: val_categorical_accuracy did not improve from 0.66960\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.3213 - categorical_accuracy: 0.5110 - val_loss: 4.0919 - val_categorical_accuracy: 0.6491 - lr: 0.0020\n",
            "Epoch 139/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.3204 - categorical_accuracy: 0.5158\n",
            "Epoch 00139: val_categorical_accuracy improved from 0.66960 to 0.67360, saving model to Desktop/Trained_models/defensive_distillation_keras_vgg16_cifar100.h5\n",
            "\n",
            "Epoch 00139: ReduceLROnPlateau reducing learning rate to 0.001384128606878221.\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.3204 - categorical_accuracy: 0.5158 - val_loss: 4.0742 - val_categorical_accuracy: 0.6736 - lr: 0.0020\n",
            "Epoch 140/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.3172 - categorical_accuracy: 0.5224\n",
            "Epoch 00140: val_categorical_accuracy improved from 0.67360 to 0.67770, saving model to Desktop/Trained_models/defensive_distillation_keras_vgg16_cifar100.h5\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.3172 - categorical_accuracy: 0.5224 - val_loss: 4.0559 - val_categorical_accuracy: 0.6777 - lr: 0.0014\n",
            "Epoch 141/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.3157 - categorical_accuracy: 0.5273\n",
            "Epoch 00141: val_categorical_accuracy improved from 0.67770 to 0.68630, saving model to Desktop/Trained_models/defensive_distillation_keras_vgg16_cifar100.h5\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.3157 - categorical_accuracy: 0.5273 - val_loss: 4.0471 - val_categorical_accuracy: 0.6863 - lr: 0.0014\n",
            "Epoch 142/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.3145 - categorical_accuracy: 0.5340\n",
            "Epoch 00142: val_categorical_accuracy improved from 0.68630 to 0.69220, saving model to Desktop/Trained_models/defensive_distillation_keras_vgg16_cifar100.h5\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.3145 - categorical_accuracy: 0.5340 - val_loss: 4.0585 - val_categorical_accuracy: 0.6922 - lr: 0.0014\n",
            "Epoch 143/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.3134 - categorical_accuracy: 0.5331\n",
            "Epoch 00143: val_categorical_accuracy did not improve from 0.69220\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.3134 - categorical_accuracy: 0.5331 - val_loss: 4.0621 - val_categorical_accuracy: 0.6743 - lr: 0.0014\n",
            "Epoch 144/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.3125 - categorical_accuracy: 0.5374\n",
            "Epoch 00144: val_categorical_accuracy did not improve from 0.69220\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.3125 - categorical_accuracy: 0.5374 - val_loss: 4.0675 - val_categorical_accuracy: 0.6835 - lr: 0.0014\n",
            "Epoch 145/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.3119 - categorical_accuracy: 0.5309\n",
            "Epoch 00145: val_categorical_accuracy did not improve from 0.69220\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.3119 - categorical_accuracy: 0.5309 - val_loss: 4.0912 - val_categorical_accuracy: 0.6759 - lr: 0.0014\n",
            "Epoch 146/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.3117 - categorical_accuracy: 0.5320\n",
            "Epoch 00146: val_categorical_accuracy did not improve from 0.69220\n",
            "\n",
            "Epoch 00146: ReduceLROnPlateau reducing learning rate to 0.0009688900085166096.\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.3117 - categorical_accuracy: 0.5320 - val_loss: 4.0696 - val_categorical_accuracy: 0.6722 - lr: 0.0014\n",
            "Epoch 147/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.3089 - categorical_accuracy: 0.5394\n",
            "Epoch 00147: val_categorical_accuracy did not improve from 0.69220\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.3089 - categorical_accuracy: 0.5394 - val_loss: 4.0688 - val_categorical_accuracy: 0.6880 - lr: 9.6889e-04\n",
            "Epoch 148/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.3079 - categorical_accuracy: 0.5452\n",
            "Epoch 00148: val_categorical_accuracy improved from 0.69220 to 0.69260, saving model to Desktop/Trained_models/defensive_distillation_keras_vgg16_cifar100.h5\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.3079 - categorical_accuracy: 0.5452 - val_loss: 4.0810 - val_categorical_accuracy: 0.6926 - lr: 9.6889e-04\n",
            "Epoch 149/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.3075 - categorical_accuracy: 0.5432\n",
            "Epoch 00149: val_categorical_accuracy improved from 0.69260 to 0.69950, saving model to Desktop/Trained_models/defensive_distillation_keras_vgg16_cifar100.h5\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.3075 - categorical_accuracy: 0.5432 - val_loss: 4.0579 - val_categorical_accuracy: 0.6995 - lr: 9.6889e-04\n",
            "Epoch 150/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.3064 - categorical_accuracy: 0.5481\n",
            "Epoch 00150: val_categorical_accuracy did not improve from 0.69950\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.3064 - categorical_accuracy: 0.5481 - val_loss: 4.0793 - val_categorical_accuracy: 0.6965 - lr: 9.6889e-04\n",
            "Epoch 151/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.3057 - categorical_accuracy: 0.5464\n",
            "Epoch 00151: val_categorical_accuracy did not improve from 0.69950\n",
            "\n",
            "Epoch 00151: ReduceLROnPlateau reducing learning rate to 0.0006782230222597718.\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.3057 - categorical_accuracy: 0.5464 - val_loss: 4.0810 - val_categorical_accuracy: 0.6846 - lr: 9.6889e-04\n",
            "Epoch 152/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.3045 - categorical_accuracy: 0.5504\n",
            "Epoch 00152: val_categorical_accuracy did not improve from 0.69950\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.3045 - categorical_accuracy: 0.5504 - val_loss: 4.0521 - val_categorical_accuracy: 0.6935 - lr: 6.7822e-04\n",
            "Epoch 153/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.3036 - categorical_accuracy: 0.5525\n",
            "Epoch 00153: val_categorical_accuracy did not improve from 0.69950\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.3036 - categorical_accuracy: 0.5525 - val_loss: 4.0596 - val_categorical_accuracy: 0.6961 - lr: 6.7822e-04\n",
            "Epoch 154/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.3034 - categorical_accuracy: 0.5585\n",
            "Epoch 00154: val_categorical_accuracy did not improve from 0.69950\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.3034 - categorical_accuracy: 0.5585 - val_loss: 4.0667 - val_categorical_accuracy: 0.6956 - lr: 6.7822e-04\n",
            "Epoch 155/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.3021 - categorical_accuracy: 0.5581\n",
            "Epoch 00155: val_categorical_accuracy did not improve from 0.69950\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.3021 - categorical_accuracy: 0.5581 - val_loss: 4.0795 - val_categorical_accuracy: 0.6988 - lr: 6.7822e-04\n",
            "Epoch 156/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.3018 - categorical_accuracy: 0.5591\n",
            "Epoch 00156: val_categorical_accuracy improved from 0.69950 to 0.70190, saving model to Desktop/Trained_models/defensive_distillation_keras_vgg16_cifar100.h5\n",
            "\n",
            "Epoch 00156: ReduceLROnPlateau reducing learning rate to 0.0004747561237309128.\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.3018 - categorical_accuracy: 0.5591 - val_loss: 4.0645 - val_categorical_accuracy: 0.7019 - lr: 6.7822e-04\n",
            "Epoch 157/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.3007 - categorical_accuracy: 0.5683\n",
            "Epoch 00157: val_categorical_accuracy improved from 0.70190 to 0.70230, saving model to Desktop/Trained_models/defensive_distillation_keras_vgg16_cifar100.h5\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.3007 - categorical_accuracy: 0.5683 - val_loss: 4.0664 - val_categorical_accuracy: 0.7023 - lr: 4.7476e-04\n",
            "Epoch 158/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.3016 - categorical_accuracy: 0.5650\n",
            "Epoch 00158: val_categorical_accuracy improved from 0.70230 to 0.70400, saving model to Desktop/Trained_models/defensive_distillation_keras_vgg16_cifar100.h5\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.3016 - categorical_accuracy: 0.5650 - val_loss: 4.0607 - val_categorical_accuracy: 0.7040 - lr: 4.7476e-04\n",
            "Epoch 159/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.3002 - categorical_accuracy: 0.5731\n",
            "Epoch 00159: val_categorical_accuracy improved from 0.70400 to 0.70870, saving model to Desktop/Trained_models/defensive_distillation_keras_vgg16_cifar100.h5\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.3002 - categorical_accuracy: 0.5731 - val_loss: 4.0776 - val_categorical_accuracy: 0.7087 - lr: 4.7476e-04\n",
            "Epoch 160/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.3000 - categorical_accuracy: 0.5756\n",
            "Epoch 00160: val_categorical_accuracy did not improve from 0.70870\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.3000 - categorical_accuracy: 0.5756 - val_loss: 4.0684 - val_categorical_accuracy: 0.7080 - lr: 4.7476e-04\n",
            "Epoch 161/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2992 - categorical_accuracy: 0.5774\n",
            "Epoch 00161: val_categorical_accuracy did not improve from 0.70870\n",
            "\n",
            "Epoch 00161: ReduceLROnPlateau reducing learning rate to 0.0003323292825371027.\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.2992 - categorical_accuracy: 0.5774 - val_loss: 4.0796 - val_categorical_accuracy: 0.7085 - lr: 4.7476e-04\n",
            "Epoch 162/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2994 - categorical_accuracy: 0.5779\n",
            "Epoch 00162: val_categorical_accuracy improved from 0.70870 to 0.70920, saving model to Desktop/Trained_models/defensive_distillation_keras_vgg16_cifar100.h5\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.2994 - categorical_accuracy: 0.5779 - val_loss: 4.0680 - val_categorical_accuracy: 0.7092 - lr: 3.3233e-04\n",
            "Epoch 163/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2987 - categorical_accuracy: 0.5768\n",
            "Epoch 00163: val_categorical_accuracy improved from 0.70920 to 0.71070, saving model to Desktop/Trained_models/defensive_distillation_keras_vgg16_cifar100.h5\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.2987 - categorical_accuracy: 0.5768 - val_loss: 4.0701 - val_categorical_accuracy: 0.7107 - lr: 3.3233e-04\n",
            "Epoch 164/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2982 - categorical_accuracy: 0.5813\n",
            "Epoch 00164: val_categorical_accuracy improved from 0.71070 to 0.71340, saving model to Desktop/Trained_models/defensive_distillation_keras_vgg16_cifar100.h5\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.2982 - categorical_accuracy: 0.5813 - val_loss: 4.0725 - val_categorical_accuracy: 0.7134 - lr: 3.3233e-04\n",
            "Epoch 165/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2976 - categorical_accuracy: 0.5794\n",
            "Epoch 00165: val_categorical_accuracy did not improve from 0.71340\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.2976 - categorical_accuracy: 0.5794 - val_loss: 4.0730 - val_categorical_accuracy: 0.7113 - lr: 3.3233e-04\n",
            "Epoch 166/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2975 - categorical_accuracy: 0.5802\n",
            "Epoch 00166: val_categorical_accuracy improved from 0.71340 to 0.71490, saving model to Desktop/Trained_models/defensive_distillation_keras_vgg16_cifar100.h5\n",
            "\n",
            "Epoch 00166: ReduceLROnPlateau reducing learning rate to 0.0002326304937014356.\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.2975 - categorical_accuracy: 0.5802 - val_loss: 4.0738 - val_categorical_accuracy: 0.7149 - lr: 3.3233e-04\n",
            "Epoch 167/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2974 - categorical_accuracy: 0.5805\n",
            "Epoch 00167: val_categorical_accuracy improved from 0.71490 to 0.71660, saving model to Desktop/Trained_models/defensive_distillation_keras_vgg16_cifar100.h5\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.2974 - categorical_accuracy: 0.5805 - val_loss: 4.0729 - val_categorical_accuracy: 0.7166 - lr: 2.3263e-04\n",
            "Epoch 168/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2971 - categorical_accuracy: 0.5823\n",
            "Epoch 00168: val_categorical_accuracy did not improve from 0.71660\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.2971 - categorical_accuracy: 0.5823 - val_loss: 4.0715 - val_categorical_accuracy: 0.7153 - lr: 2.3263e-04\n",
            "Epoch 169/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2970 - categorical_accuracy: 0.5798\n",
            "Epoch 00169: val_categorical_accuracy did not improve from 0.71660\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.2970 - categorical_accuracy: 0.5798 - val_loss: 4.0736 - val_categorical_accuracy: 0.7154 - lr: 2.3263e-04\n",
            "Epoch 170/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2976 - categorical_accuracy: 0.5827\n",
            "Epoch 00170: val_categorical_accuracy did not improve from 0.71660\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.2976 - categorical_accuracy: 0.5827 - val_loss: 4.0713 - val_categorical_accuracy: 0.7114 - lr: 2.3263e-04\n",
            "Epoch 171/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2968 - categorical_accuracy: 0.5832\n",
            "Epoch 00171: val_categorical_accuracy did not improve from 0.71660\n",
            "\n",
            "Epoch 00171: ReduceLROnPlateau reducing learning rate to 0.00016284134762827306.\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.2968 - categorical_accuracy: 0.5832 - val_loss: 4.0651 - val_categorical_accuracy: 0.7145 - lr: 2.3263e-04\n",
            "Epoch 172/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2966 - categorical_accuracy: 0.5842\n",
            "Epoch 00172: val_categorical_accuracy did not improve from 0.71660\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.2966 - categorical_accuracy: 0.5842 - val_loss: 4.0740 - val_categorical_accuracy: 0.7119 - lr: 1.6284e-04\n",
            "Epoch 173/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2959 - categorical_accuracy: 0.5883\n",
            "Epoch 00173: val_categorical_accuracy did not improve from 0.71660\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.2959 - categorical_accuracy: 0.5883 - val_loss: 4.0765 - val_categorical_accuracy: 0.7162 - lr: 1.6284e-04\n",
            "Epoch 174/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2956 - categorical_accuracy: 0.5890\n",
            "Epoch 00174: val_categorical_accuracy did not improve from 0.71660\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.2956 - categorical_accuracy: 0.5890 - val_loss: 4.0792 - val_categorical_accuracy: 0.7145 - lr: 1.6284e-04\n",
            "Epoch 175/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2962 - categorical_accuracy: 0.5908\n",
            "Epoch 00175: val_categorical_accuracy did not improve from 0.71660\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.2962 - categorical_accuracy: 0.5908 - val_loss: 4.0807 - val_categorical_accuracy: 0.7146 - lr: 1.6284e-04\n",
            "Epoch 176/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2956 - categorical_accuracy: 0.5942\n",
            "Epoch 00176: val_categorical_accuracy did not improve from 0.71660\n",
            "\n",
            "Epoch 00176: ReduceLROnPlateau reducing learning rate to 0.00011398894130252301.\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.2956 - categorical_accuracy: 0.5942 - val_loss: 4.0760 - val_categorical_accuracy: 0.7154 - lr: 1.6284e-04\n",
            "Epoch 177/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2955 - categorical_accuracy: 0.5930\n",
            "Epoch 00177: val_categorical_accuracy did not improve from 0.71660\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.2955 - categorical_accuracy: 0.5930 - val_loss: 4.0773 - val_categorical_accuracy: 0.7157 - lr: 1.1399e-04\n",
            "Epoch 178/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2958 - categorical_accuracy: 0.5932\n",
            "Epoch 00178: val_categorical_accuracy improved from 0.71660 to 0.71750, saving model to Desktop/Trained_models/defensive_distillation_keras_vgg16_cifar100.h5\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.2958 - categorical_accuracy: 0.5932 - val_loss: 4.0684 - val_categorical_accuracy: 0.7175 - lr: 1.1399e-04\n",
            "Epoch 179/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2961 - categorical_accuracy: 0.5893\n",
            "Epoch 00179: val_categorical_accuracy improved from 0.71750 to 0.71760, saving model to Desktop/Trained_models/defensive_distillation_keras_vgg16_cifar100.h5\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.2961 - categorical_accuracy: 0.5893 - val_loss: 4.0745 - val_categorical_accuracy: 0.7176 - lr: 1.1399e-04\n",
            "Epoch 180/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2951 - categorical_accuracy: 0.5951\n",
            "Epoch 00180: val_categorical_accuracy improved from 0.71760 to 0.71870, saving model to Desktop/Trained_models/defensive_distillation_keras_vgg16_cifar100.h5\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.2951 - categorical_accuracy: 0.5951 - val_loss: 4.0802 - val_categorical_accuracy: 0.7187 - lr: 1.1399e-04\n",
            "Epoch 181/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2957 - categorical_accuracy: 0.5919\n",
            "Epoch 00181: val_categorical_accuracy did not improve from 0.71870\n",
            "\n",
            "Epoch 00181: ReduceLROnPlateau reducing learning rate to 7.979225993040017e-05.\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.2957 - categorical_accuracy: 0.5919 - val_loss: 4.0760 - val_categorical_accuracy: 0.7166 - lr: 1.1399e-04\n",
            "Epoch 182/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2952 - categorical_accuracy: 0.5916\n",
            "Epoch 00182: val_categorical_accuracy did not improve from 0.71870\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.2952 - categorical_accuracy: 0.5916 - val_loss: 4.0749 - val_categorical_accuracy: 0.7176 - lr: 7.9792e-05\n",
            "Epoch 183/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2946 - categorical_accuracy: 0.5904\n",
            "Epoch 00183: val_categorical_accuracy improved from 0.71870 to 0.71920, saving model to Desktop/Trained_models/defensive_distillation_keras_vgg16_cifar100.h5\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.2946 - categorical_accuracy: 0.5904 - val_loss: 4.0779 - val_categorical_accuracy: 0.7192 - lr: 7.9792e-05\n",
            "Epoch 184/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2947 - categorical_accuracy: 0.5970\n",
            "Epoch 00184: val_categorical_accuracy improved from 0.71920 to 0.72180, saving model to Desktop/Trained_models/defensive_distillation_keras_vgg16_cifar100.h5\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.2947 - categorical_accuracy: 0.5970 - val_loss: 4.0773 - val_categorical_accuracy: 0.7218 - lr: 7.9792e-05\n",
            "Epoch 185/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2955 - categorical_accuracy: 0.5958\n",
            "Epoch 00185: val_categorical_accuracy did not improve from 0.72180\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.2955 - categorical_accuracy: 0.5958 - val_loss: 4.0804 - val_categorical_accuracy: 0.7173 - lr: 7.9792e-05\n",
            "Epoch 186/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2952 - categorical_accuracy: 0.5927\n",
            "Epoch 00186: val_categorical_accuracy did not improve from 0.72180\n",
            "\n",
            "Epoch 00186: ReduceLROnPlateau reducing learning rate to 5.585458347923122e-05.\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.2952 - categorical_accuracy: 0.5927 - val_loss: 4.0736 - val_categorical_accuracy: 0.7177 - lr: 7.9792e-05\n",
            "Epoch 187/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2951 - categorical_accuracy: 0.5958\n",
            "Epoch 00187: val_categorical_accuracy did not improve from 0.72180\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.2951 - categorical_accuracy: 0.5958 - val_loss: 4.0748 - val_categorical_accuracy: 0.7176 - lr: 5.5855e-05\n",
            "Epoch 188/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2955 - categorical_accuracy: 0.5938\n",
            "Epoch 00188: val_categorical_accuracy did not improve from 0.72180\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.2955 - categorical_accuracy: 0.5938 - val_loss: 4.0712 - val_categorical_accuracy: 0.7178 - lr: 5.5855e-05\n",
            "Epoch 189/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2951 - categorical_accuracy: 0.5969\n",
            "Epoch 00189: val_categorical_accuracy did not improve from 0.72180\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.2951 - categorical_accuracy: 0.5969 - val_loss: 4.0744 - val_categorical_accuracy: 0.7191 - lr: 5.5855e-05\n",
            "Epoch 190/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2942 - categorical_accuracy: 0.6032\n",
            "Epoch 00190: val_categorical_accuracy did not improve from 0.72180\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.2942 - categorical_accuracy: 0.6032 - val_loss: 4.0809 - val_categorical_accuracy: 0.7198 - lr: 5.5855e-05\n",
            "Epoch 191/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2947 - categorical_accuracy: 0.5994\n",
            "Epoch 00191: val_categorical_accuracy did not improve from 0.72180\n",
            "\n",
            "Epoch 00191: ReduceLROnPlateau reducing learning rate to 3.9098208435461854e-05.\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.2947 - categorical_accuracy: 0.5994 - val_loss: 4.0826 - val_categorical_accuracy: 0.7200 - lr: 5.5855e-05\n",
            "Epoch 192/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2949 - categorical_accuracy: 0.6013\n",
            "Epoch 00192: val_categorical_accuracy did not improve from 0.72180\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.2949 - categorical_accuracy: 0.6013 - val_loss: 4.0819 - val_categorical_accuracy: 0.7213 - lr: 3.9098e-05\n",
            "Epoch 193/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2951 - categorical_accuracy: 0.5976\n",
            "Epoch 00193: val_categorical_accuracy did not improve from 0.72180\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.2951 - categorical_accuracy: 0.5976 - val_loss: 4.0801 - val_categorical_accuracy: 0.7204 - lr: 3.9098e-05\n",
            "Epoch 194/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2942 - categorical_accuracy: 0.6023\n",
            "Epoch 00194: val_categorical_accuracy did not improve from 0.72180\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.2942 - categorical_accuracy: 0.6023 - val_loss: 4.0740 - val_categorical_accuracy: 0.7197 - lr: 3.9098e-05\n",
            "Epoch 195/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2955 - categorical_accuracy: 0.5976\n",
            "Epoch 00195: val_categorical_accuracy did not improve from 0.72180\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.2955 - categorical_accuracy: 0.5976 - val_loss: 4.0762 - val_categorical_accuracy: 0.7215 - lr: 3.9098e-05\n",
            "Epoch 196/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2946 - categorical_accuracy: 0.5959\n",
            "Epoch 00196: val_categorical_accuracy did not improve from 0.72180\n",
            "\n",
            "Epoch 00196: ReduceLROnPlateau reducing learning rate to 2.736874666879885e-05.\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.2946 - categorical_accuracy: 0.5959 - val_loss: 4.0762 - val_categorical_accuracy: 0.7200 - lr: 3.9098e-05\n",
            "Epoch 197/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2947 - categorical_accuracy: 0.5999\n",
            "Epoch 00197: val_categorical_accuracy did not improve from 0.72180\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.2947 - categorical_accuracy: 0.5999 - val_loss: 4.0796 - val_categorical_accuracy: 0.7191 - lr: 2.7369e-05\n",
            "Epoch 198/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2945 - categorical_accuracy: 0.5989\n",
            "Epoch 00198: val_categorical_accuracy did not improve from 0.72180\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.2945 - categorical_accuracy: 0.5989 - val_loss: 4.0790 - val_categorical_accuracy: 0.7195 - lr: 2.7369e-05\n",
            "Epoch 199/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2945 - categorical_accuracy: 0.5998\n",
            "Epoch 00199: val_categorical_accuracy did not improve from 0.72180\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.2945 - categorical_accuracy: 0.5998 - val_loss: 4.0744 - val_categorical_accuracy: 0.7190 - lr: 2.7369e-05\n",
            "Epoch 200/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2946 - categorical_accuracy: 0.6008\n",
            "Epoch 00200: val_categorical_accuracy did not improve from 0.72180\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.2946 - categorical_accuracy: 0.6008 - val_loss: 4.0750 - val_categorical_accuracy: 0.7200 - lr: 2.7369e-05\n",
            "Epoch 201/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2941 - categorical_accuracy: 0.5980\n",
            "Epoch 00201: val_categorical_accuracy did not improve from 0.72180\n",
            "\n",
            "Epoch 00201: ReduceLROnPlateau reducing learning rate to 1.9158123177476227e-05.\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.2941 - categorical_accuracy: 0.5980 - val_loss: 4.0799 - val_categorical_accuracy: 0.7197 - lr: 2.7369e-05\n",
            "Epoch 202/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2951 - categorical_accuracy: 0.5988\n",
            "Epoch 00202: val_categorical_accuracy did not improve from 0.72180\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.2951 - categorical_accuracy: 0.5988 - val_loss: 4.0782 - val_categorical_accuracy: 0.7204 - lr: 1.9158e-05\n",
            "Epoch 203/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2950 - categorical_accuracy: 0.5961\n",
            "Epoch 00203: val_categorical_accuracy improved from 0.72180 to 0.72190, saving model to Desktop/Trained_models/defensive_distillation_keras_vgg16_cifar100.h5\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.2950 - categorical_accuracy: 0.5961 - val_loss: 4.0812 - val_categorical_accuracy: 0.7219 - lr: 1.9158e-05\n",
            "Epoch 204/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2936 - categorical_accuracy: 0.6037\n",
            "Epoch 00204: val_categorical_accuracy did not improve from 0.72190\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.2936 - categorical_accuracy: 0.6037 - val_loss: 4.0767 - val_categorical_accuracy: 0.7203 - lr: 1.9158e-05\n",
            "Epoch 205/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2942 - categorical_accuracy: 0.6041\n",
            "Epoch 00205: val_categorical_accuracy did not improve from 0.72190\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.2942 - categorical_accuracy: 0.6041 - val_loss: 4.0834 - val_categorical_accuracy: 0.7198 - lr: 1.9158e-05\n",
            "Epoch 206/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2942 - categorical_accuracy: 0.5976\n",
            "Epoch 00206: val_categorical_accuracy did not improve from 0.72190\n",
            "\n",
            "Epoch 00206: ReduceLROnPlateau reducing learning rate to 1.341068673355039e-05.\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.2942 - categorical_accuracy: 0.5976 - val_loss: 4.0754 - val_categorical_accuracy: 0.7208 - lr: 1.9158e-05\n",
            "Epoch 207/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2942 - categorical_accuracy: 0.6003\n",
            "Epoch 00207: val_categorical_accuracy did not improve from 0.72190\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.2942 - categorical_accuracy: 0.6003 - val_loss: 4.0808 - val_categorical_accuracy: 0.7192 - lr: 1.3411e-05\n",
            "Epoch 208/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2948 - categorical_accuracy: 0.5985\n",
            "Epoch 00208: val_categorical_accuracy did not improve from 0.72190\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.2948 - categorical_accuracy: 0.5985 - val_loss: 4.0738 - val_categorical_accuracy: 0.7191 - lr: 1.3411e-05\n",
            "Epoch 209/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2945 - categorical_accuracy: 0.6027\n",
            "Epoch 00209: val_categorical_accuracy did not improve from 0.72190\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.2945 - categorical_accuracy: 0.6027 - val_loss: 4.0745 - val_categorical_accuracy: 0.7201 - lr: 1.3411e-05\n",
            "Epoch 210/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2944 - categorical_accuracy: 0.5993\n",
            "Epoch 00210: val_categorical_accuracy did not improve from 0.72190\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.2944 - categorical_accuracy: 0.5993 - val_loss: 4.0800 - val_categorical_accuracy: 0.7212 - lr: 1.3411e-05\n",
            "Epoch 211/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2945 - categorical_accuracy: 0.6014\n",
            "Epoch 00211: val_categorical_accuracy did not improve from 0.72190\n",
            "\n",
            "Epoch 00211: ReduceLROnPlateau reducing learning rate to 9.387480713485274e-06.\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.2945 - categorical_accuracy: 0.6014 - val_loss: 4.0811 - val_categorical_accuracy: 0.7196 - lr: 1.3411e-05\n",
            "Epoch 212/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2943 - categorical_accuracy: 0.6025\n",
            "Epoch 00212: val_categorical_accuracy did not improve from 0.72190\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.2943 - categorical_accuracy: 0.6025 - val_loss: 4.0774 - val_categorical_accuracy: 0.7186 - lr: 9.3875e-06\n",
            "Epoch 213/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2942 - categorical_accuracy: 0.6030\n",
            "Epoch 00213: val_categorical_accuracy did not improve from 0.72190\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.2942 - categorical_accuracy: 0.6030 - val_loss: 4.0771 - val_categorical_accuracy: 0.7197 - lr: 9.3875e-06\n",
            "Epoch 214/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2945 - categorical_accuracy: 0.6032\n",
            "Epoch 00214: val_categorical_accuracy did not improve from 0.72190\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.2945 - categorical_accuracy: 0.6032 - val_loss: 4.0723 - val_categorical_accuracy: 0.7180 - lr: 9.3875e-06\n",
            "Epoch 215/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2943 - categorical_accuracy: 0.6016\n",
            "Epoch 00215: val_categorical_accuracy did not improve from 0.72190\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.2943 - categorical_accuracy: 0.6016 - val_loss: 4.0745 - val_categorical_accuracy: 0.7181 - lr: 9.3875e-06\n",
            "Epoch 216/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2950 - categorical_accuracy: 0.5964\n",
            "Epoch 00216: val_categorical_accuracy did not improve from 0.72190\n",
            "\n",
            "Epoch 00216: ReduceLROnPlateau reducing learning rate to 6.571236372110434e-06.\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.2950 - categorical_accuracy: 0.5964 - val_loss: 4.0760 - val_categorical_accuracy: 0.7179 - lr: 9.3875e-06\n",
            "Epoch 217/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2946 - categorical_accuracy: 0.5999\n",
            "Epoch 00217: val_categorical_accuracy did not improve from 0.72190\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.2946 - categorical_accuracy: 0.5999 - val_loss: 4.0741 - val_categorical_accuracy: 0.7189 - lr: 6.5712e-06\n",
            "Epoch 218/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2945 - categorical_accuracy: 0.6026\n",
            "Epoch 00218: val_categorical_accuracy did not improve from 0.72190\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.2945 - categorical_accuracy: 0.6026 - val_loss: 4.0755 - val_categorical_accuracy: 0.7205 - lr: 6.5712e-06\n",
            "Epoch 219/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2951 - categorical_accuracy: 0.6004\n",
            "Epoch 00219: val_categorical_accuracy did not improve from 0.72190\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.2951 - categorical_accuracy: 0.6004 - val_loss: 4.0766 - val_categorical_accuracy: 0.7190 - lr: 6.5712e-06\n",
            "Epoch 220/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2943 - categorical_accuracy: 0.6052\n",
            "Epoch 00220: val_categorical_accuracy did not improve from 0.72190\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.2943 - categorical_accuracy: 0.6052 - val_loss: 4.0775 - val_categorical_accuracy: 0.7193 - lr: 6.5712e-06\n",
            "Epoch 221/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2940 - categorical_accuracy: 0.6060\n",
            "Epoch 00221: val_categorical_accuracy did not improve from 0.72190\n",
            "\n",
            "Epoch 00221: ReduceLROnPlateau reducing learning rate to 4.599865587806561e-06.\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.2940 - categorical_accuracy: 0.6060 - val_loss: 4.0801 - val_categorical_accuracy: 0.7208 - lr: 6.5712e-06\n",
            "Epoch 222/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2947 - categorical_accuracy: 0.5995\n",
            "Epoch 00222: val_categorical_accuracy did not improve from 0.72190\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.2947 - categorical_accuracy: 0.5995 - val_loss: 4.0773 - val_categorical_accuracy: 0.7188 - lr: 4.5999e-06\n",
            "Epoch 223/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2941 - categorical_accuracy: 0.6023\n",
            "Epoch 00223: val_categorical_accuracy did not improve from 0.72190\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.2941 - categorical_accuracy: 0.6023 - val_loss: 4.0771 - val_categorical_accuracy: 0.7197 - lr: 4.5999e-06\n",
            "Epoch 224/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2939 - categorical_accuracy: 0.6030\n",
            "Epoch 00224: val_categorical_accuracy did not improve from 0.72190\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.2939 - categorical_accuracy: 0.6030 - val_loss: 4.0758 - val_categorical_accuracy: 0.7194 - lr: 4.5999e-06\n",
            "Epoch 225/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2944 - categorical_accuracy: 0.6024\n",
            "Epoch 00225: val_categorical_accuracy did not improve from 0.72190\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.2944 - categorical_accuracy: 0.6024 - val_loss: 4.0808 - val_categorical_accuracy: 0.7200 - lr: 4.5999e-06\n",
            "Epoch 226/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2946 - categorical_accuracy: 0.6025\n",
            "Epoch 00226: val_categorical_accuracy did not improve from 0.72190\n",
            "\n",
            "Epoch 00226: ReduceLROnPlateau reducing learning rate to 3.2199057841353348e-06.\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.2946 - categorical_accuracy: 0.6025 - val_loss: 4.0793 - val_categorical_accuracy: 0.7175 - lr: 4.5999e-06\n",
            "Epoch 227/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2948 - categorical_accuracy: 0.5992\n",
            "Epoch 00227: val_categorical_accuracy did not improve from 0.72190\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.2948 - categorical_accuracy: 0.5992 - val_loss: 4.0822 - val_categorical_accuracy: 0.7183 - lr: 3.2199e-06\n",
            "Epoch 228/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2940 - categorical_accuracy: 0.6025\n",
            "Epoch 00228: val_categorical_accuracy did not improve from 0.72190\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.2940 - categorical_accuracy: 0.6025 - val_loss: 4.0784 - val_categorical_accuracy: 0.7190 - lr: 3.2199e-06\n",
            "Epoch 229/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2940 - categorical_accuracy: 0.6036\n",
            "Epoch 00229: val_categorical_accuracy did not improve from 0.72190\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.2940 - categorical_accuracy: 0.6036 - val_loss: 4.0779 - val_categorical_accuracy: 0.7196 - lr: 3.2199e-06\n",
            "Epoch 230/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2945 - categorical_accuracy: 0.6025\n",
            "Epoch 00230: val_categorical_accuracy did not improve from 0.72190\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.2945 - categorical_accuracy: 0.6025 - val_loss: 4.0777 - val_categorical_accuracy: 0.7204 - lr: 3.2199e-06\n",
            "Epoch 231/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2944 - categorical_accuracy: 0.6014\n",
            "Epoch 00231: val_categorical_accuracy did not improve from 0.72190\n",
            "\n",
            "Epoch 00231: ReduceLROnPlateau reducing learning rate to 2.2539339852301055e-06.\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.2944 - categorical_accuracy: 0.6014 - val_loss: 4.0757 - val_categorical_accuracy: 0.7197 - lr: 3.2199e-06\n",
            "Epoch 232/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2944 - categorical_accuracy: 0.6001\n",
            "Epoch 00232: val_categorical_accuracy did not improve from 0.72190\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.2944 - categorical_accuracy: 0.6001 - val_loss: 4.0775 - val_categorical_accuracy: 0.7199 - lr: 2.2539e-06\n",
            "Epoch 233/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2940 - categorical_accuracy: 0.6033\n",
            "Epoch 00233: val_categorical_accuracy did not improve from 0.72190\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.2940 - categorical_accuracy: 0.6033 - val_loss: 4.0817 - val_categorical_accuracy: 0.7200 - lr: 2.2539e-06\n",
            "Epoch 234/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2940 - categorical_accuracy: 0.6018\n",
            "Epoch 00234: val_categorical_accuracy did not improve from 0.72190\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.2940 - categorical_accuracy: 0.6018 - val_loss: 4.0769 - val_categorical_accuracy: 0.7191 - lr: 2.2539e-06\n",
            "Epoch 235/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2943 - categorical_accuracy: 0.6014\n",
            "Epoch 00235: val_categorical_accuracy did not improve from 0.72190\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.2943 - categorical_accuracy: 0.6014 - val_loss: 4.0803 - val_categorical_accuracy: 0.7201 - lr: 2.2539e-06\n",
            "Epoch 236/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2941 - categorical_accuracy: 0.6031\n",
            "Epoch 00236: val_categorical_accuracy did not improve from 0.72190\n",
            "\n",
            "Epoch 00236: ReduceLROnPlateau reducing learning rate to 1.5777537100802872e-06.\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.2941 - categorical_accuracy: 0.6031 - val_loss: 4.0783 - val_categorical_accuracy: 0.7208 - lr: 2.2539e-06\n",
            "Epoch 237/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2937 - categorical_accuracy: 0.6036\n",
            "Epoch 00237: val_categorical_accuracy did not improve from 0.72190\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.2937 - categorical_accuracy: 0.6036 - val_loss: 4.0802 - val_categorical_accuracy: 0.7199 - lr: 1.5778e-06\n",
            "Epoch 238/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2941 - categorical_accuracy: 0.6008\n",
            "Epoch 00238: val_categorical_accuracy did not improve from 0.72190\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.2941 - categorical_accuracy: 0.6008 - val_loss: 4.0746 - val_categorical_accuracy: 0.7199 - lr: 1.5778e-06\n",
            "Epoch 239/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2949 - categorical_accuracy: 0.6011\n",
            "Epoch 00239: val_categorical_accuracy did not improve from 0.72190\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.2949 - categorical_accuracy: 0.6011 - val_loss: 4.0787 - val_categorical_accuracy: 0.7198 - lr: 1.5778e-06\n",
            "Epoch 240/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2945 - categorical_accuracy: 0.6028\n",
            "Epoch 00240: val_categorical_accuracy did not improve from 0.72190\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.2945 - categorical_accuracy: 0.6028 - val_loss: 4.0725 - val_categorical_accuracy: 0.7195 - lr: 1.5778e-06\n",
            "Epoch 241/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2944 - categorical_accuracy: 0.5993\n",
            "Epoch 00241: val_categorical_accuracy did not improve from 0.72190\n",
            "\n",
            "Epoch 00241: ReduceLROnPlateau reducing learning rate to 1.1044275652238866e-06.\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.2944 - categorical_accuracy: 0.5993 - val_loss: 4.0709 - val_categorical_accuracy: 0.7191 - lr: 1.5778e-06\n",
            "Epoch 242/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2936 - categorical_accuracy: 0.6021\n",
            "Epoch 00242: val_categorical_accuracy did not improve from 0.72190\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.2936 - categorical_accuracy: 0.6021 - val_loss: 4.0735 - val_categorical_accuracy: 0.7199 - lr: 1.1044e-06\n",
            "Epoch 243/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2943 - categorical_accuracy: 0.5976\n",
            "Epoch 00243: val_categorical_accuracy did not improve from 0.72190\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.2943 - categorical_accuracy: 0.5976 - val_loss: 4.0738 - val_categorical_accuracy: 0.7206 - lr: 1.1044e-06\n",
            "Epoch 244/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2943 - categorical_accuracy: 0.6012\n",
            "Epoch 00244: val_categorical_accuracy did not improve from 0.72190\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.2943 - categorical_accuracy: 0.6012 - val_loss: 4.0770 - val_categorical_accuracy: 0.7198 - lr: 1.1044e-06\n",
            "Epoch 245/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2946 - categorical_accuracy: 0.6000\n",
            "Epoch 00245: val_categorical_accuracy did not improve from 0.72190\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.2946 - categorical_accuracy: 0.6000 - val_loss: 4.0823 - val_categorical_accuracy: 0.7183 - lr: 1.1044e-06\n",
            "Epoch 246/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2939 - categorical_accuracy: 0.6044\n",
            "Epoch 00246: val_categorical_accuracy did not improve from 0.72190\n",
            "\n",
            "Epoch 00246: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.2939 - categorical_accuracy: 0.6044 - val_loss: 4.0813 - val_categorical_accuracy: 0.7219 - lr: 1.1044e-06\n",
            "Epoch 247/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2944 - categorical_accuracy: 0.6012\n",
            "Epoch 00247: val_categorical_accuracy did not improve from 0.72190\n",
            "1562/1562 [==============================] - 176s 113ms/step - loss: 4.2944 - categorical_accuracy: 0.6012 - val_loss: 4.0783 - val_categorical_accuracy: 0.7196 - lr: 1.0000e-06\n",
            "Epoch 248/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2948 - categorical_accuracy: 0.6002\n",
            "Epoch 00248: val_categorical_accuracy did not improve from 0.72190\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.2948 - categorical_accuracy: 0.6002 - val_loss: 4.0807 - val_categorical_accuracy: 0.7193 - lr: 1.0000e-06\n",
            "Epoch 249/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2941 - categorical_accuracy: 0.6044\n",
            "Epoch 00249: val_categorical_accuracy did not improve from 0.72190\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.2941 - categorical_accuracy: 0.6044 - val_loss: 4.0777 - val_categorical_accuracy: 0.7186 - lr: 1.0000e-06\n",
            "Epoch 250/250\n",
            "1562/1562 [==============================] - ETA: 0s - loss: 4.2942 - categorical_accuracy: 0.6048\n",
            "Epoch 00250: val_categorical_accuracy did not improve from 0.72190\n",
            "1562/1562 [==============================] - 177s 113ms/step - loss: 4.2942 - categorical_accuracy: 0.6048 - val_loss: 4.0739 - val_categorical_accuracy: 0.7207 - lr: 1.0000e-06\n",
            "313/313 [==============================] - 10s 31ms/step - loss: 4.0739 - categorical_accuracy: 0.7207\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4.073850631713867, 0.7207000255584717]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize = (9,6))\n",
        "plt.plot(history_teacher.history['val_loss'],label='Test loss')\n",
        "plt.plot(history_teacher.history['loss'],label='Train loss')\n",
        "plt.title('Loss curve for resnet model')\n",
        "plt.ylabel('Loss value')\n",
        "plt.xlabel('No. epoch')\n",
        "plt.show()\n",
        "\n",
        "# Plot history: Accuracy\n",
        "plt.figure(figsize = (9,6))\n",
        "plt.plot(history_teacher.history['val_categorical_accuracy'],label = 'Test accuracy')\n",
        "plt.plot(history_teacher.history['categorical_accuracy'],label = 'Train accuracy')\n",
        "plt.title('Accuracy curve for resnet model')\n",
        "plt.ylabel('Accuracy value (%)')\n",
        "plt.xlabel('No. epoch')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "2L3ki1WWzaSw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 791
        },
        "outputId": "a5b46588-3509-449f-ce12-2bac5f416e04"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 648x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 648x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "teacher = keras.models.load_model(model_path)"
      ],
      "metadata": {
        "id": "l8deT9rZTP1r"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.backend.clear_session()"
      ],
      "metadata": {
        "id": "HnjhI6SennWm"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "distiller = Distiller(student=student, teacher=teacher)\n",
        "distiller.compile(\n",
        "    optimizer=keras.optimizers.SGD(lr=learning_rate, decay=lr_decay, momentum=0.9, nesterov=True),\n",
        "    metrics=[keras.metrics.CategoricalAccuracy()],\n",
        "    student_loss_fn=keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "    distillation_loss_fn=keras.losses.KLDivergence(),\n",
        "    alpha=0.2,\n",
        "    temperature=200,\n",
        ")\n",
        "callbacks = [ \n",
        "    tf.keras.callbacks.ModelCheckpoint('Desktop/Trained_models/student_keras_vgg16_cifar10.h5', monitor='val_categorical_accuracy', verbose=1, save_best_only=True),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau( factor = 0.7, patience = 5, min_lr = 0.000001, verbose = 1, monitor='val_student_loss'), #patience = 7 and 20 for cifar-100 , patience = 5 and 10 for cifar-10\n",
        "    #tf.keras.callbacks.EarlyStopping(monitor = 'val_accuracy' , patience = 10)\n",
        "  ]\n",
        "# Distill teacher to student\n",
        "history_distiller = distiller.fit_generator(datagen.flow(x_train, y_train,\n",
        "                                  batch_size=batch_size),\n",
        "                                  steps_per_epoch=x_train.shape[0] // batch_size,\n",
        "                                  epochs=200,\n",
        "                                  validation_data=(x_test, y_test),\n",
        "                                  callbacks = callbacks)\n",
        "\n",
        "# Evaluate student on test dataset\n",
        "distiller.evaluate(x_test, y_test)"
      ],
      "metadata": {
        "id": "-u-x9gYOoCW0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6055170-0908-4bfe-fd27-539776231bdc"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.0348 - student_loss: 4.6023 - distillation_loss: 1.2042e-07\n",
            "Epoch 00001: val_categorical_accuracy improved from -inf to 0.02380, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.0348 - student_loss: 4.6023 - distillation_loss: 1.2042e-07 - val_categorical_accuracy: 0.0238 - val_student_loss: 4.6032 - lr: 0.1000\n",
            "Epoch 2/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.0698 - student_loss: 4.6000 - distillation_loss: 1.1920e-07\n",
            "Epoch 00002: val_categorical_accuracy improved from 0.02380 to 0.03280, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.0698 - student_loss: 4.6000 - distillation_loss: 1.1919e-07 - val_categorical_accuracy: 0.0328 - val_student_loss: 4.6019 - lr: 0.1000\n",
            "Epoch 3/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.0953 - student_loss: 4.5989 - distillation_loss: 1.1923e-07\n",
            "Epoch 00003: val_categorical_accuracy improved from 0.03280 to 0.03700, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.0953 - student_loss: 4.5989 - distillation_loss: 1.1924e-07 - val_categorical_accuracy: 0.0370 - val_student_loss: 4.6010 - lr: 0.1000\n",
            "Epoch 4/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.1121 - student_loss: 4.5981 - distillation_loss: 1.1872e-07\n",
            "Epoch 00004: val_categorical_accuracy improved from 0.03700 to 0.03980, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.1121 - student_loss: 4.5981 - distillation_loss: 1.1874e-07 - val_categorical_accuracy: 0.0398 - val_student_loss: 4.6003 - lr: 0.1000\n",
            "Epoch 5/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.1281 - student_loss: 4.5974 - distillation_loss: 1.1856e-07\n",
            "Epoch 00005: val_categorical_accuracy improved from 0.03980 to 0.04550, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.1281 - student_loss: 4.5974 - distillation_loss: 1.1856e-07 - val_categorical_accuracy: 0.0455 - val_student_loss: 4.6000 - lr: 0.1000\n",
            "Epoch 6/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.1377 - student_loss: 4.5968 - distillation_loss: 1.1866e-07\n",
            "Epoch 00006: val_categorical_accuracy improved from 0.04550 to 0.06260, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.1377 - student_loss: 4.5968 - distillation_loss: 1.1866e-07 - val_categorical_accuracy: 0.0626 - val_student_loss: 4.6002 - lr: 0.1000\n",
            "Epoch 7/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.1482 - student_loss: 4.5962 - distillation_loss: 1.1843e-07\n",
            "Epoch 00007: val_categorical_accuracy did not improve from 0.06260\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.1482 - student_loss: 4.5962 - distillation_loss: 1.1843e-07 - val_categorical_accuracy: 0.0589 - val_student_loss: 4.5998 - lr: 0.1000\n",
            "Epoch 8/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.1567 - student_loss: 4.5956 - distillation_loss: 1.1851e-07\n",
            "Epoch 00008: val_categorical_accuracy did not improve from 0.06260\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.1567 - student_loss: 4.5956 - distillation_loss: 1.1851e-07 - val_categorical_accuracy: 0.0577 - val_student_loss: 4.5997 - lr: 0.1000\n",
            "Epoch 9/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.1643 - student_loss: 4.5951 - distillation_loss: 1.1810e-07\n",
            "Epoch 00009: val_categorical_accuracy improved from 0.06260 to 0.06400, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "1562/1562 [==============================] - 221s 141ms/step - categorical_accuracy: 0.1643 - student_loss: 4.5951 - distillation_loss: 1.1809e-07 - val_categorical_accuracy: 0.0640 - val_student_loss: 4.5995 - lr: 0.1000\n",
            "Epoch 10/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.1707 - student_loss: 4.5945 - distillation_loss: 1.1779e-07\n",
            "Epoch 00010: val_categorical_accuracy improved from 0.06400 to 0.06600, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.1707 - student_loss: 4.5945 - distillation_loss: 1.1780e-07 - val_categorical_accuracy: 0.0660 - val_student_loss: 4.5998 - lr: 0.1000\n",
            "Epoch 11/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.1790 - student_loss: 4.5940 - distillation_loss: 1.1819e-07\n",
            "Epoch 00011: val_categorical_accuracy did not improve from 0.06600\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.1790 - student_loss: 4.5940 - distillation_loss: 1.1818e-07 - val_categorical_accuracy: 0.0633 - val_student_loss: 4.5995 - lr: 0.1000\n",
            "Epoch 12/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.1865 - student_loss: 4.5933 - distillation_loss: 1.1814e-07\n",
            "Epoch 00012: val_categorical_accuracy improved from 0.06600 to 0.08070, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.1865 - student_loss: 4.5933 - distillation_loss: 1.1814e-07 - val_categorical_accuracy: 0.0807 - val_student_loss: 4.5988 - lr: 0.1000\n",
            "Epoch 13/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.1907 - student_loss: 4.5928 - distillation_loss: 1.1803e-07\n",
            "Epoch 00013: val_categorical_accuracy improved from 0.08070 to 0.08750, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.1907 - student_loss: 4.5928 - distillation_loss: 1.1802e-07 - val_categorical_accuracy: 0.0875 - val_student_loss: 4.5982 - lr: 0.1000\n",
            "Epoch 14/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.2019 - student_loss: 4.5922 - distillation_loss: 1.1750e-07\n",
            "Epoch 00014: val_categorical_accuracy improved from 0.08750 to 0.09450, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.2019 - student_loss: 4.5922 - distillation_loss: 1.1751e-07 - val_categorical_accuracy: 0.0945 - val_student_loss: 4.5970 - lr: 0.1000\n",
            "Epoch 15/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.2044 - student_loss: 4.5916 - distillation_loss: 1.1750e-07\n",
            "Epoch 00015: val_categorical_accuracy improved from 0.09450 to 0.09620, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.2044 - student_loss: 4.5916 - distillation_loss: 1.1751e-07 - val_categorical_accuracy: 0.0962 - val_student_loss: 4.5965 - lr: 0.1000\n",
            "Epoch 16/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.2075 - student_loss: 4.5910 - distillation_loss: 1.1701e-07\n",
            "Epoch 00016: val_categorical_accuracy improved from 0.09620 to 0.09850, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.2075 - student_loss: 4.5910 - distillation_loss: 1.1701e-07 - val_categorical_accuracy: 0.0985 - val_student_loss: 4.5969 - lr: 0.1000\n",
            "Epoch 17/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.2131 - student_loss: 4.5904 - distillation_loss: 1.1728e-07\n",
            "Epoch 00017: val_categorical_accuracy did not improve from 0.09850\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.2131 - student_loss: 4.5904 - distillation_loss: 1.1728e-07 - val_categorical_accuracy: 0.0908 - val_student_loss: 4.5960 - lr: 0.1000\n",
            "Epoch 18/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.2218 - student_loss: 4.5898 - distillation_loss: 1.1701e-07\n",
            "Epoch 00018: val_categorical_accuracy improved from 0.09850 to 0.11640, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.2218 - student_loss: 4.5898 - distillation_loss: 1.1703e-07 - val_categorical_accuracy: 0.1164 - val_student_loss: 4.5946 - lr: 0.1000\n",
            "Epoch 19/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.2254 - student_loss: 4.5892 - distillation_loss: 1.1663e-07\n",
            "Epoch 00019: val_categorical_accuracy did not improve from 0.11640\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.2254 - student_loss: 4.5892 - distillation_loss: 1.1662e-07 - val_categorical_accuracy: 0.1155 - val_student_loss: 4.5945 - lr: 0.1000\n",
            "Epoch 20/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.2302 - student_loss: 4.5885 - distillation_loss: 1.1658e-07\n",
            "Epoch 00020: val_categorical_accuracy improved from 0.11640 to 0.13220, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.2302 - student_loss: 4.5885 - distillation_loss: 1.1659e-07 - val_categorical_accuracy: 0.1322 - val_student_loss: 4.5923 - lr: 0.1000\n",
            "Epoch 21/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.2335 - student_loss: 4.5879 - distillation_loss: 1.1661e-07\n",
            "Epoch 00021: val_categorical_accuracy did not improve from 0.13220\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.2335 - student_loss: 4.5879 - distillation_loss: 1.1660e-07 - val_categorical_accuracy: 0.1169 - val_student_loss: 4.5925 - lr: 0.1000\n",
            "Epoch 22/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.2401 - student_loss: 4.5873 - distillation_loss: 1.1661e-07\n",
            "Epoch 00022: val_categorical_accuracy improved from 0.13220 to 0.15510, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.2401 - student_loss: 4.5873 - distillation_loss: 1.1661e-07 - val_categorical_accuracy: 0.1551 - val_student_loss: 4.5915 - lr: 0.1000\n",
            "Epoch 23/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.2445 - student_loss: 4.5866 - distillation_loss: 1.1617e-07\n",
            "Epoch 00023: val_categorical_accuracy did not improve from 0.15510\n",
            "1562/1562 [==============================] - 219s 140ms/step - categorical_accuracy: 0.2445 - student_loss: 4.5866 - distillation_loss: 1.1615e-07 - val_categorical_accuracy: 0.1404 - val_student_loss: 4.5904 - lr: 0.1000\n",
            "Epoch 24/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.2485 - student_loss: 4.5859 - distillation_loss: 1.1630e-07\n",
            "Epoch 00024: val_categorical_accuracy did not improve from 0.15510\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.2485 - student_loss: 4.5859 - distillation_loss: 1.1630e-07 - val_categorical_accuracy: 0.1422 - val_student_loss: 4.5892 - lr: 0.1000\n",
            "Epoch 25/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.2529 - student_loss: 4.5852 - distillation_loss: 1.1558e-07\n",
            "Epoch 00025: val_categorical_accuracy improved from 0.15510 to 0.17220, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.2529 - student_loss: 4.5852 - distillation_loss: 1.1558e-07 - val_categorical_accuracy: 0.1722 - val_student_loss: 4.5878 - lr: 0.1000\n",
            "Epoch 26/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.2588 - student_loss: 4.5845 - distillation_loss: 1.1589e-07\n",
            "Epoch 00026: val_categorical_accuracy improved from 0.17220 to 0.18860, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.2588 - student_loss: 4.5845 - distillation_loss: 1.1589e-07 - val_categorical_accuracy: 0.1886 - val_student_loss: 4.5858 - lr: 0.1000\n",
            "Epoch 27/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.2592 - student_loss: 4.5839 - distillation_loss: 1.1569e-07\n",
            "Epoch 00027: val_categorical_accuracy did not improve from 0.18860\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.2592 - student_loss: 4.5839 - distillation_loss: 1.1567e-07 - val_categorical_accuracy: 0.1601 - val_student_loss: 4.5861 - lr: 0.1000\n",
            "Epoch 28/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.2653 - student_loss: 4.5831 - distillation_loss: 1.1538e-07\n",
            "Epoch 00028: val_categorical_accuracy improved from 0.18860 to 0.20100, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.2653 - student_loss: 4.5831 - distillation_loss: 1.1538e-07 - val_categorical_accuracy: 0.2010 - val_student_loss: 4.5841 - lr: 0.1000\n",
            "Epoch 29/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.2728 - student_loss: 4.5824 - distillation_loss: 1.1514e-07\n",
            "Epoch 00029: val_categorical_accuracy did not improve from 0.20100\n",
            "1562/1562 [==============================] - 219s 141ms/step - categorical_accuracy: 0.2728 - student_loss: 4.5824 - distillation_loss: 1.1513e-07 - val_categorical_accuracy: 0.1944 - val_student_loss: 4.5820 - lr: 0.1000\n",
            "Epoch 30/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.2771 - student_loss: 4.5816 - distillation_loss: 1.1513e-07\n",
            "Epoch 00030: val_categorical_accuracy improved from 0.20100 to 0.24650, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.2771 - student_loss: 4.5816 - distillation_loss: 1.1514e-07 - val_categorical_accuracy: 0.2465 - val_student_loss: 4.5804 - lr: 0.1000\n",
            "Epoch 31/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.2803 - student_loss: 4.5809 - distillation_loss: 1.1445e-07\n",
            "Epoch 00031: val_categorical_accuracy did not improve from 0.24650\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.2803 - student_loss: 4.5809 - distillation_loss: 1.1445e-07 - val_categorical_accuracy: 0.2193 - val_student_loss: 4.5796 - lr: 0.1000\n",
            "Epoch 32/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.2856 - student_loss: 4.5801 - distillation_loss: 1.1426e-07\n",
            "Epoch 00032: val_categorical_accuracy improved from 0.24650 to 0.26620, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.2856 - student_loss: 4.5801 - distillation_loss: 1.1427e-07 - val_categorical_accuracy: 0.2662 - val_student_loss: 4.5793 - lr: 0.1000\n",
            "Epoch 33/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.2898 - student_loss: 4.5794 - distillation_loss: 1.1487e-07\n",
            "Epoch 00033: val_categorical_accuracy did not improve from 0.26620\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.2898 - student_loss: 4.5794 - distillation_loss: 1.1487e-07 - val_categorical_accuracy: 0.2498 - val_student_loss: 4.5750 - lr: 0.1000\n",
            "Epoch 34/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.2941 - student_loss: 4.5785 - distillation_loss: 1.1476e-07\n",
            "Epoch 00034: val_categorical_accuracy did not improve from 0.26620\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.2941 - student_loss: 4.5785 - distillation_loss: 1.1476e-07 - val_categorical_accuracy: 0.2348 - val_student_loss: 4.5748 - lr: 0.1000\n",
            "Epoch 35/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.3002 - student_loss: 4.5776 - distillation_loss: 1.1430e-07\n",
            "Epoch 00035: val_categorical_accuracy did not improve from 0.26620\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.3002 - student_loss: 4.5776 - distillation_loss: 1.1430e-07 - val_categorical_accuracy: 0.2155 - val_student_loss: 4.5768 - lr: 0.1000\n",
            "Epoch 36/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.3041 - student_loss: 4.5768 - distillation_loss: 1.1436e-07\n",
            "Epoch 00036: val_categorical_accuracy improved from 0.26620 to 0.28360, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.3041 - student_loss: 4.5768 - distillation_loss: 1.1436e-07 - val_categorical_accuracy: 0.2836 - val_student_loss: 4.5734 - lr: 0.1000\n",
            "Epoch 37/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.3050 - student_loss: 4.5759 - distillation_loss: 1.1387e-07\n",
            "Epoch 00037: val_categorical_accuracy did not improve from 0.28360\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.3050 - student_loss: 4.5759 - distillation_loss: 1.1386e-07 - val_categorical_accuracy: 0.2792 - val_student_loss: 4.5727 - lr: 0.1000\n",
            "Epoch 38/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.3105 - student_loss: 4.5751 - distillation_loss: 1.1354e-07\n",
            "Epoch 00038: val_categorical_accuracy did not improve from 0.28360\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.3105 - student_loss: 4.5751 - distillation_loss: 1.1355e-07 - val_categorical_accuracy: 0.2796 - val_student_loss: 4.5708 - lr: 0.1000\n",
            "Epoch 39/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.3178 - student_loss: 4.5742 - distillation_loss: 1.1343e-07\n",
            "Epoch 00039: val_categorical_accuracy improved from 0.28360 to 0.29030, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.3178 - student_loss: 4.5742 - distillation_loss: 1.1342e-07 - val_categorical_accuracy: 0.2903 - val_student_loss: 4.5681 - lr: 0.1000\n",
            "Epoch 40/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.3198 - student_loss: 4.5734 - distillation_loss: 1.1324e-07\n",
            "Epoch 00040: val_categorical_accuracy did not improve from 0.29030\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.3198 - student_loss: 4.5734 - distillation_loss: 1.1323e-07 - val_categorical_accuracy: 0.2762 - val_student_loss: 4.5670 - lr: 0.1000\n",
            "Epoch 41/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.3243 - student_loss: 4.5725 - distillation_loss: 1.1344e-07\n",
            "Epoch 00041: val_categorical_accuracy did not improve from 0.29030\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.3243 - student_loss: 4.5725 - distillation_loss: 1.1344e-07 - val_categorical_accuracy: 0.2841 - val_student_loss: 4.5655 - lr: 0.1000\n",
            "Epoch 42/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.3281 - student_loss: 4.5716 - distillation_loss: 1.1322e-07\n",
            "Epoch 00042: val_categorical_accuracy did not improve from 0.29030\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.3281 - student_loss: 4.5716 - distillation_loss: 1.1320e-07 - val_categorical_accuracy: 0.2899 - val_student_loss: 4.5639 - lr: 0.1000\n",
            "Epoch 43/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.3315 - student_loss: 4.5706 - distillation_loss: 1.1274e-07\n",
            "Epoch 00043: val_categorical_accuracy did not improve from 0.29030\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.3315 - student_loss: 4.5706 - distillation_loss: 1.1273e-07 - val_categorical_accuracy: 0.2821 - val_student_loss: 4.5671 - lr: 0.1000\n",
            "Epoch 44/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.3360 - student_loss: 4.5697 - distillation_loss: 1.1264e-07\n",
            "Epoch 00044: val_categorical_accuracy improved from 0.29030 to 0.30690, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.3360 - student_loss: 4.5697 - distillation_loss: 1.1264e-07 - val_categorical_accuracy: 0.3069 - val_student_loss: 4.5636 - lr: 0.1000\n",
            "Epoch 45/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.3386 - student_loss: 4.5689 - distillation_loss: 1.1228e-07\n",
            "Epoch 00045: val_categorical_accuracy improved from 0.30690 to 0.33010, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "1562/1562 [==============================] - 221s 141ms/step - categorical_accuracy: 0.3386 - student_loss: 4.5689 - distillation_loss: 1.1228e-07 - val_categorical_accuracy: 0.3301 - val_student_loss: 4.5590 - lr: 0.1000\n",
            "Epoch 46/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.3428 - student_loss: 4.5679 - distillation_loss: 1.1230e-07\n",
            "Epoch 00046: val_categorical_accuracy improved from 0.33010 to 0.33070, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.3428 - student_loss: 4.5679 - distillation_loss: 1.1231e-07 - val_categorical_accuracy: 0.3307 - val_student_loss: 4.5614 - lr: 0.1000\n",
            "Epoch 47/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.3443 - student_loss: 4.5670 - distillation_loss: 1.1191e-07\n",
            "Epoch 00047: val_categorical_accuracy improved from 0.33070 to 0.36330, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.3443 - student_loss: 4.5670 - distillation_loss: 1.1190e-07 - val_categorical_accuracy: 0.3633 - val_student_loss: 4.5553 - lr: 0.1000\n",
            "Epoch 48/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.3514 - student_loss: 4.5660 - distillation_loss: 1.1170e-07\n",
            "Epoch 00048: val_categorical_accuracy did not improve from 0.36330\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.3514 - student_loss: 4.5660 - distillation_loss: 1.1171e-07 - val_categorical_accuracy: 0.3229 - val_student_loss: 4.5578 - lr: 0.1000\n",
            "Epoch 49/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.3557 - student_loss: 4.5650 - distillation_loss: 1.1174e-07\n",
            "Epoch 00049: val_categorical_accuracy did not improve from 0.36330\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.3557 - student_loss: 4.5650 - distillation_loss: 1.1174e-07 - val_categorical_accuracy: 0.3469 - val_student_loss: 4.5549 - lr: 0.1000\n",
            "Epoch 50/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.3583 - student_loss: 4.5641 - distillation_loss: 1.1171e-07\n",
            "Epoch 00050: val_categorical_accuracy improved from 0.36330 to 0.36750, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.3583 - student_loss: 4.5641 - distillation_loss: 1.1171e-07 - val_categorical_accuracy: 0.3675 - val_student_loss: 4.5517 - lr: 0.1000\n",
            "Epoch 51/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.3636 - student_loss: 4.5630 - distillation_loss: 1.1110e-07\n",
            "Epoch 00051: val_categorical_accuracy improved from 0.36750 to 0.37590, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.3636 - student_loss: 4.5630 - distillation_loss: 1.1111e-07 - val_categorical_accuracy: 0.3759 - val_student_loss: 4.5490 - lr: 0.1000\n",
            "Epoch 52/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.3629 - student_loss: 4.5621 - distillation_loss: 1.1117e-07\n",
            "Epoch 00052: val_categorical_accuracy did not improve from 0.37590\n",
            "1562/1562 [==============================] - 219s 140ms/step - categorical_accuracy: 0.3629 - student_loss: 4.5621 - distillation_loss: 1.1119e-07 - val_categorical_accuracy: 0.3529 - val_student_loss: 4.5520 - lr: 0.1000\n",
            "Epoch 53/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.3676 - student_loss: 4.5611 - distillation_loss: 1.1119e-07\n",
            "Epoch 00053: val_categorical_accuracy improved from 0.37590 to 0.38800, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.3676 - student_loss: 4.5610 - distillation_loss: 1.1120e-07 - val_categorical_accuracy: 0.3880 - val_student_loss: 4.5447 - lr: 0.1000\n",
            "Epoch 54/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.3719 - student_loss: 4.5601 - distillation_loss: 1.1038e-07\n",
            "Epoch 00054: val_categorical_accuracy did not improve from 0.38800\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.3719 - student_loss: 4.5601 - distillation_loss: 1.1038e-07 - val_categorical_accuracy: 0.3848 - val_student_loss: 4.5444 - lr: 0.1000\n",
            "Epoch 55/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.3732 - student_loss: 4.5590 - distillation_loss: 1.1045e-07\n",
            "Epoch 00055: val_categorical_accuracy improved from 0.38800 to 0.41230, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.3732 - student_loss: 4.5590 - distillation_loss: 1.1045e-07 - val_categorical_accuracy: 0.4123 - val_student_loss: 4.5408 - lr: 0.1000\n",
            "Epoch 56/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.3791 - student_loss: 4.5579 - distillation_loss: 1.1006e-07\n",
            "Epoch 00056: val_categorical_accuracy did not improve from 0.41230\n",
            "1562/1562 [==============================] - 219s 140ms/step - categorical_accuracy: 0.3791 - student_loss: 4.5579 - distillation_loss: 1.1005e-07 - val_categorical_accuracy: 0.4056 - val_student_loss: 4.5402 - lr: 0.1000\n",
            "Epoch 57/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.3796 - student_loss: 4.5569 - distillation_loss: 1.1016e-07\n",
            "Epoch 00057: val_categorical_accuracy improved from 0.41230 to 0.44300, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "1562/1562 [==============================] - 221s 141ms/step - categorical_accuracy: 0.3796 - student_loss: 4.5569 - distillation_loss: 1.1016e-07 - val_categorical_accuracy: 0.4430 - val_student_loss: 4.5414 - lr: 0.1000\n",
            "Epoch 58/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.3904 - student_loss: 4.5558 - distillation_loss: 1.1017e-07\n",
            "Epoch 00058: val_categorical_accuracy did not improve from 0.44300\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.3904 - student_loss: 4.5558 - distillation_loss: 1.1015e-07 - val_categorical_accuracy: 0.4025 - val_student_loss: 4.5420 - lr: 0.1000\n",
            "Epoch 59/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.3900 - student_loss: 4.5548 - distillation_loss: 1.0972e-07\n",
            "Epoch 00059: val_categorical_accuracy did not improve from 0.44300\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.3900 - student_loss: 4.5548 - distillation_loss: 1.0974e-07 - val_categorical_accuracy: 0.4117 - val_student_loss: 4.5375 - lr: 0.1000\n",
            "Epoch 60/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.3920 - student_loss: 4.5537 - distillation_loss: 1.0964e-07\n",
            "Epoch 00060: val_categorical_accuracy improved from 0.44300 to 0.44730, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "1562/1562 [==============================] - 221s 141ms/step - categorical_accuracy: 0.3920 - student_loss: 4.5537 - distillation_loss: 1.0963e-07 - val_categorical_accuracy: 0.4473 - val_student_loss: 4.5290 - lr: 0.1000\n",
            "Epoch 61/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.3941 - student_loss: 4.5527 - distillation_loss: 1.0942e-07\n",
            "Epoch 00061: val_categorical_accuracy did not improve from 0.44730\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.3941 - student_loss: 4.5527 - distillation_loss: 1.0942e-07 - val_categorical_accuracy: 0.4333 - val_student_loss: 4.5346 - lr: 0.1000\n",
            "Epoch 62/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.3972 - student_loss: 4.5517 - distillation_loss: 1.0903e-07\n",
            "Epoch 00062: val_categorical_accuracy did not improve from 0.44730\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.3972 - student_loss: 4.5517 - distillation_loss: 1.0902e-07 - val_categorical_accuracy: 0.4058 - val_student_loss: 4.5324 - lr: 0.1000\n",
            "Epoch 63/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.4077 - student_loss: 4.5503 - distillation_loss: 1.0912e-07\n",
            "Epoch 00063: val_categorical_accuracy did not improve from 0.44730\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.4077 - student_loss: 4.5503 - distillation_loss: 1.0911e-07 - val_categorical_accuracy: 0.4439 - val_student_loss: 4.5291 - lr: 0.1000\n",
            "Epoch 64/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.4057 - student_loss: 4.5495 - distillation_loss: 1.0879e-07\n",
            "Epoch 00064: val_categorical_accuracy did not improve from 0.44730\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.4057 - student_loss: 4.5495 - distillation_loss: 1.0879e-07 - val_categorical_accuracy: 0.4369 - val_student_loss: 4.5243 - lr: 0.1000\n",
            "Epoch 65/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.4126 - student_loss: 4.5482 - distillation_loss: 1.0857e-07\n",
            "Epoch 00065: val_categorical_accuracy did not improve from 0.44730\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.4126 - student_loss: 4.5482 - distillation_loss: 1.0858e-07 - val_categorical_accuracy: 0.4296 - val_student_loss: 4.5355 - lr: 0.1000\n",
            "Epoch 66/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.4151 - student_loss: 4.5471 - distillation_loss: 1.0851e-07\n",
            "Epoch 00066: val_categorical_accuracy improved from 0.44730 to 0.46260, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.4151 - student_loss: 4.5471 - distillation_loss: 1.0850e-07 - val_categorical_accuracy: 0.4626 - val_student_loss: 4.5189 - lr: 0.1000\n",
            "Epoch 67/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.4156 - student_loss: 4.5459 - distillation_loss: 1.0835e-07\n",
            "Epoch 00067: val_categorical_accuracy improved from 0.46260 to 0.47860, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.4156 - student_loss: 4.5459 - distillation_loss: 1.0836e-07 - val_categorical_accuracy: 0.4786 - val_student_loss: 4.5231 - lr: 0.1000\n",
            "Epoch 68/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.4210 - student_loss: 4.5448 - distillation_loss: 1.0796e-07\n",
            "Epoch 00068: val_categorical_accuracy did not improve from 0.47860\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.4210 - student_loss: 4.5448 - distillation_loss: 1.0798e-07 - val_categorical_accuracy: 0.4397 - val_student_loss: 4.5240 - lr: 0.1000\n",
            "Epoch 69/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.4212 - student_loss: 4.5437 - distillation_loss: 1.0746e-07\n",
            "Epoch 00069: val_categorical_accuracy did not improve from 0.47860\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.4212 - student_loss: 4.5437 - distillation_loss: 1.0747e-07 - val_categorical_accuracy: 0.4600 - val_student_loss: 4.5196 - lr: 0.1000\n",
            "Epoch 70/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.4299 - student_loss: 4.5425 - distillation_loss: 1.0764e-07\n",
            "Epoch 00070: val_categorical_accuracy improved from 0.47860 to 0.50160, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.4299 - student_loss: 4.5425 - distillation_loss: 1.0763e-07 - val_categorical_accuracy: 0.5016 - val_student_loss: 4.5156 - lr: 0.1000\n",
            "Epoch 71/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.4301 - student_loss: 4.5413 - distillation_loss: 1.0761e-07\n",
            "Epoch 00071: val_categorical_accuracy did not improve from 0.50160\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.4301 - student_loss: 4.5413 - distillation_loss: 1.0760e-07 - val_categorical_accuracy: 0.4773 - val_student_loss: 4.5079 - lr: 0.1000\n",
            "Epoch 72/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.4345 - student_loss: 4.5401 - distillation_loss: 1.0731e-07\n",
            "Epoch 00072: val_categorical_accuracy did not improve from 0.50160\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.4345 - student_loss: 4.5401 - distillation_loss: 1.0731e-07 - val_categorical_accuracy: 0.5001 - val_student_loss: 4.5047 - lr: 0.1000\n",
            "Epoch 73/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.4352 - student_loss: 4.5389 - distillation_loss: 1.0643e-07\n",
            "Epoch 00073: val_categorical_accuracy did not improve from 0.50160\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.4352 - student_loss: 4.5389 - distillation_loss: 1.0643e-07 - val_categorical_accuracy: 0.5005 - val_student_loss: 4.5057 - lr: 0.1000\n",
            "Epoch 74/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.4422 - student_loss: 4.5376 - distillation_loss: 1.0684e-07\n",
            "Epoch 00074: val_categorical_accuracy did not improve from 0.50160\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.4422 - student_loss: 4.5376 - distillation_loss: 1.0684e-07 - val_categorical_accuracy: 0.5006 - val_student_loss: 4.5079 - lr: 0.1000\n",
            "Epoch 75/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.4450 - student_loss: 4.5366 - distillation_loss: 1.0641e-07\n",
            "Epoch 00075: val_categorical_accuracy improved from 0.50160 to 0.50660, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.4450 - student_loss: 4.5366 - distillation_loss: 1.0640e-07 - val_categorical_accuracy: 0.5066 - val_student_loss: 4.5049 - lr: 0.1000\n",
            "Epoch 76/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.4461 - student_loss: 4.5354 - distillation_loss: 1.0614e-07\n",
            "Epoch 00076: val_categorical_accuracy did not improve from 0.50660\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.4461 - student_loss: 4.5354 - distillation_loss: 1.0613e-07 - val_categorical_accuracy: 0.5045 - val_student_loss: 4.5019 - lr: 0.1000\n",
            "Epoch 77/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.4515 - student_loss: 4.5342 - distillation_loss: 1.0626e-07\n",
            "Epoch 00077: val_categorical_accuracy did not improve from 0.50660\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.4515 - student_loss: 4.5342 - distillation_loss: 1.0625e-07 - val_categorical_accuracy: 0.4807 - val_student_loss: 4.5059 - lr: 0.1000\n",
            "Epoch 78/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.4551 - student_loss: 4.5329 - distillation_loss: 1.0551e-07\n",
            "Epoch 00078: val_categorical_accuracy improved from 0.50660 to 0.51400, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.4551 - student_loss: 4.5329 - distillation_loss: 1.0553e-07 - val_categorical_accuracy: 0.5140 - val_student_loss: 4.4967 - lr: 0.1000\n",
            "Epoch 79/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.4592 - student_loss: 4.5317 - distillation_loss: 1.0538e-07\n",
            "Epoch 00079: val_categorical_accuracy improved from 0.51400 to 0.54330, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.4592 - student_loss: 4.5317 - distillation_loss: 1.0537e-07 - val_categorical_accuracy: 0.5433 - val_student_loss: 4.4839 - lr: 0.1000\n",
            "Epoch 80/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.4628 - student_loss: 4.5305 - distillation_loss: 1.0534e-07\n",
            "Epoch 00080: val_categorical_accuracy did not improve from 0.54330\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.4628 - student_loss: 4.5305 - distillation_loss: 1.0534e-07 - val_categorical_accuracy: 0.5349 - val_student_loss: 4.4900 - lr: 0.1000\n",
            "Epoch 81/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.4662 - student_loss: 4.5292 - distillation_loss: 1.0502e-07\n",
            "Epoch 00081: val_categorical_accuracy improved from 0.54330 to 0.55240, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "1562/1562 [==============================] - 221s 141ms/step - categorical_accuracy: 0.4662 - student_loss: 4.5292 - distillation_loss: 1.0502e-07 - val_categorical_accuracy: 0.5524 - val_student_loss: 4.4833 - lr: 0.1000\n",
            "Epoch 82/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.4663 - student_loss: 4.5280 - distillation_loss: 1.0520e-07\n",
            "Epoch 00082: val_categorical_accuracy did not improve from 0.55240\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.4663 - student_loss: 4.5280 - distillation_loss: 1.0520e-07 - val_categorical_accuracy: 0.5429 - val_student_loss: 4.4837 - lr: 0.1000\n",
            "Epoch 83/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.4729 - student_loss: 4.5267 - distillation_loss: 1.0459e-07\n",
            "Epoch 00083: val_categorical_accuracy did not improve from 0.55240\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.4729 - student_loss: 4.5267 - distillation_loss: 1.0457e-07 - val_categorical_accuracy: 0.5289 - val_student_loss: 4.4837 - lr: 0.1000\n",
            "Epoch 84/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.4772 - student_loss: 4.5253 - distillation_loss: 1.0474e-07\n",
            "Epoch 00084: val_categorical_accuracy did not improve from 0.55240\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.4772 - student_loss: 4.5253 - distillation_loss: 1.0473e-07 - val_categorical_accuracy: 0.5479 - val_student_loss: 4.4774 - lr: 0.1000\n",
            "Epoch 85/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.4790 - student_loss: 4.5241 - distillation_loss: 1.0454e-07\n",
            "Epoch 00085: val_categorical_accuracy did not improve from 0.55240\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.4790 - student_loss: 4.5241 - distillation_loss: 1.0453e-07 - val_categorical_accuracy: 0.5345 - val_student_loss: 4.4828 - lr: 0.1000\n",
            "Epoch 86/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.4829 - student_loss: 4.5228 - distillation_loss: 1.0474e-07\n",
            "Epoch 00086: val_categorical_accuracy did not improve from 0.55240\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.4829 - student_loss: 4.5228 - distillation_loss: 1.0474e-07 - val_categorical_accuracy: 0.5469 - val_student_loss: 4.4708 - lr: 0.1000\n",
            "Epoch 87/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.4847 - student_loss: 4.5215 - distillation_loss: 1.0416e-07\n",
            "Epoch 00087: val_categorical_accuracy did not improve from 0.55240\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.4847 - student_loss: 4.5216 - distillation_loss: 1.0416e-07 - val_categorical_accuracy: 0.5509 - val_student_loss: 4.4663 - lr: 0.1000\n",
            "Epoch 88/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.4903 - student_loss: 4.5201 - distillation_loss: 1.0365e-07\n",
            "Epoch 00088: val_categorical_accuracy improved from 0.55240 to 0.56130, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.4903 - student_loss: 4.5201 - distillation_loss: 1.0366e-07 - val_categorical_accuracy: 0.5613 - val_student_loss: 4.4742 - lr: 0.1000\n",
            "Epoch 89/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.4961 - student_loss: 4.5189 - distillation_loss: 1.0373e-07\n",
            "Epoch 00089: val_categorical_accuracy did not improve from 0.56130\n",
            "1562/1562 [==============================] - 219s 141ms/step - categorical_accuracy: 0.4961 - student_loss: 4.5189 - distillation_loss: 1.0374e-07 - val_categorical_accuracy: 0.5556 - val_student_loss: 4.4592 - lr: 0.1000\n",
            "Epoch 90/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.4979 - student_loss: 4.5174 - distillation_loss: 1.0327e-07\n",
            "Epoch 00090: val_categorical_accuracy improved from 0.56130 to 0.57120, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.4979 - student_loss: 4.5174 - distillation_loss: 1.0328e-07 - val_categorical_accuracy: 0.5712 - val_student_loss: 4.4511 - lr: 0.1000\n",
            "Epoch 91/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.5011 - student_loss: 4.5161 - distillation_loss: 1.0295e-07\n",
            "Epoch 00091: val_categorical_accuracy did not improve from 0.57120\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.5011 - student_loss: 4.5161 - distillation_loss: 1.0295e-07 - val_categorical_accuracy: 0.5638 - val_student_loss: 4.4554 - lr: 0.1000\n",
            "Epoch 92/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.5036 - student_loss: 4.5150 - distillation_loss: 1.0298e-07\n",
            "Epoch 00092: val_categorical_accuracy did not improve from 0.57120\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.5036 - student_loss: 4.5150 - distillation_loss: 1.0299e-07 - val_categorical_accuracy: 0.5461 - val_student_loss: 4.4634 - lr: 0.1000\n",
            "Epoch 93/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.5075 - student_loss: 4.5134 - distillation_loss: 1.0279e-07\n",
            "Epoch 00093: val_categorical_accuracy did not improve from 0.57120\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.5075 - student_loss: 4.5134 - distillation_loss: 1.0280e-07 - val_categorical_accuracy: 0.5705 - val_student_loss: 4.4547 - lr: 0.1000\n",
            "Epoch 94/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.5096 - student_loss: 4.5122 - distillation_loss: 1.0272e-07\n",
            "Epoch 00094: val_categorical_accuracy did not improve from 0.57120\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.5096 - student_loss: 4.5122 - distillation_loss: 1.0272e-07 - val_categorical_accuracy: 0.5510 - val_student_loss: 4.4622 - lr: 0.1000\n",
            "Epoch 95/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.5148 - student_loss: 4.5108 - distillation_loss: 1.0251e-07\n",
            "Epoch 00095: val_categorical_accuracy improved from 0.57120 to 0.57800, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.5148 - student_loss: 4.5108 - distillation_loss: 1.0251e-07 - val_categorical_accuracy: 0.5780 - val_student_loss: 4.4468 - lr: 0.1000\n",
            "Epoch 96/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.5212 - student_loss: 4.5093 - distillation_loss: 1.0233e-07\n",
            "Epoch 00096: val_categorical_accuracy did not improve from 0.57800\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.5212 - student_loss: 4.5093 - distillation_loss: 1.0232e-07 - val_categorical_accuracy: 0.5687 - val_student_loss: 4.4451 - lr: 0.1000\n",
            "Epoch 97/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.5220 - student_loss: 4.5079 - distillation_loss: 1.0181e-07\n",
            "Epoch 00097: val_categorical_accuracy did not improve from 0.57800\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.5220 - student_loss: 4.5079 - distillation_loss: 1.0181e-07 - val_categorical_accuracy: 0.5732 - val_student_loss: 4.4396 - lr: 0.1000\n",
            "Epoch 98/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.5239 - student_loss: 4.5068 - distillation_loss: 1.0181e-07\n",
            "Epoch 00098: val_categorical_accuracy did not improve from 0.57800\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.5239 - student_loss: 4.5068 - distillation_loss: 1.0179e-07 - val_categorical_accuracy: 0.5683 - val_student_loss: 4.4425 - lr: 0.1000\n",
            "Epoch 99/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.5310 - student_loss: 4.5053 - distillation_loss: 1.0139e-07\n",
            "Epoch 00099: val_categorical_accuracy improved from 0.57800 to 0.57830, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.5310 - student_loss: 4.5053 - distillation_loss: 1.0141e-07 - val_categorical_accuracy: 0.5783 - val_student_loss: 4.4335 - lr: 0.1000\n",
            "Epoch 100/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.5312 - student_loss: 4.5039 - distillation_loss: 1.0141e-07\n",
            "Epoch 00100: val_categorical_accuracy did not improve from 0.57830\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.5312 - student_loss: 4.5039 - distillation_loss: 1.0141e-07 - val_categorical_accuracy: 0.5780 - val_student_loss: 4.4262 - lr: 0.1000\n",
            "Epoch 101/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.5366 - student_loss: 4.5025 - distillation_loss: 1.0123e-07\n",
            "Epoch 00101: val_categorical_accuracy improved from 0.57830 to 0.59110, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.5366 - student_loss: 4.5025 - distillation_loss: 1.0125e-07 - val_categorical_accuracy: 0.5911 - val_student_loss: 4.4321 - lr: 0.1000\n",
            "Epoch 102/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.5389 - student_loss: 4.5011 - distillation_loss: 1.0106e-07\n",
            "Epoch 00102: val_categorical_accuracy did not improve from 0.59110\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.5389 - student_loss: 4.5011 - distillation_loss: 1.0106e-07 - val_categorical_accuracy: 0.5787 - val_student_loss: 4.4308 - lr: 0.1000\n",
            "Epoch 103/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.5445 - student_loss: 4.4998 - distillation_loss: 1.0122e-07\n",
            "Epoch 00103: val_categorical_accuracy improved from 0.59110 to 0.59340, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.5445 - student_loss: 4.4998 - distillation_loss: 1.0121e-07 - val_categorical_accuracy: 0.5934 - val_student_loss: 4.4232 - lr: 0.1000\n",
            "Epoch 104/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.5496 - student_loss: 4.4983 - distillation_loss: 1.0093e-07\n",
            "Epoch 00104: val_categorical_accuracy did not improve from 0.59340\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.5496 - student_loss: 4.4982 - distillation_loss: 1.0094e-07 - val_categorical_accuracy: 0.5800 - val_student_loss: 4.4256 - lr: 0.1000\n",
            "Epoch 105/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.5513 - student_loss: 4.4968 - distillation_loss: 1.0046e-07\n",
            "Epoch 00105: val_categorical_accuracy improved from 0.59340 to 0.60530, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.5513 - student_loss: 4.4968 - distillation_loss: 1.0047e-07 - val_categorical_accuracy: 0.6053 - val_student_loss: 4.4110 - lr: 0.1000\n",
            "Epoch 106/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.5538 - student_loss: 4.4955 - distillation_loss: 1.0018e-07\n",
            "Epoch 00106: val_categorical_accuracy did not improve from 0.60530\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.5538 - student_loss: 4.4955 - distillation_loss: 1.0017e-07 - val_categorical_accuracy: 0.6009 - val_student_loss: 4.4198 - lr: 0.1000\n",
            "Epoch 107/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.5560 - student_loss: 4.4941 - distillation_loss: 1.0008e-07\n",
            "Epoch 00107: val_categorical_accuracy did not improve from 0.60530\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.5560 - student_loss: 4.4941 - distillation_loss: 1.0010e-07 - val_categorical_accuracy: 0.5946 - val_student_loss: 4.4190 - lr: 0.1000\n",
            "Epoch 108/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.5589 - student_loss: 4.4925 - distillation_loss: 1.0010e-07\n",
            "Epoch 00108: val_categorical_accuracy did not improve from 0.60530\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.5589 - student_loss: 4.4925 - distillation_loss: 1.0009e-07 - val_categorical_accuracy: 0.5943 - val_student_loss: 4.4183 - lr: 0.1000\n",
            "Epoch 109/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.5640 - student_loss: 4.4909 - distillation_loss: 9.9305e-08\n",
            "Epoch 00109: val_categorical_accuracy improved from 0.60530 to 0.61290, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.5640 - student_loss: 4.4909 - distillation_loss: 9.9302e-08 - val_categorical_accuracy: 0.6129 - val_student_loss: 4.4016 - lr: 0.1000\n",
            "Epoch 110/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.5710 - student_loss: 4.4898 - distillation_loss: 9.9503e-08\n",
            "Epoch 00110: val_categorical_accuracy improved from 0.61290 to 0.61630, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.5710 - student_loss: 4.4898 - distillation_loss: 9.9511e-08 - val_categorical_accuracy: 0.6163 - val_student_loss: 4.4006 - lr: 0.1000\n",
            "Epoch 111/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.5718 - student_loss: 4.4884 - distillation_loss: 9.9174e-08\n",
            "Epoch 00111: val_categorical_accuracy did not improve from 0.61630\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.5718 - student_loss: 4.4884 - distillation_loss: 9.9169e-08 - val_categorical_accuracy: 0.6027 - val_student_loss: 4.4029 - lr: 0.1000\n",
            "Epoch 112/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.5762 - student_loss: 4.4866 - distillation_loss: 9.8972e-08\n",
            "Epoch 00112: val_categorical_accuracy did not improve from 0.61630\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.5762 - student_loss: 4.4866 - distillation_loss: 9.8972e-08 - val_categorical_accuracy: 0.6146 - val_student_loss: 4.3831 - lr: 0.1000\n",
            "Epoch 113/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.5793 - student_loss: 4.4851 - distillation_loss: 9.8834e-08\n",
            "Epoch 00113: val_categorical_accuracy improved from 0.61630 to 0.61960, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.5793 - student_loss: 4.4851 - distillation_loss: 9.8844e-08 - val_categorical_accuracy: 0.6196 - val_student_loss: 4.3925 - lr: 0.1000\n",
            "Epoch 114/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.5868 - student_loss: 4.4835 - distillation_loss: 9.8573e-08\n",
            "Epoch 00114: val_categorical_accuracy did not improve from 0.61960\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.5868 - student_loss: 4.4835 - distillation_loss: 9.8576e-08 - val_categorical_accuracy: 0.6172 - val_student_loss: 4.3799 - lr: 0.1000\n",
            "Epoch 115/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.5886 - student_loss: 4.4821 - distillation_loss: 9.8428e-08\n",
            "Epoch 00115: val_categorical_accuracy did not improve from 0.61960\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.5886 - student_loss: 4.4821 - distillation_loss: 9.8432e-08 - val_categorical_accuracy: 0.6071 - val_student_loss: 4.4006 - lr: 0.1000\n",
            "Epoch 116/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.5884 - student_loss: 4.4808 - distillation_loss: 9.8429e-08\n",
            "Epoch 00116: val_categorical_accuracy improved from 0.61960 to 0.63010, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.5884 - student_loss: 4.4808 - distillation_loss: 9.8418e-08 - val_categorical_accuracy: 0.6301 - val_student_loss: 4.3802 - lr: 0.1000\n",
            "Epoch 117/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.5941 - student_loss: 4.4794 - distillation_loss: 9.8091e-08\n",
            "Epoch 00117: val_categorical_accuracy did not improve from 0.63010\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.5941 - student_loss: 4.4794 - distillation_loss: 9.8104e-08 - val_categorical_accuracy: 0.6193 - val_student_loss: 4.3707 - lr: 0.1000\n",
            "Epoch 118/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.5987 - student_loss: 4.4778 - distillation_loss: 9.8175e-08\n",
            "Epoch 00118: val_categorical_accuracy did not improve from 0.63010\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.5987 - student_loss: 4.4778 - distillation_loss: 9.8189e-08 - val_categorical_accuracy: 0.6275 - val_student_loss: 4.3705 - lr: 0.1000\n",
            "Epoch 119/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.6001 - student_loss: 4.4763 - distillation_loss: 9.7687e-08\n",
            "Epoch 00119: val_categorical_accuracy improved from 0.63010 to 0.64110, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.6001 - student_loss: 4.4763 - distillation_loss: 9.7688e-08 - val_categorical_accuracy: 0.6411 - val_student_loss: 4.3637 - lr: 0.1000\n",
            "Epoch 120/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.6059 - student_loss: 4.4747 - distillation_loss: 9.7425e-08\n",
            "Epoch 00120: val_categorical_accuracy did not improve from 0.64110\n",
            "1562/1562 [==============================] - 219s 140ms/step - categorical_accuracy: 0.6059 - student_loss: 4.4747 - distillation_loss: 9.7416e-08 - val_categorical_accuracy: 0.6311 - val_student_loss: 4.3693 - lr: 0.1000\n",
            "Epoch 121/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.6079 - student_loss: 4.4733 - distillation_loss: 9.7524e-08\n",
            "Epoch 00121: val_categorical_accuracy did not improve from 0.64110\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.6079 - student_loss: 4.4733 - distillation_loss: 9.7522e-08 - val_categorical_accuracy: 0.6269 - val_student_loss: 4.3529 - lr: 0.1000\n",
            "Epoch 122/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.6082 - student_loss: 4.4720 - distillation_loss: 9.7281e-08\n",
            "Epoch 00122: val_categorical_accuracy did not improve from 0.64110\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.6082 - student_loss: 4.4720 - distillation_loss: 9.7275e-08 - val_categorical_accuracy: 0.6276 - val_student_loss: 4.3760 - lr: 0.1000\n",
            "Epoch 123/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.6120 - student_loss: 4.4701 - distillation_loss: 9.6802e-08\n",
            "Epoch 00123: val_categorical_accuracy did not improve from 0.64110\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.6120 - student_loss: 4.4701 - distillation_loss: 9.6821e-08 - val_categorical_accuracy: 0.6336 - val_student_loss: 4.3657 - lr: 0.1000\n",
            "Epoch 124/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.6188 - student_loss: 4.4686 - distillation_loss: 9.7174e-08\n",
            "Epoch 00124: val_categorical_accuracy improved from 0.64110 to 0.64390, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.6188 - student_loss: 4.4686 - distillation_loss: 9.7191e-08 - val_categorical_accuracy: 0.6439 - val_student_loss: 4.3500 - lr: 0.1000\n",
            "Epoch 125/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.6230 - student_loss: 4.4669 - distillation_loss: 9.6515e-08\n",
            "Epoch 00125: val_categorical_accuracy did not improve from 0.64390\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.6230 - student_loss: 4.4669 - distillation_loss: 9.6524e-08 - val_categorical_accuracy: 0.6432 - val_student_loss: 4.3636 - lr: 0.1000\n",
            "Epoch 126/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.6247 - student_loss: 4.4656 - distillation_loss: 9.6994e-08\n",
            "Epoch 00126: val_categorical_accuracy did not improve from 0.64390\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.6247 - student_loss: 4.4656 - distillation_loss: 9.6980e-08 - val_categorical_accuracy: 0.6412 - val_student_loss: 4.3436 - lr: 0.1000\n",
            "Epoch 127/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.6260 - student_loss: 4.4641 - distillation_loss: 9.6378e-08\n",
            "Epoch 00127: val_categorical_accuracy did not improve from 0.64390\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.6260 - student_loss: 4.4641 - distillation_loss: 9.6382e-08 - val_categorical_accuracy: 0.6268 - val_student_loss: 4.3488 - lr: 0.1000\n",
            "Epoch 128/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.6298 - student_loss: 4.4623 - distillation_loss: 9.6447e-08\n",
            "Epoch 00128: val_categorical_accuracy improved from 0.64390 to 0.64680, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.6298 - student_loss: 4.4623 - distillation_loss: 9.6439e-08 - val_categorical_accuracy: 0.6468 - val_student_loss: 4.3376 - lr: 0.1000\n",
            "Epoch 129/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.6349 - student_loss: 4.4610 - distillation_loss: 9.6143e-08\n",
            "Epoch 00129: val_categorical_accuracy did not improve from 0.64680\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.6349 - student_loss: 4.4610 - distillation_loss: 9.6151e-08 - val_categorical_accuracy: 0.6333 - val_student_loss: 4.3417 - lr: 0.1000\n",
            "Epoch 130/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.6377 - student_loss: 4.4595 - distillation_loss: 9.6643e-08\n",
            "Epoch 00130: val_categorical_accuracy improved from 0.64680 to 0.64790, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.6377 - student_loss: 4.4595 - distillation_loss: 9.6636e-08 - val_categorical_accuracy: 0.6479 - val_student_loss: 4.3429 - lr: 0.1000\n",
            "Epoch 131/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.6404 - student_loss: 4.4578 - distillation_loss: 9.5682e-08\n",
            "Epoch 00131: val_categorical_accuracy did not improve from 0.64790\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.6404 - student_loss: 4.4578 - distillation_loss: 9.5674e-08 - val_categorical_accuracy: 0.6472 - val_student_loss: 4.3413 - lr: 0.1000\n",
            "Epoch 132/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.6398 - student_loss: 4.4565 - distillation_loss: 9.5467e-08\n",
            "Epoch 00132: val_categorical_accuracy did not improve from 0.64790\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.6398 - student_loss: 4.4565 - distillation_loss: 9.5467e-08 - val_categorical_accuracy: 0.6314 - val_student_loss: 4.3156 - lr: 0.1000\n",
            "Epoch 133/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.6481 - student_loss: 4.4548 - distillation_loss: 9.5722e-08\n",
            "Epoch 00133: val_categorical_accuracy improved from 0.64790 to 0.65160, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.6481 - student_loss: 4.4548 - distillation_loss: 9.5729e-08 - val_categorical_accuracy: 0.6516 - val_student_loss: 4.3191 - lr: 0.1000\n",
            "Epoch 134/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.6538 - student_loss: 4.4532 - distillation_loss: 9.5170e-08\n",
            "Epoch 00134: val_categorical_accuracy did not improve from 0.65160\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.6538 - student_loss: 4.4532 - distillation_loss: 9.5180e-08 - val_categorical_accuracy: 0.6418 - val_student_loss: 4.3094 - lr: 0.1000\n",
            "Epoch 135/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.6557 - student_loss: 4.4517 - distillation_loss: 9.4959e-08\n",
            "Epoch 00135: val_categorical_accuracy improved from 0.65160 to 0.65350, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.6557 - student_loss: 4.4517 - distillation_loss: 9.4958e-08 - val_categorical_accuracy: 0.6535 - val_student_loss: 4.3228 - lr: 0.1000\n",
            "Epoch 136/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.6560 - student_loss: 4.4501 - distillation_loss: 9.4926e-08\n",
            "Epoch 00136: val_categorical_accuracy did not improve from 0.65350\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.6560 - student_loss: 4.4501 - distillation_loss: 9.4923e-08 - val_categorical_accuracy: 0.6520 - val_student_loss: 4.3146 - lr: 0.1000\n",
            "Epoch 137/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.6629 - student_loss: 4.4483 - distillation_loss: 9.4802e-08\n",
            "Epoch 00137: val_categorical_accuracy did not improve from 0.65350\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.6629 - student_loss: 4.4483 - distillation_loss: 9.4814e-08 - val_categorical_accuracy: 0.6481 - val_student_loss: 4.3137 - lr: 0.1000\n",
            "Epoch 138/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.6631 - student_loss: 4.4469 - distillation_loss: 9.4802e-08\n",
            "Epoch 00138: val_categorical_accuracy improved from 0.65350 to 0.66180, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.6631 - student_loss: 4.4469 - distillation_loss: 9.4797e-08 - val_categorical_accuracy: 0.6618 - val_student_loss: 4.3149 - lr: 0.1000\n",
            "Epoch 139/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.6624 - student_loss: 4.4457 - distillation_loss: 9.4675e-08\n",
            "Epoch 00139: val_categorical_accuracy improved from 0.66180 to 0.66690, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.6624 - student_loss: 4.4457 - distillation_loss: 9.4673e-08 - val_categorical_accuracy: 0.6669 - val_student_loss: 4.2984 - lr: 0.1000\n",
            "Epoch 140/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.6694 - student_loss: 4.4437 - distillation_loss: 9.4226e-08\n",
            "Epoch 00140: val_categorical_accuracy did not improve from 0.66690\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.6694 - student_loss: 4.4437 - distillation_loss: 9.4228e-08 - val_categorical_accuracy: 0.6580 - val_student_loss: 4.3034 - lr: 0.1000\n",
            "Epoch 141/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.6711 - student_loss: 4.4422 - distillation_loss: 9.4632e-08\n",
            "Epoch 00141: val_categorical_accuracy improved from 0.66690 to 0.66930, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.6711 - student_loss: 4.4422 - distillation_loss: 9.4630e-08 - val_categorical_accuracy: 0.6693 - val_student_loss: 4.2726 - lr: 0.1000\n",
            "Epoch 142/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.6771 - student_loss: 4.4406 - distillation_loss: 9.4244e-08\n",
            "Epoch 00142: val_categorical_accuracy did not improve from 0.66930\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.6771 - student_loss: 4.4406 - distillation_loss: 9.4235e-08 - val_categorical_accuracy: 0.6585 - val_student_loss: 4.3026 - lr: 0.1000\n",
            "Epoch 143/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.6824 - student_loss: 4.4386 - distillation_loss: 9.3623e-08\n",
            "Epoch 00143: val_categorical_accuracy did not improve from 0.66930\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.6824 - student_loss: 4.4386 - distillation_loss: 9.3622e-08 - val_categorical_accuracy: 0.6620 - val_student_loss: 4.2934 - lr: 0.1000\n",
            "Epoch 144/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.6818 - student_loss: 4.4376 - distillation_loss: 9.4411e-08\n",
            "Epoch 00144: val_categorical_accuracy did not improve from 0.66930\n",
            "1562/1562 [==============================] - 219s 140ms/step - categorical_accuracy: 0.6818 - student_loss: 4.4376 - distillation_loss: 9.4405e-08 - val_categorical_accuracy: 0.6674 - val_student_loss: 4.2937 - lr: 0.1000\n",
            "Epoch 145/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.6872 - student_loss: 4.4357 - distillation_loss: 9.3812e-08\n",
            "Epoch 00145: val_categorical_accuracy did not improve from 0.66930\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.6872 - student_loss: 4.4357 - distillation_loss: 9.3806e-08 - val_categorical_accuracy: 0.6628 - val_student_loss: 4.2913 - lr: 0.1000\n",
            "Epoch 146/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.6895 - student_loss: 4.4341 - distillation_loss: 9.3981e-08\n",
            "Epoch 00146: val_categorical_accuracy improved from 0.66930 to 0.67060, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "\n",
            "Epoch 00146: ReduceLROnPlateau reducing learning rate to 0.07000000104308128.\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.6895 - student_loss: 4.4341 - distillation_loss: 9.3994e-08 - val_categorical_accuracy: 0.6706 - val_student_loss: 4.2887 - lr: 0.1000\n",
            "Epoch 147/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.6959 - student_loss: 4.4322 - distillation_loss: 9.3616e-08\n",
            "Epoch 00147: val_categorical_accuracy improved from 0.67060 to 0.68020, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.6959 - student_loss: 4.4322 - distillation_loss: 9.3619e-08 - val_categorical_accuracy: 0.6802 - val_student_loss: 4.2622 - lr: 0.0700\n",
            "Epoch 148/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.7027 - student_loss: 4.4306 - distillation_loss: 9.3126e-08\n",
            "Epoch 00148: val_categorical_accuracy did not improve from 0.68020\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.7027 - student_loss: 4.4306 - distillation_loss: 9.3128e-08 - val_categorical_accuracy: 0.6752 - val_student_loss: 4.2818 - lr: 0.0700\n",
            "Epoch 149/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.7009 - student_loss: 4.4299 - distillation_loss: 9.3201e-08\n",
            "Epoch 00149: val_categorical_accuracy did not improve from 0.68020\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.7009 - student_loss: 4.4299 - distillation_loss: 9.3200e-08 - val_categorical_accuracy: 0.6784 - val_student_loss: 4.2695 - lr: 0.0700\n",
            "Epoch 150/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.7083 - student_loss: 4.4283 - distillation_loss: 9.3516e-08\n",
            "Epoch 00150: val_categorical_accuracy did not improve from 0.68020\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.7083 - student_loss: 4.4283 - distillation_loss: 9.3516e-08 - val_categorical_accuracy: 0.6741 - val_student_loss: 4.2566 - lr: 0.0700\n",
            "Epoch 151/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.7065 - student_loss: 4.4275 - distillation_loss: 9.2606e-08\n",
            "Epoch 00151: val_categorical_accuracy improved from 0.68020 to 0.68210, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.7065 - student_loss: 4.4275 - distillation_loss: 9.2605e-08 - val_categorical_accuracy: 0.6821 - val_student_loss: 4.2647 - lr: 0.0700\n",
            "Epoch 152/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.7062 - student_loss: 4.4260 - distillation_loss: 9.3235e-08\n",
            "Epoch 00152: val_categorical_accuracy did not improve from 0.68210\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.7062 - student_loss: 4.4260 - distillation_loss: 9.3223e-08 - val_categorical_accuracy: 0.6791 - val_student_loss: 4.2695 - lr: 0.0700\n",
            "Epoch 153/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.7128 - student_loss: 4.4249 - distillation_loss: 9.3032e-08\n",
            "Epoch 00153: val_categorical_accuracy did not improve from 0.68210\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.7128 - student_loss: 4.4249 - distillation_loss: 9.3032e-08 - val_categorical_accuracy: 0.6744 - val_student_loss: 4.2722 - lr: 0.0700\n",
            "Epoch 154/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.7143 - student_loss: 4.4239 - distillation_loss: 9.2424e-08\n",
            "Epoch 00154: val_categorical_accuracy did not improve from 0.68210\n",
            "1562/1562 [==============================] - 219s 141ms/step - categorical_accuracy: 0.7143 - student_loss: 4.4239 - distillation_loss: 9.2429e-08 - val_categorical_accuracy: 0.6784 - val_student_loss: 4.2796 - lr: 0.0700\n",
            "Epoch 155/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.7150 - student_loss: 4.4225 - distillation_loss: 9.2414e-08\n",
            "Epoch 00155: val_categorical_accuracy improved from 0.68210 to 0.68640, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "\n",
            "Epoch 00155: ReduceLROnPlateau reducing learning rate to 0.04900000020861625.\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.7150 - student_loss: 4.4225 - distillation_loss: 9.2417e-08 - val_categorical_accuracy: 0.6864 - val_student_loss: 4.2701 - lr: 0.0700\n",
            "Epoch 156/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.7230 - student_loss: 4.4212 - distillation_loss: 9.2766e-08\n",
            "Epoch 00156: val_categorical_accuracy did not improve from 0.68640\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.7230 - student_loss: 4.4212 - distillation_loss: 9.2774e-08 - val_categorical_accuracy: 0.6863 - val_student_loss: 4.2636 - lr: 0.0490\n",
            "Epoch 157/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.7278 - student_loss: 4.4203 - distillation_loss: 9.2124e-08\n",
            "Epoch 00157: val_categorical_accuracy did not improve from 0.68640\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.7278 - student_loss: 4.4203 - distillation_loss: 9.2119e-08 - val_categorical_accuracy: 0.6845 - val_student_loss: 4.2480 - lr: 0.0490\n",
            "Epoch 158/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.7268 - student_loss: 4.4198 - distillation_loss: 9.2098e-08\n",
            "Epoch 00158: val_categorical_accuracy did not improve from 0.68640\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.7268 - student_loss: 4.4198 - distillation_loss: 9.2089e-08 - val_categorical_accuracy: 0.6863 - val_student_loss: 4.2472 - lr: 0.0490\n",
            "Epoch 159/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.7294 - student_loss: 4.4190 - distillation_loss: 9.2381e-08\n",
            "Epoch 00159: val_categorical_accuracy did not improve from 0.68640\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.7294 - student_loss: 4.4190 - distillation_loss: 9.2375e-08 - val_categorical_accuracy: 0.6843 - val_student_loss: 4.2582 - lr: 0.0490\n",
            "Epoch 160/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.7279 - student_loss: 4.4180 - distillation_loss: 9.2238e-08\n",
            "Epoch 00160: val_categorical_accuracy improved from 0.68640 to 0.68830, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.7279 - student_loss: 4.4180 - distillation_loss: 9.2217e-08 - val_categorical_accuracy: 0.6883 - val_student_loss: 4.2468 - lr: 0.0490\n",
            "Epoch 161/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.7349 - student_loss: 4.4171 - distillation_loss: 9.2264e-08\n",
            "Epoch 00161: val_categorical_accuracy did not improve from 0.68830\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.7349 - student_loss: 4.4171 - distillation_loss: 9.2261e-08 - val_categorical_accuracy: 0.6843 - val_student_loss: 4.2463 - lr: 0.0490\n",
            "Epoch 162/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.7360 - student_loss: 4.4162 - distillation_loss: 9.2511e-08\n",
            "Epoch 00162: val_categorical_accuracy did not improve from 0.68830\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.7360 - student_loss: 4.4162 - distillation_loss: 9.2502e-08 - val_categorical_accuracy: 0.6864 - val_student_loss: 4.2415 - lr: 0.0490\n",
            "Epoch 163/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.7347 - student_loss: 4.4156 - distillation_loss: 9.1978e-08\n",
            "Epoch 00163: val_categorical_accuracy improved from 0.68830 to 0.69360, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.7347 - student_loss: 4.4156 - distillation_loss: 9.1977e-08 - val_categorical_accuracy: 0.6936 - val_student_loss: 4.2466 - lr: 0.0490\n",
            "Epoch 164/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.7392 - student_loss: 4.4148 - distillation_loss: 9.2038e-08\n",
            "Epoch 00164: val_categorical_accuracy did not improve from 0.69360\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.7392 - student_loss: 4.4148 - distillation_loss: 9.2036e-08 - val_categorical_accuracy: 0.6876 - val_student_loss: 4.2489 - lr: 0.0490\n",
            "Epoch 165/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.7388 - student_loss: 4.4139 - distillation_loss: 9.2169e-08\n",
            "Epoch 00165: val_categorical_accuracy did not improve from 0.69360\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.7388 - student_loss: 4.4139 - distillation_loss: 9.2161e-08 - val_categorical_accuracy: 0.6850 - val_student_loss: 4.2137 - lr: 0.0490\n",
            "Epoch 166/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.7408 - student_loss: 4.4129 - distillation_loss: 9.2094e-08\n",
            "Epoch 00166: val_categorical_accuracy did not improve from 0.69360\n",
            "1562/1562 [==============================] - 219s 140ms/step - categorical_accuracy: 0.7408 - student_loss: 4.4129 - distillation_loss: 9.2109e-08 - val_categorical_accuracy: 0.6885 - val_student_loss: 4.2484 - lr: 0.0490\n",
            "Epoch 167/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.7408 - student_loss: 4.4123 - distillation_loss: 9.1735e-08\n",
            "Epoch 00167: val_categorical_accuracy did not improve from 0.69360\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.7408 - student_loss: 4.4123 - distillation_loss: 9.1739e-08 - val_categorical_accuracy: 0.6841 - val_student_loss: 4.2212 - lr: 0.0490\n",
            "Epoch 168/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.7445 - student_loss: 4.4114 - distillation_loss: 9.1976e-08\n",
            "Epoch 00168: val_categorical_accuracy did not improve from 0.69360\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.7445 - student_loss: 4.4114 - distillation_loss: 9.1981e-08 - val_categorical_accuracy: 0.6831 - val_student_loss: 4.2502 - lr: 0.0490\n",
            "Epoch 169/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.7423 - student_loss: 4.4108 - distillation_loss: 9.1688e-08\n",
            "Epoch 00169: val_categorical_accuracy did not improve from 0.69360\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.7423 - student_loss: 4.4108 - distillation_loss: 9.1690e-08 - val_categorical_accuracy: 0.6928 - val_student_loss: 4.2409 - lr: 0.0490\n",
            "Epoch 170/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.7451 - student_loss: 4.4102 - distillation_loss: 9.1899e-08\n",
            "Epoch 00170: val_categorical_accuracy improved from 0.69360 to 0.69490, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "\n",
            "Epoch 00170: ReduceLROnPlateau reducing learning rate to 0.03429999910295009.\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.7451 - student_loss: 4.4102 - distillation_loss: 9.1899e-08 - val_categorical_accuracy: 0.6949 - val_student_loss: 4.2296 - lr: 0.0490\n",
            "Epoch 171/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.7519 - student_loss: 4.4088 - distillation_loss: 9.1926e-08\n",
            "Epoch 00171: val_categorical_accuracy did not improve from 0.69490\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.7519 - student_loss: 4.4088 - distillation_loss: 9.1922e-08 - val_categorical_accuracy: 0.6942 - val_student_loss: 4.2419 - lr: 0.0343\n",
            "Epoch 172/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.7506 - student_loss: 4.4085 - distillation_loss: 9.1948e-08\n",
            "Epoch 00172: val_categorical_accuracy did not improve from 0.69490\n",
            "1562/1562 [==============================] - 221s 141ms/step - categorical_accuracy: 0.7506 - student_loss: 4.4085 - distillation_loss: 9.1947e-08 - val_categorical_accuracy: 0.6934 - val_student_loss: 4.2399 - lr: 0.0343\n",
            "Epoch 173/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.7538 - student_loss: 4.4073 - distillation_loss: 9.1611e-08\n",
            "Epoch 00173: val_categorical_accuracy improved from 0.69490 to 0.69850, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "1562/1562 [==============================] - 221s 141ms/step - categorical_accuracy: 0.7538 - student_loss: 4.4073 - distillation_loss: 9.1596e-08 - val_categorical_accuracy: 0.6985 - val_student_loss: 4.2361 - lr: 0.0343\n",
            "Epoch 174/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.7525 - student_loss: 4.4074 - distillation_loss: 9.1332e-08\n",
            "Epoch 00174: val_categorical_accuracy did not improve from 0.69850\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.7525 - student_loss: 4.4074 - distillation_loss: 9.1331e-08 - val_categorical_accuracy: 0.6972 - val_student_loss: 4.2247 - lr: 0.0343\n",
            "Epoch 175/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.7540 - student_loss: 4.4069 - distillation_loss: 9.2009e-08\n",
            "Epoch 00175: val_categorical_accuracy did not improve from 0.69850\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.7540 - student_loss: 4.4069 - distillation_loss: 9.2010e-08 - val_categorical_accuracy: 0.6939 - val_student_loss: 4.2062 - lr: 0.0343\n",
            "Epoch 176/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.7586 - student_loss: 4.4058 - distillation_loss: 9.1062e-08\n",
            "Epoch 00176: val_categorical_accuracy did not improve from 0.69850\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.7586 - student_loss: 4.4058 - distillation_loss: 9.1079e-08 - val_categorical_accuracy: 0.6935 - val_student_loss: 4.2503 - lr: 0.0343\n",
            "Epoch 177/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.7570 - student_loss: 4.4055 - distillation_loss: 9.1885e-08\n",
            "Epoch 00177: val_categorical_accuracy did not improve from 0.69850\n",
            "1562/1562 [==============================] - 219s 140ms/step - categorical_accuracy: 0.7570 - student_loss: 4.4055 - distillation_loss: 9.1887e-08 - val_categorical_accuracy: 0.6964 - val_student_loss: 4.2296 - lr: 0.0343\n",
            "Epoch 178/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.7580 - student_loss: 4.4049 - distillation_loss: 9.1588e-08\n",
            "Epoch 00178: val_categorical_accuracy did not improve from 0.69850\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.7580 - student_loss: 4.4049 - distillation_loss: 9.1576e-08 - val_categorical_accuracy: 0.6956 - val_student_loss: 4.2427 - lr: 0.0343\n",
            "Epoch 179/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.7597 - student_loss: 4.4043 - distillation_loss: 9.0915e-08\n",
            "Epoch 00179: val_categorical_accuracy did not improve from 0.69850\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.7597 - student_loss: 4.4043 - distillation_loss: 9.0908e-08 - val_categorical_accuracy: 0.6954 - val_student_loss: 4.2221 - lr: 0.0343\n",
            "Epoch 180/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.7603 - student_loss: 4.4035 - distillation_loss: 9.1420e-08\n",
            "Epoch 00180: val_categorical_accuracy did not improve from 0.69850\n",
            "\n",
            "Epoch 00180: ReduceLROnPlateau reducing learning rate to 0.024009999632835385.\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.7603 - student_loss: 4.4035 - distillation_loss: 9.1416e-08 - val_categorical_accuracy: 0.6962 - val_student_loss: 4.2257 - lr: 0.0343\n",
            "Epoch 181/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.7670 - student_loss: 4.4027 - distillation_loss: 9.1164e-08\n",
            "Epoch 00181: val_categorical_accuracy improved from 0.69850 to 0.70010, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.7670 - student_loss: 4.4027 - distillation_loss: 9.1153e-08 - val_categorical_accuracy: 0.7001 - val_student_loss: 4.2351 - lr: 0.0240\n",
            "Epoch 182/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.7625 - student_loss: 4.4028 - distillation_loss: 9.1609e-08\n",
            "Epoch 00182: val_categorical_accuracy did not improve from 0.70010\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.7625 - student_loss: 4.4028 - distillation_loss: 9.1617e-08 - val_categorical_accuracy: 0.6947 - val_student_loss: 4.2304 - lr: 0.0240\n",
            "Epoch 183/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.7664 - student_loss: 4.4021 - distillation_loss: 9.0959e-08\n",
            "Epoch 00183: val_categorical_accuracy did not improve from 0.70010\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.7664 - student_loss: 4.4021 - distillation_loss: 9.0956e-08 - val_categorical_accuracy: 0.6997 - val_student_loss: 4.2322 - lr: 0.0240\n",
            "Epoch 184/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.7614 - student_loss: 4.4020 - distillation_loss: 9.1159e-08\n",
            "Epoch 00184: val_categorical_accuracy improved from 0.70010 to 0.70100, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.7614 - student_loss: 4.4020 - distillation_loss: 9.1154e-08 - val_categorical_accuracy: 0.7010 - val_student_loss: 4.2207 - lr: 0.0240\n",
            "Epoch 185/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.7661 - student_loss: 4.4018 - distillation_loss: 9.1384e-08\n",
            "Epoch 00185: val_categorical_accuracy improved from 0.70100 to 0.70120, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "\n",
            "Epoch 00185: ReduceLROnPlateau reducing learning rate to 0.01680699922144413.\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.7661 - student_loss: 4.4018 - distillation_loss: 9.1380e-08 - val_categorical_accuracy: 0.7012 - val_student_loss: 4.2167 - lr: 0.0240\n",
            "Epoch 186/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.7689 - student_loss: 4.4010 - distillation_loss: 9.1046e-08\n",
            "Epoch 00186: val_categorical_accuracy did not improve from 0.70120\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.7689 - student_loss: 4.4010 - distillation_loss: 9.1064e-08 - val_categorical_accuracy: 0.7008 - val_student_loss: 4.2124 - lr: 0.0168\n",
            "Epoch 187/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.7708 - student_loss: 4.4004 - distillation_loss: 9.0559e-08\n",
            "Epoch 00187: val_categorical_accuracy improved from 0.70120 to 0.70380, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.7708 - student_loss: 4.4004 - distillation_loss: 9.0560e-08 - val_categorical_accuracy: 0.7038 - val_student_loss: 4.2093 - lr: 0.0168\n",
            "Epoch 188/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.7699 - student_loss: 4.4004 - distillation_loss: 9.0868e-08\n",
            "Epoch 00188: val_categorical_accuracy did not improve from 0.70380\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.7699 - student_loss: 4.4004 - distillation_loss: 9.0876e-08 - val_categorical_accuracy: 0.6989 - val_student_loss: 4.2069 - lr: 0.0168\n",
            "Epoch 189/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.7699 - student_loss: 4.3999 - distillation_loss: 9.1349e-08\n",
            "Epoch 00189: val_categorical_accuracy did not improve from 0.70380\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.7699 - student_loss: 4.3999 - distillation_loss: 9.1361e-08 - val_categorical_accuracy: 0.7015 - val_student_loss: 4.2123 - lr: 0.0168\n",
            "Epoch 190/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.7702 - student_loss: 4.4001 - distillation_loss: 9.0860e-08\n",
            "Epoch 00190: val_categorical_accuracy did not improve from 0.70380\n",
            "\n",
            "Epoch 00190: ReduceLROnPlateau reducing learning rate to 0.01176489945501089.\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.7702 - student_loss: 4.4001 - distillation_loss: 9.0861e-08 - val_categorical_accuracy: 0.7024 - val_student_loss: 4.2112 - lr: 0.0168\n",
            "Epoch 191/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.7726 - student_loss: 4.3994 - distillation_loss: 9.0923e-08\n",
            "Epoch 00191: val_categorical_accuracy did not improve from 0.70380\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.7726 - student_loss: 4.3994 - distillation_loss: 9.0914e-08 - val_categorical_accuracy: 0.7038 - val_student_loss: 4.2155 - lr: 0.0118\n",
            "Epoch 192/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.7735 - student_loss: 4.3992 - distillation_loss: 9.1086e-08\n",
            "Epoch 00192: val_categorical_accuracy did not improve from 0.70380\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.7735 - student_loss: 4.3992 - distillation_loss: 9.1094e-08 - val_categorical_accuracy: 0.7021 - val_student_loss: 4.2096 - lr: 0.0118\n",
            "Epoch 193/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.7726 - student_loss: 4.3989 - distillation_loss: 9.1114e-08\n",
            "Epoch 00193: val_categorical_accuracy improved from 0.70380 to 0.70550, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.7726 - student_loss: 4.3989 - distillation_loss: 9.1120e-08 - val_categorical_accuracy: 0.7055 - val_student_loss: 4.2007 - lr: 0.0118\n",
            "Epoch 194/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.7720 - student_loss: 4.3992 - distillation_loss: 9.0917e-08\n",
            "Epoch 00194: val_categorical_accuracy did not improve from 0.70550\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.7720 - student_loss: 4.3991 - distillation_loss: 9.0923e-08 - val_categorical_accuracy: 0.7029 - val_student_loss: 4.2175 - lr: 0.0118\n",
            "Epoch 195/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.7748 - student_loss: 4.3984 - distillation_loss: 9.0918e-08\n",
            "Epoch 00195: val_categorical_accuracy did not improve from 0.70550\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.7748 - student_loss: 4.3983 - distillation_loss: 9.0910e-08 - val_categorical_accuracy: 0.7048 - val_student_loss: 4.2201 - lr: 0.0118\n",
            "Epoch 196/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.7733 - student_loss: 4.3988 - distillation_loss: 9.0918e-08\n",
            "Epoch 00196: val_categorical_accuracy did not improve from 0.70550\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.7733 - student_loss: 4.3988 - distillation_loss: 9.0921e-08 - val_categorical_accuracy: 0.7043 - val_student_loss: 4.2252 - lr: 0.0118\n",
            "Epoch 197/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.7749 - student_loss: 4.3979 - distillation_loss: 9.0900e-08\n",
            "Epoch 00197: val_categorical_accuracy improved from 0.70550 to 0.70650, saving model to Desktop/Trained_models/student_keras_vgg16_cifar10.h5\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.7749 - student_loss: 4.3979 - distillation_loss: 9.0896e-08 - val_categorical_accuracy: 0.7065 - val_student_loss: 4.2149 - lr: 0.0118\n",
            "Epoch 198/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.7763 - student_loss: 4.3980 - distillation_loss: 9.0801e-08\n",
            "Epoch 00198: val_categorical_accuracy did not improve from 0.70650\n",
            "\n",
            "Epoch 00198: ReduceLROnPlateau reducing learning rate to 0.008235429879277945.\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.7763 - student_loss: 4.3980 - distillation_loss: 9.0806e-08 - val_categorical_accuracy: 0.7019 - val_student_loss: 4.2119 - lr: 0.0118\n",
            "Epoch 199/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.7739 - student_loss: 4.3982 - distillation_loss: 9.1048e-08\n",
            "Epoch 00199: val_categorical_accuracy did not improve from 0.70650\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.7739 - student_loss: 4.3982 - distillation_loss: 9.1054e-08 - val_categorical_accuracy: 0.7021 - val_student_loss: 4.2128 - lr: 0.0082\n",
            "Epoch 200/200\n",
            "1562/1562 [==============================] - ETA: 0s - categorical_accuracy: 0.7781 - student_loss: 4.3977 - distillation_loss: 9.1101e-08\n",
            "Epoch 00200: val_categorical_accuracy did not improve from 0.70650\n",
            "1562/1562 [==============================] - 220s 141ms/step - categorical_accuracy: 0.7781 - student_loss: 4.3977 - distillation_loss: 9.1109e-08 - val_categorical_accuracy: 0.7034 - val_student_loss: 4.2045 - lr: 0.0082\n",
            "313/313 [==============================] - 10s 31ms/step - categorical_accuracy: 0.7034 - student_loss: 4.2663\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7034000158309937"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize = (9,6))\n",
        "plt.plot(history_distiller.history['val_student_loss'],label='Test loss')\n",
        "plt.plot(history_distiller.history['student_loss'],label='Train loss')\n",
        "plt.title('Loss curve for resnet model')\n",
        "plt.ylabel('Loss value')\n",
        "plt.xlabel('No. epoch')\n",
        "plt.show()\n",
        "\n",
        "# Plot history: Accuracy\n",
        "plt.figure(figsize = (9,6))\n",
        "plt.plot(history_distiller.history['val_categorical_accuracy'],label = 'Test accuracy')\n",
        "plt.plot(history_distiller.history['categorical_accuracy'],label = 'Train accuracy')\n",
        "plt.title('Accuracy curve for resnet model')\n",
        "plt.ylabel('Accuracy value (%)')\n",
        "plt.xlabel('No. epoch')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "l6ftcZ7WoKSS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 791
        },
        "outputId": "83dd1570-ebc6-44e0-d572-b2da06348b34"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 648x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 648x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_prediction(model , image , true_value):\n",
        "  true_label_index = -1\n",
        "  for i in range(len(true_value)):\n",
        "\n",
        "    if(true_value[i]==1):\n",
        "      true_label_index = i\n",
        "      break\n",
        "\n",
        "  prediction = model.predict(image)[0]\n",
        "  probability = float('-inf')\n",
        "  predicted_label_index = -1\n",
        "\n",
        "  for i in range(len(prediction)):\n",
        "\n",
        "    if(prediction[i]>probability):\n",
        "      probability = prediction[i]\n",
        "      predicted_label_index = i\n",
        "\n",
        "  if(true_label_index!=predicted_label_index):\n",
        "    return 1\n",
        "  return 0"
      ],
      "metadata": {
        "id": "HPpOPmwZoKMp"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras,os\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential,Model\n",
        "from keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D , Dropout\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from keras.initializers import glorot_uniform\n",
        "from keras import applications\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50 , preprocess_input\n",
        "from keras import regularizers\n",
        "from absl import app, flags\n",
        "\n",
        "# Install bleeding edge version of cleverhans\n",
        "!pip install git+https://github.com/tensorflow/cleverhans.git#egg=cleverhans\n",
        "from cleverhans.tf2.attacks.projected_gradient_descent import projected_gradient_descent\n",
        "from cleverhans.tf2.attacks.fast_gradient_method import fast_gradient_method"
      ],
      "metadata": {
        "id": "u3phCvWqUjWD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2532ed43-8dd2-4dfc-ddae-4ba4b3710376"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: cleverhans from git+https://github.com/tensorflow/cleverhans.git#egg=cleverhans in ./.local/lib/python3.6/site-packages (4.0.0)\n",
            "Requirement already satisfied: joblib in ./.local/lib/python3.6/site-packages (from cleverhans) (1.1.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from cleverhans) (3.3.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from cleverhans) (0.10.0)\n",
            "Requirement already satisfied: easydict in ./.local/lib/python3.6/site-packages (from cleverhans) (1.9)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from cleverhans) (1.4.1)\n",
            "Requirement already satisfied: nose in ./.local/lib/python3.6/site-packages (from cleverhans) (1.3.7)\n",
            "Requirement already satisfied: mnist in ./.local/lib/python3.6/site-packages (from cleverhans) (0.2.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cleverhans) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from cleverhans) (1.19.1)\n",
            "Requirement already satisfied: tensorflow-probability in ./.local/lib/python3.6/site-packages (from cleverhans) (0.16.0)\n",
            "Requirement already satisfied: pycodestyle in ./.local/lib/python3.6/site-packages (from cleverhans) (2.8.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->cleverhans) (1.2.0)\n",
            "Requirement already satisfied: certifi>=2020.06.20 in /usr/local/lib/python3.6/dist-packages (from matplotlib->cleverhans) (2020.6.20)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->cleverhans) (2.8.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.6/dist-packages (from matplotlib->cleverhans) (7.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->cleverhans) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.6/dist-packages (from matplotlib->cleverhans) (2.4.7)\n",
            "Requirement already satisfied: decorator in ./.local/lib/python3.6/site-packages (from tensorflow-probability->cleverhans) (4.4.2)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability->cleverhans) (0.3.3)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in ./.local/lib/python3.6/site-packages (from tensorflow-probability->cleverhans) (2.0.0)\n",
            "Requirement already satisfied: dm-tree in ./.local/lib/python3.6/site-packages (from tensorflow-probability->cleverhans) (0.1.6)\n",
            "\u001b[33mWARNING: You are using pip version 20.2.2; however, version 21.3.1 is available.\n",
            "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import img_to_array\n",
        "\n",
        "epsilon_end = 0.8\n",
        "#logits_model = tf.keras.Model(model.input,model.layers[-1].output)\n",
        "total_images = 200\n",
        "\n",
        "result={}\n",
        "epsilon = 0.1\n",
        "while(epsilon <= epsilon_end):\n",
        "\n",
        "  fgsm_counter = 0\n",
        "  pgd_counter = 0\n",
        "  print(\"Epsilon value - \",epsilon)\n",
        "  print()\n",
        "  for image_index in range(total_images):\n",
        "  \n",
        "  \n",
        "    image = x_test[image_index]\n",
        "    image = img_to_array(image)\n",
        "    image = image.reshape(1, 32, 32, 3)\n",
        "    true_value = y_test[image_index]\n",
        "    original_prediction = make_prediction(distiller,image,true_value)\n",
        "    fgsm_sample = fast_gradient_method(distiller, image, epsilon, np.inf, targeted=False)\n",
        "    pgd_sample = projected_gradient_descent(distiller, image, epsilon, 0.01, 40, np.inf)\n",
        "    print(epsilon,' ',image_index)\n",
        "    fgsm_prediction = make_prediction(distiller , fgsm_sample , true_value)\n",
        "    pgd_prediction = make_prediction(distiller , pgd_sample , true_value)\n",
        "    pgd_counter+=pgd_prediction\n",
        "    fgsm_counter+=fgsm_prediction\n",
        "    print(pgd_counter,' ',fgsm_counter)\n",
        "  result[epsilon] = (fgsm_counter , pgd_counter)\n",
        "  epsilon+=0.1\n"
      ],
      "metadata": {
        "id": "LjVpMPnXURRO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2ad85d5-9198-4e50-e996-e1f1b950502a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epsilon value -  0.1\n",
            "\n",
            "0.1   0\n",
            "0   0\n",
            "0.1   1\n",
            "0   0\n",
            "0.1   2\n",
            "1   1\n",
            "0.1   3\n",
            "1   1\n",
            "0.1   4\n",
            "1   1\n",
            "0.1   5\n",
            "2   2\n",
            "0.1   6\n",
            "2   2\n",
            "0.1   7\n",
            "3   3\n",
            "0.1   8\n",
            "3   3\n",
            "0.1   9\n",
            "3   3\n",
            "0.1   10\n",
            "4   4\n",
            "0.1   11\n",
            "4   4\n",
            "0.1   12\n",
            "4   4\n",
            "0.1   13\n",
            "4   4\n",
            "0.1   14\n",
            "4   4\n",
            "0.1   15\n",
            "4   4\n",
            "0.1   16\n",
            "4   4\n",
            "0.1   17\n",
            "4   4\n",
            "0.1   18\n",
            "4   4\n",
            "0.1   19\n",
            "4   4\n",
            "0.1   20\n",
            "4   4\n",
            "0.1   21\n",
            "4   4\n",
            "0.1   22\n",
            "4   4\n",
            "0.1   23\n",
            "5   5\n",
            "0.1   24\n",
            "5   5\n",
            "0.1   25\n",
            "5   5\n",
            "0.1   26\n",
            "5   5\n",
            "0.1   27\n",
            "5   5\n",
            "0.1   28\n",
            "6   6\n",
            "0.1   29\n",
            "7   7\n",
            "0.1   30\n",
            "7   7\n",
            "0.1   31\n",
            "7   7\n",
            "0.1   32\n",
            "7   7\n",
            "0.1   33\n",
            "8   8\n",
            "0.1   34\n",
            "8   8\n",
            "0.1   35\n",
            "9   9\n",
            "0.1   36\n",
            "9   9\n",
            "0.1   37\n",
            "9   9\n",
            "0.1   38\n",
            "9   9\n",
            "0.1   39\n",
            "9   9\n",
            "0.1   40\n",
            "9   9\n",
            "0.1   41\n",
            "9   9\n",
            "0.1   42\n",
            "9   9\n",
            "0.1   43\n",
            "10   10\n",
            "0.1   44\n",
            "10   10\n",
            "0.1   45\n",
            "10   10\n",
            "0.1   46\n",
            "10   10\n",
            "0.1   47\n",
            "10   10\n",
            "0.1   48\n",
            "11   11\n",
            "0.1   49\n",
            "12   12\n",
            "0.1   50\n",
            "12   12\n",
            "0.1   51\n",
            "12   12\n",
            "0.1   52\n",
            "12   12\n",
            "0.1   53\n",
            "12   12\n",
            "0.1   54\n",
            "12   12\n",
            "0.1   55\n",
            "12   12\n",
            "0.1   56\n",
            "12   12\n",
            "0.1   57\n",
            "12   12\n",
            "0.1   58\n",
            "12   12\n",
            "0.1   59\n",
            "12   12\n",
            "0.1   60\n",
            "12   12\n",
            "0.1   61\n",
            "12   12\n",
            "0.1   62\n",
            "13   13\n",
            "0.1   63\n",
            "14   14\n",
            "0.1   64\n",
            "15   15\n",
            "0.1   65\n",
            "16   16\n",
            "0.1   66\n",
            "16   16\n",
            "0.1   67\n",
            "16   16\n",
            "0.1   68\n",
            "16   16\n",
            "0.1   69\n",
            "16   16\n",
            "0.1   70\n",
            "16   16\n",
            "0.1   71\n",
            "16   16\n",
            "0.1   72\n",
            "17   17\n",
            "0.1   73\n",
            "17   17\n",
            "0.1   74\n",
            "18   18\n",
            "0.1   75\n",
            "18   18\n",
            "0.1   76\n",
            "18   18\n",
            "0.1   77\n",
            "18   18\n",
            "0.1   78\n",
            "18   18\n",
            "0.1   79\n",
            "19   19\n",
            "0.1   80\n",
            "19   19\n",
            "0.1   81\n",
            "19   19\n",
            "0.1   82\n",
            "19   19\n",
            "0.1   83\n",
            "19   19\n",
            "0.1   84\n",
            "19   19\n",
            "0.1   85\n",
            "19   19\n",
            "0.1   86\n",
            "19   19\n",
            "0.1   87\n",
            "20   20\n",
            "0.1   88\n",
            "21   21\n",
            "0.1   89\n",
            "22   22\n",
            "0.1   90\n",
            "22   22\n",
            "0.1   91\n",
            "22   22\n",
            "0.1   92\n",
            "22   22\n",
            "0.1   93\n",
            "22   22\n",
            "0.1   94\n",
            "22   22\n",
            "0.1   95\n",
            "22   22\n",
            "0.1   96\n",
            "22   22\n",
            "0.1   97\n",
            "22   22\n",
            "0.1   98\n",
            "22   22\n",
            "0.1   99\n",
            "23   23\n",
            "0.1   100\n",
            "24   24\n",
            "0.1   101\n",
            "25   25\n",
            "0.1   102\n",
            "25   25\n",
            "0.1   103\n",
            "26   26\n",
            "0.1   104\n",
            "26   26\n",
            "0.1   105\n",
            "26   26\n",
            "0.1   106\n",
            "26   26\n",
            "0.1   107\n",
            "26   26\n",
            "0.1   108\n",
            "26   26\n",
            "0.1   109\n",
            "26   26\n",
            "0.1   110\n",
            "26   26\n",
            "0.1   111\n",
            "26   26\n",
            "0.1   112\n",
            "26   26\n",
            "0.1   113\n",
            "26   26\n",
            "0.1   114\n",
            "26   26\n",
            "0.1   115\n",
            "26   26\n",
            "0.1   116\n",
            "27   27\n",
            "0.1   117\n",
            "27   27\n",
            "0.1   118\n",
            "28   28\n",
            "0.1   119\n",
            "28   28\n",
            "0.1   120\n",
            "28   28\n",
            "0.1   121\n",
            "28   28\n",
            "0.1   122\n",
            "28   28\n",
            "0.1   123\n",
            "28   28\n",
            "0.1   124\n",
            "28   28\n",
            "0.1   125\n",
            "28   28\n",
            "0.1   126\n",
            "28   28\n",
            "0.1   127\n",
            "29   29\n",
            "0.1   128\n",
            "29   29\n",
            "0.1   129\n",
            "30   30\n",
            "0.1   130\n",
            "31   31\n",
            "0.1   131\n",
            "31   31\n",
            "0.1   132\n",
            "31   31\n",
            "0.1   133\n",
            "31   31\n",
            "0.1   134\n",
            "32   32\n",
            "0.1   135\n",
            "32   32\n",
            "0.1   136\n",
            "32   32\n",
            "0.1   137\n",
            "33   33\n",
            "0.1   138\n",
            "33   33\n",
            "0.1   139\n",
            "33   33\n",
            "0.1   140\n",
            "33   33\n",
            "0.1   141\n",
            "33   33\n",
            "0.1   142\n",
            "33   33\n",
            "0.1   143\n",
            "34   34\n",
            "0.1   144\n",
            "34   34\n",
            "0.1   145\n",
            "34   34\n",
            "0.1   146\n",
            "35   35\n",
            "0.1   147\n",
            "35   35\n",
            "0.1   148\n",
            "35   35\n",
            "0.1   149\n",
            "35   35\n",
            "0.1   150\n",
            "35   35\n",
            "0.1   151\n",
            "36   36\n",
            "0.1   152\n",
            "37   37\n",
            "0.1   153\n",
            "38   38\n",
            "0.1   154\n",
            "38   38\n",
            "0.1   155\n",
            "38   38\n",
            "0.1   156\n",
            "38   38\n",
            "0.1   157\n",
            "38   38\n",
            "0.1   158\n",
            "38   38\n",
            "0.1   159\n",
            "39   39\n",
            "0.1   160\n",
            "39   39\n",
            "0.1   161\n",
            "39   39\n",
            "0.1   162\n",
            "40   40\n",
            "0.1   163\n",
            "40   40\n",
            "0.1   164\n",
            "41   41\n",
            "0.1   165\n",
            "41   41\n",
            "0.1   166\n",
            "41   41\n",
            "0.1   167\n",
            "41   41\n",
            "0.1   168\n",
            "41   41\n",
            "0.1   169\n",
            "41   41\n",
            "0.1   170\n",
            "41   41\n",
            "0.1   171\n",
            "42   42\n",
            "0.1   172\n",
            "42   42\n",
            "0.1   173\n",
            "42   42\n",
            "0.1   174\n",
            "42   42\n",
            "0.1   175\n",
            "42   42\n",
            "0.1   176\n",
            "43   43\n",
            "0.1   177\n",
            "43   43\n",
            "0.1   178\n",
            "43   43\n",
            "0.1   179\n",
            "43   43\n",
            "0.1   180\n",
            "43   43\n",
            "0.1   181\n",
            "44   44\n",
            "0.1   182\n",
            "44   44\n",
            "0.1   183\n",
            "45   45\n",
            "0.1   184\n",
            "46   46\n",
            "0.1   185\n",
            "46   46\n",
            "0.1   186\n",
            "47   47\n",
            "0.1   187\n",
            "47   47\n",
            "0.1   188\n",
            "47   47\n",
            "0.1   189\n",
            "47   47\n",
            "0.1   190\n",
            "47   47\n",
            "0.1   191\n",
            "47   47\n",
            "0.1   192\n",
            "48   48\n",
            "0.1   193\n",
            "48   48\n",
            "0.1   194\n",
            "48   48\n",
            "0.1   195\n",
            "48   48\n",
            "0.1   196\n",
            "48   48\n",
            "0.1   197\n",
            "48   48\n",
            "0.1   198\n",
            "48   48\n",
            "0.1   199\n",
            "48   48\n",
            "Epsilon value -  0.2\n",
            "\n",
            "0.2   0\n",
            "1   1\n",
            "0.2   1\n",
            "1   1\n",
            "0.2   2\n",
            "2   2\n",
            "0.2   3\n",
            "2   2\n",
            "0.2   4\n",
            "2   2\n",
            "0.2   5\n",
            "3   3\n",
            "0.2   6\n",
            "3   3\n",
            "0.2   7\n",
            "4   4\n",
            "0.2   8\n",
            "5   5\n",
            "0.2   9\n",
            "5   5\n",
            "0.2   10\n",
            "6   6\n",
            "0.2   11\n",
            "6   6\n",
            "0.2   12\n",
            "6   6\n",
            "0.2   13\n",
            "6   6\n",
            "0.2   14\n",
            "6   6\n",
            "0.2   15\n",
            "6   6\n",
            "0.2   16\n",
            "6   6\n",
            "0.2   17\n",
            "7   7\n",
            "0.2   18\n",
            "7   7\n",
            "0.2   19\n",
            "7   7\n",
            "0.2   20\n",
            "7   7\n",
            "0.2   21\n",
            "7   7\n",
            "0.2   22\n",
            "7   7\n",
            "0.2   23\n",
            "8   8\n",
            "0.2   24\n",
            "8   8\n",
            "0.2   25\n",
            "8   8\n",
            "0.2   26\n",
            "8   8\n",
            "0.2   27\n",
            "8   8\n",
            "0.2   28\n",
            "9   9\n",
            "0.2   29\n",
            "10   10\n",
            "0.2   30\n",
            "10   10\n",
            "0.2   31\n",
            "10   10\n",
            "0.2   32\n",
            "10   10\n",
            "0.2   33\n",
            "11   11\n",
            "0.2   34\n",
            "11   11\n",
            "0.2   35\n",
            "12   12\n",
            "0.2   36\n",
            "12   12\n",
            "0.2   37\n",
            "12   12\n",
            "0.2   38\n",
            "12   12\n",
            "0.2   39\n",
            "12   12\n",
            "0.2   40\n",
            "12   12\n",
            "0.2   41\n",
            "12   12\n",
            "0.2   42\n",
            "12   12\n",
            "0.2   43\n",
            "13   13\n",
            "0.2   44\n",
            "13   13\n",
            "0.2   45\n",
            "13   13\n",
            "0.2   46\n",
            "13   13\n",
            "0.2   47\n",
            "13   13\n",
            "0.2   48\n",
            "14   14\n",
            "0.2   49\n",
            "15   15\n",
            "0.2   50\n",
            "15   15\n",
            "0.2   51\n",
            "15   15\n",
            "0.2   52\n",
            "15   15\n",
            "0.2   53\n",
            "15   15\n",
            "0.2   54\n",
            "15   15\n",
            "0.2   55\n",
            "15   15\n",
            "0.2   56\n",
            "15   15\n",
            "0.2   57\n",
            "15   15\n",
            "0.2   58\n",
            "15   15\n",
            "0.2   59\n",
            "15   15\n",
            "0.2   60\n",
            "15   15\n",
            "0.2   61\n",
            "15   15\n",
            "0.2   62\n",
            "16   16\n",
            "0.2   63\n",
            "17   17\n",
            "0.2   64\n",
            "18   18\n",
            "0.2   65\n",
            "19   19\n",
            "0.2   66\n",
            "19   19\n",
            "0.2   67\n",
            "19   19\n",
            "0.2   68\n",
            "19   19\n",
            "0.2   69\n",
            "19   19\n",
            "0.2   70\n",
            "19   19\n",
            "0.2   71\n",
            "19   19\n",
            "0.2   72\n",
            "20   20\n",
            "0.2   73\n",
            "20   20\n",
            "0.2   74\n",
            "21   21\n",
            "0.2   75\n",
            "21   21\n",
            "0.2   76\n",
            "21   21\n",
            "0.2   77\n",
            "21   21\n",
            "0.2   78\n",
            "21   21\n",
            "0.2   79\n",
            "22   22\n",
            "0.2   80\n",
            "22   22\n",
            "0.2   81\n",
            "22   22\n",
            "0.2   82\n",
            "22   22\n",
            "0.2   83\n",
            "23   23\n",
            "0.2   84\n",
            "23   23\n",
            "0.2   85\n",
            "23   23\n",
            "0.2   86\n",
            "23   23\n",
            "0.2   87\n",
            "24   24\n",
            "0.2   88\n",
            "24   24\n",
            "0.2   89\n",
            "25   25\n",
            "0.2   90\n",
            "25   25\n",
            "0.2   91\n",
            "25   25\n",
            "0.2   92\n",
            "25   25\n",
            "0.2   93\n",
            "25   25\n",
            "0.2   94\n",
            "25   25\n",
            "0.2   95\n",
            "25   25\n",
            "0.2   96\n",
            "25   25\n",
            "0.2   97\n",
            "25   25\n",
            "0.2   98\n",
            "25   25\n",
            "0.2   99\n",
            "26   26\n",
            "0.2   100\n",
            "27   27\n",
            "0.2   101\n",
            "28   28\n",
            "0.2   102\n",
            "28   28\n",
            "0.2   103\n",
            "29   29\n",
            "0.2   104\n",
            "29   29\n",
            "0.2   105\n",
            "29   29\n",
            "0.2   106\n",
            "29   29\n",
            "0.2   107\n",
            "29   29\n",
            "0.2   108\n",
            "29   29\n",
            "0.2   109\n",
            "29   29\n",
            "0.2   110\n",
            "29   29\n",
            "0.2   111\n",
            "29   29\n",
            "0.2   112\n",
            "29   29\n",
            "0.2   113\n",
            "29   29\n",
            "0.2   114\n",
            "29   29\n",
            "0.2   115\n",
            "29   29\n",
            "0.2   116\n",
            "30   30\n",
            "0.2   117\n",
            "30   30\n",
            "0.2   118\n",
            "31   31\n",
            "0.2   119\n",
            "31   31\n",
            "0.2   120\n",
            "31   31\n",
            "0.2   121\n",
            "31   31\n",
            "0.2   122\n",
            "31   31\n",
            "0.2   123\n",
            "31   31\n",
            "0.2   124\n",
            "31   31\n",
            "0.2   125\n",
            "31   31\n",
            "0.2   126\n",
            "31   31\n",
            "0.2   127\n",
            "32   32\n",
            "0.2   128\n",
            "32   32\n",
            "0.2   129\n",
            "33   33\n",
            "0.2   130\n",
            "34   34\n",
            "0.2   131\n",
            "34   34\n",
            "0.2   132\n",
            "34   34\n",
            "0.2   133\n",
            "34   34\n",
            "0.2   134\n",
            "35   35\n",
            "0.2   135\n",
            "35   35\n",
            "0.2   136\n",
            "35   35\n",
            "0.2   137\n",
            "36   36\n",
            "0.2   138\n",
            "36   36\n",
            "0.2   139\n",
            "36   36\n",
            "0.2   140\n",
            "36   36\n",
            "0.2   141\n",
            "37   36\n",
            "0.2   142\n",
            "37   36\n",
            "0.2   143\n",
            "38   37\n",
            "0.2   144\n",
            "38   37\n",
            "0.2   145\n",
            "38   37\n",
            "0.2   146\n",
            "39   38\n",
            "0.2   147\n",
            "39   38\n",
            "0.2   148\n",
            "39   38\n",
            "0.2   149\n",
            "39   38\n",
            "0.2   150\n",
            "39   38\n",
            "0.2   151\n",
            "40   39\n",
            "0.2   152\n",
            "41   40\n",
            "0.2   153\n",
            "42   41\n",
            "0.2   154\n",
            "42   41\n",
            "0.2   155\n",
            "42   41\n",
            "0.2   156\n",
            "42   41\n",
            "0.2   157\n",
            "42   41\n",
            "0.2   158\n",
            "42   41\n",
            "0.2   159\n",
            "43   42\n",
            "0.2   160\n",
            "43   42\n",
            "0.2   161\n",
            "43   42\n",
            "0.2   162\n",
            "44   43\n",
            "0.2   163\n",
            "44   43\n",
            "0.2   164\n",
            "44   43\n",
            "0.2   165\n",
            "44   43\n",
            "0.2   166\n",
            "44   43\n",
            "0.2   167\n",
            "44   43\n",
            "0.2   168\n",
            "44   43\n",
            "0.2   169\n",
            "44   43\n",
            "0.2   170\n",
            "44   43\n",
            "0.2   171\n",
            "45   44\n",
            "0.2   172\n",
            "46   45\n",
            "0.2   173\n",
            "46   45\n",
            "0.2   174\n",
            "46   45\n",
            "0.2   175\n",
            "46   45\n",
            "0.2   176\n",
            "47   46\n",
            "0.2   177\n",
            "47   46\n",
            "0.2   178\n",
            "47   46\n",
            "0.2   179\n",
            "47   46\n",
            "0.2   180\n",
            "47   46\n",
            "0.2   181\n",
            "48   47\n",
            "0.2   182\n",
            "48   47\n",
            "0.2   183\n",
            "49   48\n",
            "0.2   184\n",
            "50   49\n",
            "0.2   185\n",
            "50   49\n",
            "0.2   186\n",
            "51   50\n",
            "0.2   187\n",
            "51   50\n",
            "0.2   188\n",
            "51   50\n",
            "0.2   189\n",
            "51   50\n",
            "0.2   190\n",
            "51   50\n",
            "0.2   191\n",
            "52   51\n",
            "0.2   192\n",
            "53   52\n",
            "0.2   193\n",
            "53   52\n",
            "0.2   194\n",
            "53   52\n",
            "0.2   195\n",
            "53   52\n",
            "0.2   196\n",
            "53   52\n",
            "0.2   197\n",
            "54   53\n",
            "0.2   198\n",
            "54   53\n",
            "0.2   199\n",
            "54   53\n",
            "Epsilon value -  0.30000000000000004\n",
            "\n",
            "0.30000000000000004   0\n",
            "1   1\n",
            "0.30000000000000004   1\n",
            "1   1\n",
            "0.30000000000000004   2\n",
            "2   2\n",
            "0.30000000000000004   3\n",
            "2   2\n",
            "0.30000000000000004   4\n",
            "2   2\n",
            "0.30000000000000004   5\n",
            "3   3\n",
            "0.30000000000000004   6\n",
            "3   3\n",
            "0.30000000000000004   7\n",
            "4   4\n",
            "0.30000000000000004   8\n",
            "5   5\n",
            "0.30000000000000004   9\n",
            "5   5\n",
            "0.30000000000000004   10\n",
            "5   6\n",
            "0.30000000000000004   11\n",
            "5   6\n",
            "0.30000000000000004   12\n",
            "5   6\n",
            "0.30000000000000004   13\n",
            "5   6\n",
            "0.30000000000000004   14\n",
            "5   6\n",
            "0.30000000000000004   15\n",
            "5   6\n",
            "0.30000000000000004   16\n",
            "5   6\n",
            "0.30000000000000004   17\n",
            "6   7\n",
            "0.30000000000000004   18\n",
            "6   7\n",
            "0.30000000000000004   19\n",
            "6   7\n",
            "0.30000000000000004   20\n",
            "6   7\n",
            "0.30000000000000004   21\n",
            "6   7\n",
            "0.30000000000000004   22\n",
            "6   7\n",
            "0.30000000000000004   23\n",
            "7   8\n",
            "0.30000000000000004   24\n",
            "7   8\n",
            "0.30000000000000004   25\n",
            "7   8\n",
            "0.30000000000000004   26\n",
            "7   8\n",
            "0.30000000000000004   27\n",
            "7   8\n",
            "0.30000000000000004   28\n",
            "8   9\n",
            "0.30000000000000004   29\n",
            "9   10\n",
            "0.30000000000000004   30\n",
            "9   10\n",
            "0.30000000000000004   31\n",
            "9   10\n",
            "0.30000000000000004   32\n",
            "9   10\n",
            "0.30000000000000004   33\n",
            "10   11\n",
            "0.30000000000000004   34\n",
            "10   11\n",
            "0.30000000000000004   35\n",
            "11   12\n",
            "0.30000000000000004   36\n",
            "11   12\n",
            "0.30000000000000004   37\n",
            "12   13\n",
            "0.30000000000000004   38\n",
            "12   13\n",
            "0.30000000000000004   39\n",
            "12   13\n",
            "0.30000000000000004   40\n",
            "12   13\n",
            "0.30000000000000004   41\n",
            "12   13\n",
            "0.30000000000000004   42\n",
            "12   13\n",
            "0.30000000000000004   43\n",
            "13   14\n",
            "0.30000000000000004   44\n",
            "13   14\n",
            "0.30000000000000004   45\n",
            "13   14\n",
            "0.30000000000000004   46\n",
            "13   14\n",
            "0.30000000000000004   47\n",
            "13   14\n",
            "0.30000000000000004   48\n",
            "14   15\n",
            "0.30000000000000004   49\n",
            "15   16\n",
            "0.30000000000000004   50\n",
            "15   16\n",
            "0.30000000000000004   51\n",
            "15   16\n",
            "0.30000000000000004   52\n",
            "15   16\n",
            "0.30000000000000004   53\n",
            "15   16\n",
            "0.30000000000000004   54\n",
            "15   16\n",
            "0.30000000000000004   55\n",
            "16   17\n",
            "0.30000000000000004   56\n",
            "16   17\n",
            "0.30000000000000004   57\n",
            "16   17\n",
            "0.30000000000000004   58\n",
            "16   17\n",
            "0.30000000000000004   59\n",
            "17   18\n",
            "0.30000000000000004   60\n",
            "17   18\n",
            "0.30000000000000004   61\n",
            "18   19\n",
            "0.30000000000000004   62\n",
            "19   20\n",
            "0.30000000000000004   63\n",
            "20   21\n",
            "0.30000000000000004   64\n",
            "21   22\n",
            "0.30000000000000004   65\n",
            "22   23\n",
            "0.30000000000000004   66\n",
            "22   23\n",
            "0.30000000000000004   67\n",
            "22   23\n",
            "0.30000000000000004   68\n",
            "22   23\n",
            "0.30000000000000004   69\n",
            "22   23\n",
            "0.30000000000000004   70\n",
            "22   23\n",
            "0.30000000000000004   71\n",
            "22   23\n",
            "0.30000000000000004   72\n",
            "23   24\n",
            "0.30000000000000004   73\n",
            "23   24\n",
            "0.30000000000000004   74\n",
            "24   25\n",
            "0.30000000000000004   75\n",
            "24   25\n",
            "0.30000000000000004   76\n",
            "24   25\n",
            "0.30000000000000004   77\n",
            "24   25\n",
            "0.30000000000000004   78\n",
            "24   25\n",
            "0.30000000000000004   79\n",
            "25   26\n",
            "0.30000000000000004   80\n",
            "25   26\n",
            "0.30000000000000004   81\n",
            "25   26\n",
            "0.30000000000000004   82\n",
            "25   26\n",
            "0.30000000000000004   83\n",
            "26   27\n",
            "0.30000000000000004   84\n",
            "27   28\n",
            "0.30000000000000004   85\n",
            "27   28\n",
            "0.30000000000000004   86\n",
            "27   28\n",
            "0.30000000000000004   87\n",
            "28   29\n",
            "0.30000000000000004   88\n",
            "28   29\n",
            "0.30000000000000004   89\n",
            "29   30\n",
            "0.30000000000000004   90\n",
            "30   31\n",
            "0.30000000000000004   91\n",
            "30   31\n",
            "0.30000000000000004   92\n",
            "30   31\n",
            "0.30000000000000004   93\n",
            "30   31\n",
            "0.30000000000000004   94\n",
            "30   31\n",
            "0.30000000000000004   95\n",
            "30   31\n",
            "0.30000000000000004   96\n",
            "31   32\n",
            "0.30000000000000004   97\n",
            "31   32\n",
            "0.30000000000000004   98\n",
            "31   32\n",
            "0.30000000000000004   99\n",
            "32   33\n",
            "0.30000000000000004   100\n",
            "32   33\n",
            "0.30000000000000004   101\n",
            "33   34\n",
            "0.30000000000000004   102\n",
            "33   34\n",
            "0.30000000000000004   103\n",
            "34   35\n",
            "0.30000000000000004   104\n",
            "34   35\n",
            "0.30000000000000004   105\n",
            "34   35\n",
            "0.30000000000000004   106\n",
            "34   35\n",
            "0.30000000000000004   107\n",
            "34   35\n",
            "0.30000000000000004   108\n",
            "34   35\n",
            "0.30000000000000004   109\n",
            "34   35\n",
            "0.30000000000000004   110\n",
            "34   35\n",
            "0.30000000000000004   111\n",
            "34   35\n",
            "0.30000000000000004   112\n",
            "34   35\n",
            "0.30000000000000004   113\n",
            "34   35\n",
            "0.30000000000000004   114\n",
            "34   35\n",
            "0.30000000000000004   115\n",
            "34   35\n",
            "0.30000000000000004   116\n",
            "35   36\n",
            "0.30000000000000004   117\n",
            "35   36\n",
            "0.30000000000000004   118\n",
            "35   36\n",
            "0.30000000000000004   119\n",
            "35   36\n",
            "0.30000000000000004   120\n",
            "35   36\n",
            "0.30000000000000004   121\n",
            "35   36\n",
            "0.30000000000000004   122\n",
            "35   36\n",
            "0.30000000000000004   123\n",
            "35   36\n",
            "0.30000000000000004   124\n",
            "35   36\n",
            "0.30000000000000004   125\n",
            "35   36\n",
            "0.30000000000000004   126\n",
            "35   36\n",
            "0.30000000000000004   127\n",
            "36   37\n",
            "0.30000000000000004   128\n",
            "36   37\n",
            "0.30000000000000004   129\n",
            "37   38\n",
            "0.30000000000000004   130\n",
            "38   39\n",
            "0.30000000000000004   131\n",
            "38   39\n",
            "0.30000000000000004   132\n",
            "38   39\n",
            "0.30000000000000004   133\n",
            "38   39\n",
            "0.30000000000000004   134\n",
            "39   40\n",
            "0.30000000000000004   135\n",
            "39   40\n",
            "0.30000000000000004   136\n",
            "39   40\n",
            "0.30000000000000004   137\n",
            "40   41\n",
            "0.30000000000000004   138\n",
            "40   41\n",
            "0.30000000000000004   139\n",
            "40   41\n",
            "0.30000000000000004   140\n",
            "40   41\n",
            "0.30000000000000004   141\n",
            "41   42\n",
            "0.30000000000000004   142\n",
            "41   42\n",
            "0.30000000000000004   143\n",
            "42   43\n",
            "0.30000000000000004   144\n",
            "42   43\n",
            "0.30000000000000004   145\n",
            "42   43\n",
            "0.30000000000000004   146\n",
            "43   44\n",
            "0.30000000000000004   147\n",
            "43   44\n",
            "0.30000000000000004   148\n",
            "43   44\n",
            "0.30000000000000004   149\n",
            "43   44\n",
            "0.30000000000000004   150\n",
            "43   44\n",
            "0.30000000000000004   151\n",
            "44   45\n",
            "0.30000000000000004   152\n",
            "45   46\n",
            "0.30000000000000004   153\n",
            "46   47\n",
            "0.30000000000000004   154\n",
            "46   47\n",
            "0.30000000000000004   155\n",
            "46   47\n",
            "0.30000000000000004   156\n",
            "46   47\n",
            "0.30000000000000004   157\n",
            "46   47\n",
            "0.30000000000000004   158\n",
            "46   47\n",
            "0.30000000000000004   159\n",
            "47   48\n",
            "0.30000000000000004   160\n",
            "47   48\n",
            "0.30000000000000004   161\n",
            "47   48\n",
            "0.30000000000000004   162\n",
            "48   49\n",
            "0.30000000000000004   163\n",
            "48   49\n",
            "0.30000000000000004   164\n",
            "48   49\n",
            "0.30000000000000004   165\n",
            "48   49\n",
            "0.30000000000000004   166\n",
            "48   49\n",
            "0.30000000000000004   167\n",
            "48   49\n",
            "0.30000000000000004   168\n",
            "48   49\n",
            "0.30000000000000004   169\n",
            "48   49\n",
            "0.30000000000000004   170\n",
            "48   49\n",
            "0.30000000000000004   171\n",
            "49   50\n",
            "0.30000000000000004   172\n",
            "50   51\n",
            "0.30000000000000004   173\n",
            "50   51\n",
            "0.30000000000000004   174\n",
            "50   51\n",
            "0.30000000000000004   175\n",
            "50   51\n",
            "0.30000000000000004   176\n",
            "51   52\n",
            "0.30000000000000004   177\n",
            "51   52\n",
            "0.30000000000000004   178\n",
            "51   52\n",
            "0.30000000000000004   179\n",
            "51   52\n",
            "0.30000000000000004   180\n",
            "51   52\n",
            "0.30000000000000004   181\n",
            "52   53\n",
            "0.30000000000000004   182\n",
            "52   53\n",
            "0.30000000000000004   183\n",
            "53   54\n",
            "0.30000000000000004   184\n",
            "54   55\n",
            "0.30000000000000004   185\n",
            "54   55\n",
            "0.30000000000000004   186\n",
            "55   56\n",
            "0.30000000000000004   187\n",
            "56   57\n",
            "0.30000000000000004   188\n",
            "56   57\n",
            "0.30000000000000004   189\n",
            "56   57\n",
            "0.30000000000000004   190\n",
            "56   57\n",
            "0.30000000000000004   191\n",
            "57   58\n",
            "0.30000000000000004   192\n",
            "58   59\n",
            "0.30000000000000004   193\n",
            "58   59\n",
            "0.30000000000000004   194\n",
            "58   59\n",
            "0.30000000000000004   195\n",
            "58   59\n",
            "0.30000000000000004   196\n",
            "58   59\n",
            "0.30000000000000004   197\n",
            "59   60\n",
            "0.30000000000000004   198\n",
            "59   60\n",
            "0.30000000000000004   199\n",
            "59   60\n",
            "Epsilon value -  0.4\n",
            "\n",
            "0.4   0\n",
            "1   1\n",
            "0.4   1\n",
            "2   2\n",
            "0.4   2\n",
            "3   3\n",
            "0.4   3\n",
            "3   3\n",
            "0.4   4\n",
            "3   3\n",
            "0.4   5\n",
            "4   4\n",
            "0.4   6\n",
            "4   4\n",
            "0.4   7\n",
            "5   5\n",
            "0.4   8\n",
            "6   6\n",
            "0.4   9\n",
            "6   6\n",
            "0.4   10\n",
            "6   6\n",
            "0.4   11\n",
            "6   6\n",
            "0.4   12\n",
            "6   6\n",
            "0.4   13\n",
            "6   6\n",
            "0.4   14\n",
            "6   6\n",
            "0.4   15\n",
            "6   6\n",
            "0.4   16\n",
            "6   6\n",
            "0.4   17\n",
            "7   7\n",
            "0.4   18\n",
            "7   7\n",
            "0.4   19\n",
            "7   7\n",
            "0.4   20\n",
            "7   7\n",
            "0.4   21\n",
            "7   7\n",
            "0.4   22\n",
            "7   7\n",
            "0.4   23\n",
            "8   8\n",
            "0.4   24\n",
            "8   8\n",
            "0.4   25\n",
            "8   8\n",
            "0.4   26\n",
            "8   8\n",
            "0.4   27\n",
            "8   8\n",
            "0.4   28\n",
            "8   8\n",
            "0.4   29\n",
            "9   9\n",
            "0.4   30\n",
            "9   9\n",
            "0.4   31\n",
            "9   9\n",
            "0.4   32\n",
            "9   9\n",
            "0.4   33\n",
            "10   10\n",
            "0.4   34\n",
            "10   10\n",
            "0.4   35\n",
            "11   11\n",
            "0.4   36\n",
            "11   11\n",
            "0.4   37\n",
            "12   12\n",
            "0.4   38\n",
            "12   12\n",
            "0.4   39\n",
            "12   12\n",
            "0.4   40\n",
            "12   12\n",
            "0.4   41\n",
            "12   12\n",
            "0.4   42\n",
            "12   12\n",
            "0.4   43\n",
            "13   13\n",
            "0.4   44\n",
            "13   13\n",
            "0.4   45\n",
            "13   13\n",
            "0.4   46\n",
            "14   14\n",
            "0.4   47\n",
            "14   14\n",
            "0.4   48\n",
            "15   15\n",
            "0.4   49\n",
            "16   16\n",
            "0.4   50\n",
            "16   16\n",
            "0.4   51\n",
            "16   16\n",
            "0.4   52\n",
            "16   16\n",
            "0.4   53\n",
            "16   16\n",
            "0.4   54\n",
            "16   16\n",
            "0.4   55\n",
            "17   17\n",
            "0.4   56\n",
            "17   17\n",
            "0.4   57\n",
            "17   17\n",
            "0.4   58\n",
            "17   17\n",
            "0.4   59\n",
            "18   18\n",
            "0.4   60\n",
            "18   18\n",
            "0.4   61\n",
            "19   19\n",
            "0.4   62\n",
            "20   20\n",
            "0.4   63\n",
            "21   21\n",
            "0.4   64\n",
            "22   22\n",
            "0.4   65\n",
            "23   23\n",
            "0.4   66\n",
            "23   23\n",
            "0.4   67\n",
            "24   24\n",
            "0.4   68\n",
            "24   24\n",
            "0.4   69\n",
            "24   24\n",
            "0.4   70\n",
            "24   24\n",
            "0.4   71\n",
            "24   24\n",
            "0.4   72\n",
            "25   25\n",
            "0.4   73\n",
            "25   25\n",
            "0.4   74\n",
            "26   26\n",
            "0.4   75\n",
            "26   26\n",
            "0.4   76\n",
            "26   26\n",
            "0.4   77\n",
            "27   27\n",
            "0.4   78\n",
            "27   27\n",
            "0.4   79\n",
            "28   28\n",
            "0.4   80\n",
            "28   28\n",
            "0.4   81\n",
            "28   28\n",
            "0.4   82\n",
            "28   28\n",
            "0.4   83\n",
            "29   29\n",
            "0.4   84\n",
            "30   30\n",
            "0.4   85\n",
            "30   30\n",
            "0.4   86\n",
            "30   30\n",
            "0.4   87\n",
            "30   30\n",
            "0.4   88\n",
            "30   30\n",
            "0.4   89\n",
            "31   31\n",
            "0.4   90\n",
            "32   32\n",
            "0.4   91\n",
            "32   32\n",
            "0.4   92\n",
            "32   32\n",
            "0.4   93\n",
            "32   32\n",
            "0.4   94\n",
            "32   32\n",
            "0.4   95\n",
            "32   32\n",
            "0.4   96\n",
            "33   33\n",
            "0.4   97\n",
            "33   33\n",
            "0.4   98\n",
            "33   33\n",
            "0.4   99\n",
            "34   34\n",
            "0.4   100\n",
            "34   34\n",
            "0.4   101\n",
            "35   35\n",
            "0.4   102\n",
            "35   35\n",
            "0.4   103\n",
            "36   36\n",
            "0.4   104\n",
            "36   36\n",
            "0.4   105\n",
            "36   36\n",
            "0.4   106\n",
            "36   36\n",
            "0.4   107\n",
            "36   36\n",
            "0.4   108\n",
            "37   37\n",
            "0.4   109\n",
            "37   37\n",
            "0.4   110\n",
            "37   37\n",
            "0.4   111\n",
            "37   37\n",
            "0.4   112\n",
            "37   37\n",
            "0.4   113\n",
            "37   37\n",
            "0.4   114\n",
            "37   37\n",
            "0.4   115\n",
            "37   37\n",
            "0.4   116\n",
            "38   38\n",
            "0.4   117\n",
            "38   38\n",
            "0.4   118\n",
            "38   38\n",
            "0.4   119\n",
            "38   38\n",
            "0.4   120\n",
            "38   38\n",
            "0.4   121\n",
            "38   38\n",
            "0.4   122\n",
            "38   38\n",
            "0.4   123\n",
            "38   38\n",
            "0.4   124\n",
            "38   38\n",
            "0.4   125\n",
            "38   38\n",
            "0.4   126\n",
            "38   38\n",
            "0.4   127\n",
            "39   39\n",
            "0.4   128\n",
            "39   39\n",
            "0.4   129\n",
            "40   40\n",
            "0.4   130\n",
            "41   41\n",
            "0.4   131\n",
            "41   41\n",
            "0.4   132\n",
            "41   41\n",
            "0.4   133\n",
            "41   41\n",
            "0.4   134\n",
            "42   42\n",
            "0.4   135\n",
            "42   42\n",
            "0.4   136\n",
            "42   42\n",
            "0.4   137\n",
            "43   43\n",
            "0.4   138\n",
            "43   43\n",
            "0.4   139\n",
            "43   43\n",
            "0.4   140\n",
            "43   43\n",
            "0.4   141\n",
            "44   44\n",
            "0.4   142\n",
            "44   44\n",
            "0.4   143\n",
            "45   45\n",
            "0.4   144\n",
            "45   45\n",
            "0.4   145\n",
            "45   45\n",
            "0.4   146\n",
            "46   46\n",
            "0.4   147\n",
            "46   46\n",
            "0.4   148\n",
            "46   46\n",
            "0.4   149\n",
            "46   46\n",
            "0.4   150\n",
            "46   46\n",
            "0.4   151\n",
            "47   47\n",
            "0.4   152\n",
            "48   48\n",
            "0.4   153\n",
            "49   49\n",
            "0.4   154\n",
            "49   49\n",
            "0.4   155\n",
            "49   49\n",
            "0.4   156\n",
            "49   49\n",
            "0.4   157\n",
            "49   49\n",
            "0.4   158\n",
            "49   49\n",
            "0.4   159\n",
            "50   50\n",
            "0.4   160\n",
            "50   50\n",
            "0.4   161\n",
            "50   50\n",
            "0.4   162\n",
            "51   51\n",
            "0.4   163\n",
            "51   51\n",
            "0.4   164\n",
            "51   51\n",
            "0.4   165\n",
            "51   51\n",
            "0.4   166\n",
            "51   51\n",
            "0.4   167\n",
            "51   51\n",
            "0.4   168\n",
            "51   51\n",
            "0.4   169\n",
            "51   51\n",
            "0.4   170\n",
            "51   51\n",
            "0.4   171\n",
            "52   52\n",
            "0.4   172\n",
            "53   53\n",
            "0.4   173\n",
            "53   53\n",
            "0.4   174\n",
            "53   53\n",
            "0.4   175\n",
            "54   54\n",
            "0.4   176\n",
            "55   55\n",
            "0.4   177\n",
            "55   55\n",
            "0.4   178\n",
            "55   55\n",
            "0.4   179\n",
            "55   55\n",
            "0.4   180\n",
            "55   55\n",
            "0.4   181\n",
            "56   56\n",
            "0.4   182\n",
            "56   56\n",
            "0.4   183\n",
            "56   56\n",
            "0.4   184\n",
            "57   57\n",
            "0.4   185\n",
            "57   57\n",
            "0.4   186\n",
            "58   58\n",
            "0.4   187\n",
            "59   59\n",
            "0.4   188\n",
            "59   59\n",
            "0.4   189\n",
            "59   59\n",
            "0.4   190\n",
            "59   59\n",
            "0.4   191\n",
            "60   60\n",
            "0.4   192\n",
            "61   61\n",
            "0.4   193\n",
            "61   61\n",
            "0.4   194\n",
            "61   61\n",
            "0.4   195\n",
            "61   61\n",
            "0.4   196\n",
            "61   61\n",
            "0.4   197\n",
            "62   62\n",
            "0.4   198\n",
            "62   62\n",
            "0.4   199\n",
            "62   62\n",
            "Epsilon value -  0.5\n",
            "\n",
            "0.5   0\n",
            "1   1\n",
            "0.5   1\n",
            "2   2\n",
            "0.5   2\n",
            "3   3\n",
            "0.5   3\n",
            "3   3\n",
            "0.5   4\n",
            "3   3\n",
            "0.5   5\n",
            "4   4\n",
            "0.5   6\n",
            "4   4\n",
            "0.5   7\n",
            "5   5\n",
            "0.5   8\n",
            "6   6\n",
            "0.5   9\n",
            "6   6\n",
            "0.5   10\n",
            "6   6\n",
            "0.5   11\n",
            "6   6\n",
            "0.5   12\n",
            "6   6\n",
            "0.5   13\n",
            "6   6\n",
            "0.5   14\n",
            "6   6\n",
            "0.5   15\n",
            "6   6\n",
            "0.5   16\n",
            "6   6\n",
            "0.5   17\n",
            "7   7\n",
            "0.5   18\n",
            "7   7\n",
            "0.5   19\n",
            "7   7\n",
            "0.5   20\n",
            "7   7\n",
            "0.5   21\n",
            "7   7\n",
            "0.5   22\n",
            "7   7\n",
            "0.5   23\n",
            "8   8\n",
            "0.5   24\n",
            "8   8\n",
            "0.5   25\n",
            "8   8\n",
            "0.5   26\n",
            "8   8\n",
            "0.5   27\n",
            "8   8\n",
            "0.5   28\n",
            "8   8\n",
            "0.5   29\n",
            "9   9\n",
            "0.5   30\n",
            "9   9\n",
            "0.5   31\n",
            "9   9\n",
            "0.5   32\n",
            "9   9\n",
            "0.5   33\n",
            "10   10\n",
            "0.5   34\n",
            "10   10\n",
            "0.5   35\n",
            "11   11\n",
            "0.5   36\n",
            "11   12\n",
            "0.5   37\n",
            "12   13\n",
            "0.5   38\n",
            "12   13\n",
            "0.5   39\n",
            "12   13\n",
            "0.5   40\n",
            "12   13\n",
            "0.5   41\n",
            "12   13\n",
            "0.5   42\n",
            "12   13\n",
            "0.5   43\n",
            "13   14\n",
            "0.5   44\n",
            "13   14\n",
            "0.5   45\n",
            "13   14\n",
            "0.5   46\n",
            "14   15\n",
            "0.5   47\n",
            "14   15\n",
            "0.5   48\n",
            "15   16\n",
            "0.5   49\n",
            "16   17\n",
            "0.5   50\n",
            "16   17\n",
            "0.5   51\n",
            "16   17\n",
            "0.5   52\n",
            "16   17\n",
            "0.5   53\n",
            "16   17\n",
            "0.5   54\n",
            "16   17\n",
            "0.5   55\n",
            "17   18\n",
            "0.5   56\n",
            "17   18\n",
            "0.5   57\n",
            "17   18\n",
            "0.5   58\n",
            "17   18\n",
            "0.5   59\n",
            "18   19\n",
            "0.5   60\n",
            "18   19\n",
            "0.5   61\n",
            "19   20\n",
            "0.5   62\n",
            "20   21\n",
            "0.5   63\n",
            "21   22\n",
            "0.5   64\n",
            "22   23\n",
            "0.5   65\n",
            "23   24\n",
            "0.5   66\n",
            "23   24\n",
            "0.5   67\n",
            "24   25\n",
            "0.5   68\n",
            "24   25\n",
            "0.5   69\n",
            "24   25\n",
            "0.5   70\n",
            "24   25\n",
            "0.5   71\n",
            "24   25\n",
            "0.5   72\n",
            "25   26\n",
            "0.5   73\n",
            "25   26\n",
            "0.5   74\n",
            "26   27\n",
            "0.5   75\n",
            "26   27\n",
            "0.5   76\n",
            "26   27\n",
            "0.5   77\n",
            "27   28\n",
            "0.5   78\n",
            "27   28\n",
            "0.5   79\n",
            "28   29\n",
            "0.5   80\n",
            "28   29\n",
            "0.5   81\n",
            "28   29\n",
            "0.5   82\n",
            "28   29\n",
            "0.5   83\n",
            "29   30\n",
            "0.5   84\n",
            "30   31\n",
            "0.5   85\n",
            "30   31\n",
            "0.5   86\n",
            "30   31\n",
            "0.5   87\n",
            "30   31\n",
            "0.5   88\n",
            "30   31\n",
            "0.5   89\n",
            "31   32\n",
            "0.5   90\n",
            "32   33\n",
            "0.5   91\n",
            "32   33\n",
            "0.5   92\n",
            "32   33\n",
            "0.5   93\n",
            "32   33\n",
            "0.5   94\n",
            "32   33\n",
            "0.5   95\n",
            "32   33\n",
            "0.5   96\n",
            "33   34\n",
            "0.5   97\n",
            "33   34\n",
            "0.5   98\n",
            "33   34\n",
            "0.5   99\n",
            "34   35\n",
            "0.5   100\n",
            "34   35\n",
            "0.5   101\n",
            "35   36\n",
            "0.5   102\n",
            "35   36\n",
            "0.5   103\n",
            "36   37\n",
            "0.5   104\n",
            "36   37\n",
            "0.5   105\n",
            "36   37\n",
            "0.5   106\n",
            "36   37\n",
            "0.5   107\n",
            "36   37\n",
            "0.5   108\n",
            "37   38\n",
            "0.5   109\n",
            "37   38\n",
            "0.5   110\n",
            "37   38\n",
            "0.5   111\n",
            "37   38\n",
            "0.5   112\n",
            "37   38\n",
            "0.5   113\n",
            "37   39\n",
            "0.5   114\n",
            "37   39\n",
            "0.5   115\n",
            "37   39\n",
            "0.5   116\n",
            "38   40\n",
            "0.5   117\n",
            "38   40\n",
            "0.5   118\n",
            "38   40\n",
            "0.5   119\n",
            "38   40\n",
            "0.5   120\n",
            "38   40\n",
            "0.5   121\n",
            "38   40\n",
            "0.5   122\n",
            "38   40\n",
            "0.5   123\n",
            "38   40\n",
            "0.5   124\n",
            "38   40\n",
            "0.5   125\n",
            "38   40\n",
            "0.5   126\n",
            "38   40\n",
            "0.5   127\n",
            "39   41\n",
            "0.5   128\n",
            "39   41\n",
            "0.5   129\n",
            "40   42\n",
            "0.5   130\n",
            "41   43\n",
            "0.5   131\n",
            "41   43\n",
            "0.5   132\n",
            "41   43\n",
            "0.5   133\n",
            "41   43\n",
            "0.5   134\n",
            "42   44\n",
            "0.5   135\n",
            "42   44\n",
            "0.5   136\n",
            "42   44\n",
            "0.5   137\n",
            "43   45\n",
            "0.5   138\n",
            "43   45\n",
            "0.5   139\n",
            "43   45\n",
            "0.5   140\n",
            "43   45\n",
            "0.5   141\n",
            "44   46\n",
            "0.5   142\n",
            "44   46\n",
            "0.5   143\n",
            "45   47\n",
            "0.5   144\n",
            "45   47\n",
            "0.5   145\n",
            "45   47\n",
            "0.5   146\n",
            "46   48\n",
            "0.5   147\n",
            "46   48\n",
            "0.5   148\n",
            "46   48\n",
            "0.5   149\n",
            "46   48\n",
            "0.5   150\n",
            "46   48\n",
            "0.5   151\n",
            "47   49\n",
            "0.5   152\n",
            "48   50\n",
            "0.5   153\n",
            "49   51\n",
            "0.5   154\n",
            "49   51\n",
            "0.5   155\n",
            "49   51\n",
            "0.5   156\n",
            "49   51\n",
            "0.5   157\n",
            "49   51\n",
            "0.5   158\n",
            "49   52\n",
            "0.5   159\n",
            "50   53\n",
            "0.5   160\n",
            "50   53\n",
            "0.5   161\n",
            "50   53\n",
            "0.5   162\n",
            "51   54\n",
            "0.5   163\n",
            "51   54\n",
            "0.5   164\n",
            "51   54\n",
            "0.5   165\n",
            "51   55\n",
            "0.5   166\n",
            "51   55\n",
            "0.5   167\n",
            "51   55\n",
            "0.5   168\n",
            "51   55\n",
            "0.5   169\n",
            "51   55\n",
            "0.5   170\n",
            "51   55\n",
            "0.5   171\n",
            "52   56\n",
            "0.5   172\n",
            "53   57\n",
            "0.5   173\n",
            "53   57\n",
            "0.5   174\n",
            "53   57\n",
            "0.5   175\n",
            "54   58\n",
            "0.5   176\n",
            "55   59\n",
            "0.5   177\n",
            "55   59\n",
            "0.5   178\n",
            "55   59\n",
            "0.5   179\n",
            "55   59\n",
            "0.5   180\n",
            "55   59\n",
            "0.5   181\n",
            "56   60\n",
            "0.5   182\n",
            "56   60\n",
            "0.5   183\n",
            "56   60\n",
            "0.5   184\n",
            "57   61\n",
            "0.5   185\n",
            "57   61\n",
            "0.5   186\n",
            "58   62\n",
            "0.5   187\n",
            "59   63\n",
            "0.5   188\n",
            "59   63\n",
            "0.5   189\n",
            "59   63\n",
            "0.5   190\n",
            "59   63\n",
            "0.5   191\n",
            "60   64\n",
            "0.5   192\n",
            "61   65\n",
            "0.5   193\n",
            "61   65\n",
            "0.5   194\n",
            "61   65\n",
            "0.5   195\n",
            "61   65\n",
            "0.5   196\n",
            "61   65\n",
            "0.5   197\n",
            "62   66\n",
            "0.5   198\n",
            "62   66\n",
            "0.5   199\n",
            "62   66\n",
            "Epsilon value -  0.6\n",
            "\n",
            "0.6   0\n",
            "1   1\n",
            "0.6   1\n",
            "2   2\n",
            "0.6   2\n",
            "3   3\n",
            "0.6   3\n",
            "3   3\n",
            "0.6   4\n",
            "3   3\n",
            "0.6   5\n",
            "4   4\n",
            "0.6   6\n",
            "4   5\n",
            "0.6   7\n",
            "5   6\n",
            "0.6   8\n",
            "6   7\n",
            "0.6   9\n",
            "6   7\n",
            "0.6   10\n",
            "6   7\n",
            "0.6   11\n",
            "6   7\n",
            "0.6   12\n",
            "6   7\n",
            "0.6   13\n",
            "6   7\n",
            "0.6   14\n",
            "6   7\n",
            "0.6   15\n",
            "6   7\n",
            "0.6   16\n",
            "6   7\n",
            "0.6   17\n",
            "7   8\n",
            "0.6   18\n",
            "7   8\n",
            "0.6   19\n",
            "7   8\n",
            "0.6   20\n",
            "7   8\n",
            "0.6   21\n",
            "7   8\n",
            "0.6   22\n",
            "7   8\n",
            "0.6   23\n",
            "8   9\n",
            "0.6   24\n",
            "8   9\n",
            "0.6   25\n",
            "8   9\n",
            "0.6   26\n",
            "8   9\n",
            "0.6   27\n",
            "8   9\n",
            "0.6   28\n",
            "8   9\n",
            "0.6   29\n",
            "9   10\n",
            "0.6   30\n",
            "9   10\n",
            "0.6   31\n",
            "9   10\n",
            "0.6   32\n",
            "9   10\n",
            "0.6   33\n",
            "10   11\n",
            "0.6   34\n",
            "10   11\n",
            "0.6   35\n",
            "11   12\n",
            "0.6   36\n",
            "11   13\n",
            "0.6   37\n",
            "12   14\n",
            "0.6   38\n",
            "12   14\n",
            "0.6   39\n",
            "12   14\n",
            "0.6   40\n",
            "12   15\n",
            "0.6   41\n",
            "12   15\n",
            "0.6   42\n",
            "12   15\n",
            "0.6   43\n",
            "13   16\n",
            "0.6   44\n",
            "13   16\n",
            "0.6   45\n",
            "13   16\n",
            "0.6   46\n",
            "14   17\n",
            "0.6   47\n",
            "14   17\n",
            "0.6   48\n",
            "15   18\n",
            "0.6   49\n",
            "16   19\n",
            "0.6   50\n",
            "16   19\n",
            "0.6   51\n",
            "16   19\n",
            "0.6   52\n",
            "16   19\n",
            "0.6   53\n",
            "16   19\n",
            "0.6   54\n",
            "16   19\n",
            "0.6   55\n",
            "17   20\n",
            "0.6   56\n",
            "17   20\n",
            "0.6   57\n",
            "17   20\n",
            "0.6   58\n",
            "17   20\n",
            "0.6   59\n",
            "18   21\n",
            "0.6   60\n",
            "18   22\n",
            "0.6   61\n",
            "19   23\n",
            "0.6   62\n",
            "20   24\n",
            "0.6   63\n",
            "21   25\n",
            "0.6   64\n",
            "22   26\n",
            "0.6   65\n",
            "23   27\n",
            "0.6   66\n",
            "23   27\n",
            "0.6   67\n",
            "24   28\n",
            "0.6   68\n",
            "24   28\n",
            "0.6   69\n",
            "24   28\n",
            "0.6   70\n",
            "24   28\n",
            "0.6   71\n",
            "24   28\n",
            "0.6   72\n",
            "25   29\n",
            "0.6   73\n",
            "25   29\n",
            "0.6   74\n",
            "26   30\n",
            "0.6   75\n",
            "26   30\n",
            "0.6   76\n",
            "26   30\n",
            "0.6   77\n",
            "27   31\n",
            "0.6   78\n",
            "27   31\n",
            "0.6   79\n",
            "28   32\n",
            "0.6   80\n",
            "28   32\n",
            "0.6   81\n",
            "28   32\n",
            "0.6   82\n",
            "28   32\n",
            "0.6   83\n",
            "29   33\n",
            "0.6   84\n",
            "30   34\n",
            "0.6   85\n",
            "30   34\n",
            "0.6   86\n",
            "30   34\n",
            "0.6   87\n",
            "30   34\n",
            "0.6   88\n",
            "30   34\n",
            "0.6   89\n",
            "31   35\n",
            "0.6   90\n",
            "32   36\n",
            "0.6   91\n",
            "32   36\n",
            "0.6   92\n",
            "32   36\n",
            "0.6   93\n",
            "32   37\n",
            "0.6   94\n",
            "32   37\n",
            "0.6   95\n",
            "32   37\n",
            "0.6   96\n",
            "33   38\n",
            "0.6   97\n",
            "33   39\n",
            "0.6   98\n",
            "33   39\n",
            "0.6   99\n",
            "34   40\n",
            "0.6   100\n",
            "34   40\n",
            "0.6   101\n",
            "35   41\n",
            "0.6   102\n",
            "35   42\n",
            "0.6   103\n",
            "36   43\n",
            "0.6   104\n",
            "36   43\n",
            "0.6   105\n",
            "36   43\n",
            "0.6   106\n",
            "36   43\n",
            "0.6   107\n",
            "36   43\n",
            "0.6   108\n",
            "37   44\n",
            "0.6   109\n",
            "37   44\n",
            "0.6   110\n",
            "37   44\n",
            "0.6   111\n",
            "37   44\n",
            "0.6   112\n",
            "37   44\n",
            "0.6   113\n",
            "37   45\n",
            "0.6   114\n",
            "37   45\n",
            "0.6   115\n",
            "37   45\n",
            "0.6   116\n",
            "38   46\n",
            "0.6   117\n",
            "38   46\n",
            "0.6   118\n",
            "38   46\n",
            "0.6   119\n",
            "38   46\n",
            "0.6   120\n",
            "38   46\n",
            "0.6   121\n",
            "38   46\n",
            "0.6   122\n",
            "38   46\n",
            "0.6   123\n",
            "38   46\n",
            "0.6   124\n",
            "38   46\n",
            "0.6   125\n",
            "38   46\n",
            "0.6   126\n",
            "38   46\n",
            "0.6   127\n",
            "39   47\n",
            "0.6   128\n",
            "39   47\n",
            "0.6   129\n",
            "40   47\n",
            "0.6   130\n",
            "41   48\n",
            "0.6   131\n",
            "41   48\n",
            "0.6   132\n",
            "41   48\n",
            "0.6   133\n",
            "41   48\n",
            "0.6   134\n",
            "42   49\n",
            "0.6   135\n",
            "42   49\n",
            "0.6   136\n",
            "42   49\n",
            "0.6   137\n",
            "43   50\n",
            "0.6   138\n",
            "43   50\n",
            "0.6   139\n",
            "43   50\n",
            "0.6   140\n",
            "43   50\n",
            "0.6   141\n",
            "44   51\n",
            "0.6   142\n",
            "44   51\n",
            "0.6   143\n",
            "45   52\n",
            "0.6   144\n",
            "45   52\n",
            "0.6   145\n",
            "45   52\n",
            "0.6   146\n",
            "46   53\n",
            "0.6   147\n",
            "46   53\n",
            "0.6   148\n",
            "46   53\n",
            "0.6   149\n",
            "46   53\n",
            "0.6   150\n",
            "46   53\n",
            "0.6   151\n",
            "47   54\n",
            "0.6   152\n",
            "48   55\n",
            "0.6   153\n",
            "49   56\n",
            "0.6   154\n",
            "49   57\n",
            "0.6   155\n",
            "49   57\n",
            "0.6   156\n",
            "49   57\n",
            "0.6   157\n",
            "49   57\n",
            "0.6   158\n",
            "49   58\n",
            "0.6   159\n",
            "50   59\n",
            "0.6   160\n",
            "50   59\n",
            "0.6   161\n",
            "50   59\n",
            "0.6   162\n",
            "51   60\n",
            "0.6   163\n",
            "51   60\n",
            "0.6   164\n",
            "51   60\n",
            "0.6   165\n",
            "51   61\n",
            "0.6   166\n",
            "51   61\n",
            "0.6   167\n",
            "51   61\n",
            "0.6   168\n",
            "51   61\n",
            "0.6   169\n",
            "51   61\n",
            "0.6   170\n",
            "51   62\n",
            "0.6   171\n",
            "52   63\n",
            "0.6   172\n",
            "53   64\n",
            "0.6   173\n",
            "53   64\n",
            "0.6   174\n",
            "53   64\n",
            "0.6   175\n",
            "54   65\n",
            "0.6   176\n",
            "55   66\n",
            "0.6   177\n",
            "55   66\n",
            "0.6   178\n",
            "55   66\n",
            "0.6   179\n",
            "55   66\n",
            "0.6   180\n",
            "55   66\n",
            "0.6   181\n",
            "56   67\n",
            "0.6   182\n",
            "56   67\n",
            "0.6   183\n",
            "56   67\n",
            "0.6   184\n",
            "57   68\n",
            "0.6   185\n",
            "57   68\n",
            "0.6   186\n",
            "58   69\n",
            "0.6   187\n",
            "59   70\n",
            "0.6   188\n",
            "59   70\n",
            "0.6   189\n",
            "59   70\n",
            "0.6   190\n",
            "59   70\n",
            "0.6   191\n",
            "60   71\n",
            "0.6   192\n",
            "61   72\n",
            "0.6   193\n",
            "61   72\n",
            "0.6   194\n",
            "61   72\n",
            "0.6   195\n",
            "61   72\n",
            "0.6   196\n",
            "61   72\n",
            "0.6   197\n",
            "62   73\n",
            "0.6   198\n",
            "62   73\n",
            "0.6   199\n",
            "62   73\n",
            "Epsilon value -  0.7\n",
            "\n",
            "0.7   0\n",
            "1   1\n",
            "0.7   1\n",
            "2   2\n",
            "0.7   2\n",
            "3   3\n",
            "0.7   3\n",
            "3   3\n",
            "0.7   4\n",
            "3   3\n",
            "0.7   5\n",
            "4   4\n",
            "0.7   6\n",
            "4   5\n",
            "0.7   7\n",
            "5   6\n",
            "0.7   8\n",
            "6   7\n",
            "0.7   9\n",
            "6   7\n",
            "0.7   10\n",
            "6   7\n",
            "0.7   11\n",
            "6   7\n",
            "0.7   12\n",
            "6   7\n",
            "0.7   13\n",
            "6   7\n",
            "0.7   14\n",
            "6   7\n",
            "0.7   15\n",
            "6   7\n",
            "0.7   16\n",
            "6   7\n",
            "0.7   17\n",
            "7   8\n",
            "0.7   18\n",
            "7   8\n",
            "0.7   19\n",
            "7   8\n",
            "0.7   20\n",
            "7   8\n",
            "0.7   21\n",
            "7   8\n",
            "0.7   22\n",
            "7   8\n",
            "0.7   23\n",
            "8   9\n",
            "0.7   24\n",
            "8   9\n",
            "0.7   25\n",
            "8   9\n",
            "0.7   26\n",
            "8   9\n",
            "0.7   27\n",
            "8   9\n",
            "0.7   28\n",
            "8   9\n",
            "0.7   29\n",
            "9   10\n",
            "0.7   30\n",
            "9   10\n",
            "0.7   31\n",
            "9   10\n",
            "0.7   32\n",
            "9   10\n",
            "0.7   33\n",
            "10   11\n",
            "0.7   34\n",
            "10   11\n",
            "0.7   35\n",
            "11   12\n",
            "0.7   36\n",
            "11   13\n",
            "0.7   37\n",
            "12   14\n",
            "0.7   38\n",
            "12   14\n",
            "0.7   39\n",
            "12   14\n",
            "0.7   40\n",
            "12   15\n",
            "0.7   41\n",
            "12   15\n",
            "0.7   42\n",
            "12   15\n",
            "0.7   43\n",
            "13   16\n",
            "0.7   44\n",
            "13   16\n",
            "0.7   45\n",
            "13   16\n",
            "0.7   46\n",
            "14   17\n",
            "0.7   47\n",
            "14   17\n",
            "0.7   48\n",
            "15   18\n",
            "0.7   49\n",
            "16   19\n",
            "0.7   50\n",
            "16   19\n",
            "0.7   51\n",
            "16   19\n",
            "0.7   52\n",
            "16   19\n",
            "0.7   53\n",
            "16   19\n",
            "0.7   54\n",
            "16   19\n",
            "0.7   55\n",
            "17   20\n",
            "0.7   56\n",
            "17   20\n",
            "0.7   57\n",
            "17   20\n",
            "0.7   58\n",
            "17   20\n",
            "0.7   59\n",
            "18   21\n",
            "0.7   60\n",
            "18   22\n",
            "0.7   61\n",
            "19   23\n",
            "0.7   62\n",
            "20   24\n",
            "0.7   63\n",
            "21   25\n",
            "0.7   64\n",
            "22   26\n",
            "0.7   65\n",
            "23   27\n",
            "0.7   66\n",
            "23   28\n",
            "0.7   67\n",
            "24   29\n",
            "0.7   68\n",
            "24   29\n",
            "0.7   69\n",
            "24   30\n",
            "0.7   70\n",
            "24   30\n",
            "0.7   71\n",
            "24   30\n",
            "0.7   72\n",
            "25   31\n",
            "0.7   73\n",
            "25   31\n",
            "0.7   74\n",
            "26   32\n",
            "0.7   75\n",
            "26   32\n",
            "0.7   76\n",
            "26   32\n",
            "0.7   77\n",
            "27   33\n",
            "0.7   78\n",
            "27   33\n",
            "0.7   79\n",
            "28   34\n",
            "0.7   80\n",
            "28   34\n",
            "0.7   81\n",
            "28   34\n",
            "0.7   82\n",
            "28   34\n",
            "0.7   83\n",
            "29   35\n",
            "0.7   84\n",
            "30   36\n",
            "0.7   85\n",
            "30   36\n",
            "0.7   86\n",
            "30   36\n",
            "0.7   87\n",
            "30   36\n",
            "0.7   88\n",
            "30   36\n",
            "0.7   89\n",
            "31   37\n",
            "0.7   90\n",
            "32   38\n",
            "0.7   91\n",
            "32   38\n",
            "0.7   92\n",
            "32   38\n",
            "0.7   93\n",
            "32   39\n",
            "0.7   94\n",
            "32   39\n",
            "0.7   95\n",
            "32   40\n",
            "0.7   96\n",
            "33   41\n",
            "0.7   97\n",
            "33   42\n",
            "0.7   98\n",
            "33   42\n",
            "0.7   99\n",
            "34   43\n",
            "0.7   100\n",
            "34   43\n",
            "0.7   101\n",
            "35   44\n",
            "0.7   102\n",
            "35   45\n",
            "0.7   103\n",
            "36   46\n",
            "0.7   104\n",
            "36   46\n",
            "0.7   105\n",
            "36   46\n",
            "0.7   106\n",
            "36   46\n",
            "0.7   107\n",
            "36   46\n",
            "0.7   108\n",
            "37   47\n",
            "0.7   109\n",
            "37   47\n",
            "0.7   110\n",
            "37   47\n",
            "0.7   111\n",
            "37   47\n",
            "0.7   112\n",
            "37   47\n",
            "0.7   113\n",
            "37   48\n",
            "0.7   114\n",
            "37   48\n",
            "0.7   115\n",
            "37   48\n",
            "0.7   116\n",
            "38   49\n",
            "0.7   117\n",
            "38   49\n",
            "0.7   118\n",
            "38   49\n",
            "0.7   119\n",
            "38   49\n",
            "0.7   120\n",
            "38   50\n",
            "0.7   121\n",
            "38   51\n",
            "0.7   122\n",
            "38   51\n",
            "0.7   123\n",
            "38   52\n",
            "0.7   124\n",
            "38   52\n",
            "0.7   125\n",
            "38   52\n",
            "0.7   126\n",
            "38   52\n",
            "0.7   127\n",
            "39   53\n",
            "0.7   128\n",
            "39   53\n",
            "0.7   129\n",
            "40   53\n",
            "0.7   130\n",
            "41   54\n",
            "0.7   131\n",
            "41   54\n",
            "0.7   132\n",
            "41   55\n",
            "0.7   133\n",
            "41   55\n",
            "0.7   134\n",
            "42   56\n",
            "0.7   135\n",
            "42   56\n",
            "0.7   136\n",
            "42   56\n",
            "0.7   137\n",
            "43   57\n",
            "0.7   138\n",
            "43   57\n",
            "0.7   139\n",
            "43   57\n",
            "0.7   140\n",
            "43   58\n",
            "0.7   141\n",
            "44   59\n",
            "0.7   142\n",
            "44   59\n",
            "0.7   143\n",
            "45   60\n",
            "0.7   144\n",
            "45   60\n",
            "0.7   145\n",
            "45   60\n",
            "0.7   146\n",
            "46   61\n",
            "0.7   147\n",
            "46   61\n",
            "0.7   148\n",
            "46   62\n",
            "0.7   149\n",
            "46   62\n",
            "0.7   150\n",
            "46   62\n",
            "0.7   151\n",
            "47   63\n",
            "0.7   152\n",
            "48   64\n",
            "0.7   153\n",
            "49   65\n",
            "0.7   154\n",
            "49   66\n",
            "0.7   155\n",
            "49   66\n",
            "0.7   156\n",
            "49   66\n",
            "0.7   157\n",
            "49   66\n",
            "0.7   158\n",
            "49   67\n",
            "0.7   159\n",
            "50   68\n",
            "0.7   160\n",
            "50   68\n",
            "0.7   161\n",
            "50   68\n",
            "0.7   162\n",
            "51   69\n",
            "0.7   163\n",
            "51   69\n",
            "0.7   164\n",
            "51   70\n",
            "0.7   165\n",
            "51   71\n",
            "0.7   166\n",
            "51   71\n",
            "0.7   167\n",
            "51   71\n",
            "0.7   168\n",
            "51   71\n",
            "0.7   169\n",
            "51   71\n",
            "0.7   170\n",
            "51   72\n",
            "0.7   171\n",
            "52   73\n",
            "0.7   172\n",
            "53   74\n",
            "0.7   173\n",
            "53   74\n",
            "0.7   174\n",
            "53   74\n",
            "0.7   175\n",
            "54   75\n",
            "0.7   176\n",
            "55   76\n",
            "0.7   177\n",
            "55   76\n",
            "0.7   178\n",
            "55   76\n",
            "0.7   179\n",
            "55   76\n",
            "0.7   180\n",
            "55   77\n",
            "0.7   181\n",
            "56   78\n",
            "0.7   182\n",
            "56   78\n",
            "0.7   183\n",
            "56   78\n",
            "0.7   184\n",
            "57   79\n",
            "0.7   185\n",
            "57   79\n",
            "0.7   186\n",
            "58   80\n",
            "0.7   187\n",
            "59   81\n",
            "0.7   188\n",
            "59   82\n",
            "0.7   189\n",
            "59   82\n",
            "0.7   190\n",
            "59   82\n",
            "0.7   191\n",
            "60   83\n",
            "0.7   192\n",
            "61   84\n",
            "0.7   193\n",
            "61   84\n",
            "0.7   194\n",
            "61   84\n",
            "0.7   195\n",
            "61   84\n",
            "0.7   196\n",
            "61   85\n",
            "0.7   197\n",
            "62   86\n",
            "0.7   198\n",
            "62   86\n",
            "0.7   199\n",
            "62   86\n",
            "Epsilon value -  0.7999999999999999\n",
            "\n",
            "0.7999999999999999   0\n",
            "1   1\n",
            "0.7999999999999999   1\n",
            "2   2\n",
            "0.7999999999999999   2\n",
            "3   3\n",
            "0.7999999999999999   3\n",
            "3   3\n",
            "0.7999999999999999   4\n",
            "3   3\n",
            "0.7999999999999999   5\n",
            "4   4\n",
            "0.7999999999999999   6\n",
            "4   5\n",
            "0.7999999999999999   7\n",
            "5   6\n",
            "0.7999999999999999   8\n",
            "6   7\n",
            "0.7999999999999999   9\n",
            "6   7\n",
            "0.7999999999999999   10\n",
            "6   7\n",
            "0.7999999999999999   11\n",
            "6   7\n",
            "0.7999999999999999   12\n",
            "6   8\n",
            "0.7999999999999999   13\n",
            "6   8\n",
            "0.7999999999999999   14\n",
            "6   8\n",
            "0.7999999999999999   15\n",
            "6   8\n",
            "0.7999999999999999   16\n",
            "6   8\n",
            "0.7999999999999999   17\n",
            "7   9\n",
            "0.7999999999999999   18\n",
            "7   9\n",
            "0.7999999999999999   19\n",
            "7   9\n",
            "0.7999999999999999   20\n",
            "7   9\n",
            "0.7999999999999999   21\n",
            "7   9\n",
            "0.7999999999999999   22\n",
            "7   9\n",
            "0.7999999999999999   23\n",
            "8   10\n",
            "0.7999999999999999   24\n",
            "8   10\n",
            "0.7999999999999999   25\n",
            "8   10\n",
            "0.7999999999999999   26\n",
            "8   10\n",
            "0.7999999999999999   27\n",
            "8   10\n",
            "0.7999999999999999   28\n",
            "8   10\n",
            "0.7999999999999999   29\n",
            "9   11\n",
            "0.7999999999999999   30\n",
            "9   11\n",
            "0.7999999999999999   31\n",
            "9   11\n",
            "0.7999999999999999   32\n",
            "9   11\n",
            "0.7999999999999999   33\n",
            "10   12\n",
            "0.7999999999999999   34\n",
            "10   12\n",
            "0.7999999999999999   35\n",
            "11   13\n",
            "0.7999999999999999   36\n",
            "11   14\n",
            "0.7999999999999999   37\n",
            "12   15\n",
            "0.7999999999999999   38\n",
            "12   15\n",
            "0.7999999999999999   39\n",
            "12   15\n",
            "0.7999999999999999   40\n",
            "12   16\n",
            "0.7999999999999999   41\n",
            "12   16\n",
            "0.7999999999999999   42\n",
            "12   16\n",
            "0.7999999999999999   43\n",
            "13   17\n",
            "0.7999999999999999   44\n",
            "13   17\n",
            "0.7999999999999999   45\n",
            "13   17\n",
            "0.7999999999999999   46\n",
            "14   18\n",
            "0.7999999999999999   47\n",
            "14   18\n",
            "0.7999999999999999   48\n",
            "15   19\n",
            "0.7999999999999999   49\n",
            "16   20\n",
            "0.7999999999999999   50\n",
            "16   20\n",
            "0.7999999999999999   51\n",
            "16   20\n",
            "0.7999999999999999   52\n",
            "16   20\n",
            "0.7999999999999999   53\n",
            "16   20\n",
            "0.7999999999999999   54\n",
            "16   20\n",
            "0.7999999999999999   55\n",
            "17   21\n",
            "0.7999999999999999   56\n",
            "17   21\n",
            "0.7999999999999999   57\n",
            "17   21\n",
            "0.7999999999999999   58\n",
            "17   21\n",
            "0.7999999999999999   59\n",
            "18   22\n",
            "0.7999999999999999   60\n",
            "18   23\n",
            "0.7999999999999999   61\n",
            "19   24\n",
            "0.7999999999999999   62\n",
            "20   25\n",
            "0.7999999999999999   63\n",
            "21   26\n",
            "0.7999999999999999   64\n",
            "22   27\n",
            "0.7999999999999999   65\n",
            "23   28\n",
            "0.7999999999999999   66\n",
            "23   29\n",
            "0.7999999999999999   67\n",
            "24   30\n",
            "0.7999999999999999   68\n",
            "24   30\n",
            "0.7999999999999999   69\n",
            "24   31\n",
            "0.7999999999999999   70\n",
            "24   32\n",
            "0.7999999999999999   71\n",
            "24   32\n",
            "0.7999999999999999   72\n",
            "25   33\n",
            "0.7999999999999999   73\n",
            "25   33\n",
            "0.7999999999999999   74\n",
            "26   34\n",
            "0.7999999999999999   75\n",
            "26   34\n",
            "0.7999999999999999   76\n",
            "26   34\n",
            "0.7999999999999999   77\n",
            "27   35\n",
            "0.7999999999999999   78\n",
            "27   35\n",
            "0.7999999999999999   79\n",
            "28   36\n",
            "0.7999999999999999   80\n",
            "28   36\n",
            "0.7999999999999999   81\n",
            "28   36\n",
            "0.7999999999999999   82\n",
            "28   36\n",
            "0.7999999999999999   83\n",
            "29   37\n",
            "0.7999999999999999   84\n",
            "30   38\n",
            "0.7999999999999999   85\n",
            "30   38\n",
            "0.7999999999999999   86\n",
            "30   38\n",
            "0.7999999999999999   87\n",
            "30   38\n",
            "0.7999999999999999   88\n",
            "30   38\n",
            "0.7999999999999999   89\n",
            "31   39\n",
            "0.7999999999999999   90\n",
            "32   40\n",
            "0.7999999999999999   91\n",
            "32   40\n",
            "0.7999999999999999   92\n",
            "32   40\n",
            "0.7999999999999999   93\n",
            "32   41\n",
            "0.7999999999999999   94\n",
            "32   41\n",
            "0.7999999999999999   95\n",
            "32   42\n",
            "0.7999999999999999   96\n",
            "33   43\n",
            "0.7999999999999999   97\n",
            "33   44\n",
            "0.7999999999999999   98\n",
            "33   44\n",
            "0.7999999999999999   99\n",
            "34   45\n",
            "0.7999999999999999   100\n",
            "34   45\n",
            "0.7999999999999999   101\n",
            "35   46\n",
            "0.7999999999999999   102\n",
            "35   47\n",
            "0.7999999999999999   103\n",
            "36   48\n",
            "0.7999999999999999   104\n",
            "36   48\n",
            "0.7999999999999999   105\n",
            "36   48\n",
            "0.7999999999999999   106\n",
            "36   48\n",
            "0.7999999999999999   107\n",
            "36   48\n",
            "0.7999999999999999   108\n",
            "37   49\n",
            "0.7999999999999999   109\n",
            "37   49\n",
            "0.7999999999999999   110\n",
            "37   49\n",
            "0.7999999999999999   111\n",
            "37   49\n",
            "0.7999999999999999   112\n",
            "37   49\n",
            "0.7999999999999999   113\n",
            "37   50\n",
            "0.7999999999999999   114\n",
            "37   50\n",
            "0.7999999999999999   115\n",
            "37   50\n",
            "0.7999999999999999   116\n",
            "38   51\n",
            "0.7999999999999999   117\n",
            "38   51\n",
            "0.7999999999999999   118\n",
            "38   51\n",
            "0.7999999999999999   119\n",
            "38   51\n",
            "0.7999999999999999   120\n",
            "38   52\n",
            "0.7999999999999999   121\n",
            "38   53\n",
            "0.7999999999999999   122\n",
            "38   53\n",
            "0.7999999999999999   123\n",
            "38   54\n",
            "0.7999999999999999   124\n",
            "38   54\n",
            "0.7999999999999999   125\n",
            "38   55\n",
            "0.7999999999999999   126\n",
            "38   55\n",
            "0.7999999999999999   127\n",
            "39   56\n",
            "0.7999999999999999   128\n",
            "39   56\n",
            "0.7999999999999999   129\n",
            "40   56\n",
            "0.7999999999999999   130\n",
            "41   57\n",
            "0.7999999999999999   131\n",
            "41   57\n",
            "0.7999999999999999   132\n",
            "41   58\n",
            "0.7999999999999999   133\n",
            "41   58\n",
            "0.7999999999999999   134\n",
            "42   59\n",
            "0.7999999999999999   135\n",
            "42   59\n",
            "0.7999999999999999   136\n",
            "42   59\n",
            "0.7999999999999999   137\n",
            "43   60\n",
            "0.7999999999999999   138\n",
            "43   60\n",
            "0.7999999999999999   139\n",
            "43   60\n",
            "0.7999999999999999   140\n",
            "43   61\n",
            "0.7999999999999999   141\n",
            "44   62\n",
            "0.7999999999999999   142\n",
            "44   62\n",
            "0.7999999999999999   143\n",
            "45   63\n",
            "0.7999999999999999   144\n",
            "45   63\n",
            "0.7999999999999999   145\n",
            "45   63\n",
            "0.7999999999999999   146\n",
            "46   64\n",
            "0.7999999999999999   147\n",
            "46   64\n",
            "0.7999999999999999   148\n",
            "46   65\n",
            "0.7999999999999999   149\n",
            "46   65\n",
            "0.7999999999999999   150\n",
            "46   65\n",
            "0.7999999999999999   151\n",
            "47   66\n",
            "0.7999999999999999   152\n",
            "48   67\n",
            "0.7999999999999999   153\n",
            "49   68\n",
            "0.7999999999999999   154\n",
            "49   69\n",
            "0.7999999999999999   155\n",
            "49   69\n",
            "0.7999999999999999   156\n",
            "49   69\n",
            "0.7999999999999999   157\n",
            "49   69\n",
            "0.7999999999999999   158\n",
            "49   70\n",
            "0.7999999999999999   159\n",
            "50   71\n",
            "0.7999999999999999   160\n",
            "50   71\n",
            "0.7999999999999999   161\n",
            "50   71\n",
            "0.7999999999999999   162\n",
            "51   72\n",
            "0.7999999999999999   163\n",
            "51   73\n",
            "0.7999999999999999   164\n",
            "51   74\n",
            "0.7999999999999999   165\n",
            "51   75\n",
            "0.7999999999999999   166\n",
            "51   75\n",
            "0.7999999999999999   167\n",
            "51   75\n",
            "0.7999999999999999   168\n",
            "51   75\n",
            "0.7999999999999999   169\n",
            "51   75\n",
            "0.7999999999999999   170\n",
            "51   76\n",
            "0.7999999999999999   171\n",
            "52   77\n",
            "0.7999999999999999   172\n",
            "53   78\n",
            "0.7999999999999999   173\n",
            "53   78\n",
            "0.7999999999999999   174\n",
            "53   78\n",
            "0.7999999999999999   175\n",
            "54   79\n",
            "0.7999999999999999   176\n",
            "55   80\n",
            "0.7999999999999999   177\n",
            "55   80\n",
            "0.7999999999999999   178\n",
            "55   80\n",
            "0.7999999999999999   179\n",
            "55   80\n",
            "0.7999999999999999   180\n",
            "55   81\n",
            "0.7999999999999999   181\n",
            "56   82\n",
            "0.7999999999999999   182\n",
            "56   82\n",
            "0.7999999999999999   183\n",
            "56   82\n",
            "0.7999999999999999   184\n",
            "57   83\n",
            "0.7999999999999999   185\n",
            "57   84\n",
            "0.7999999999999999   186\n",
            "58   85\n",
            "0.7999999999999999   187\n",
            "59   86\n",
            "0.7999999999999999   188\n",
            "59   87\n",
            "0.7999999999999999   189\n",
            "59   87\n",
            "0.7999999999999999   190\n",
            "59   87\n",
            "0.7999999999999999   191\n",
            "60   88\n",
            "0.7999999999999999   192\n",
            "61   89\n",
            "0.7999999999999999   193\n",
            "61   89\n",
            "0.7999999999999999   194\n",
            "61   89\n",
            "0.7999999999999999   195\n",
            "61   89\n",
            "0.7999999999999999   196\n",
            "61   90\n",
            "0.7999999999999999   197\n",
            "62   91\n",
            "0.7999999999999999   198\n",
            "62   91\n",
            "0.7999999999999999   199\n",
            "62   91\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for key in result.keys():\n",
        "  print(key,' ',result[key])"
      ],
      "metadata": {
        "id": "1JP3AMDHUXWK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd84d74f-e19b-4987-e8e3-c634ff009b47"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.1   (48, 48)\n",
            "0.2   (53, 54)\n",
            "0.30000000000000004   (60, 59)\n",
            "0.4   (62, 62)\n",
            "0.5   (66, 62)\n",
            "0.6   (73, 62)\n",
            "0.7   (86, 62)\n",
            "0.7999999999999999   (91, 62)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "PUor0wovVRZQ"
      },
      "execution_count": 29,
      "outputs": []
    }
  ]
}