{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "defensive_distillation_resnet_cifar10.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPSOrmmoec1h3fMQx4eSGK+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kirath2205/Final-Year-Project/blob/main/defensive_distillation_resnet_cifar10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "c9Vpz-objd13"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Input, UpSampling2D, Flatten, BatchNormalization, Dense, Dropout, GlobalAveragePooling2D , UpSampling3D , Lambda\n",
        "from tensorflow.keras import optimizers\n",
        "from keras.datasets import cifar100,cifar10,fashion_mnist\n",
        "import tensorflow as tf\n",
        "from keras.utils import np_utils\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from skimage.transform import resize\n",
        "from keras import applications\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50 , preprocess_input , decode_predictions\n",
        "from keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Distiller(keras.Model):\n",
        "    def __init__(self, student, teacher):\n",
        "        super(Distiller, self).__init__()\n",
        "        self.teacher = teacher\n",
        "        self.student = student\n",
        "\n",
        "    def compile(\n",
        "        self,\n",
        "        optimizer,\n",
        "        metrics,\n",
        "        student_loss_fn,\n",
        "        distillation_loss_fn,\n",
        "        alpha=0.2,\n",
        "        temperature=20,\n",
        "    ):\n",
        "        \"\"\" Configure the distiller.\n",
        "\n",
        "        Args:\n",
        "            optimizer: Keras optimizer for the student weights\n",
        "            metrics: Keras metrics for evaluation\n",
        "            student_loss_fn: Loss function of difference between student\n",
        "                predictions and ground-truth\n",
        "            distillation_loss_fn: Loss function of difference between soft\n",
        "                student predictions and soft teacher predictions\n",
        "            alpha: weight to student_loss_fn and 1-alpha to distillation_loss_fn\n",
        "            temperature: Temperature for softening probability distributions.\n",
        "                Larger temperature gives softer distributions.\n",
        "        \"\"\"\n",
        "        super(Distiller, self).compile(optimizer=optimizer, metrics=metrics)\n",
        "        self.student_loss_fn = student_loss_fn\n",
        "        self.distillation_loss_fn = distillation_loss_fn\n",
        "        self.alpha = alpha\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def train_step(self, data):\n",
        "        # Unpack data\n",
        "        x, y = data\n",
        "\n",
        "        # Forward pass of teacher\n",
        "        teacher_predictions = self.teacher(x, training=False)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Forward pass of student\n",
        "            student_predictions = self.student(x, training=True)\n",
        "\n",
        "            # Compute losses\n",
        "            student_loss = self.student_loss_fn(y, student_predictions)\n",
        "            distillation_loss = self.distillation_loss_fn(\n",
        "                tf.nn.softmax(teacher_predictions / self.temperature, axis=1),\n",
        "                tf.nn.softmax(student_predictions / self.temperature, axis=1),\n",
        "            )\n",
        "            loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n",
        "\n",
        "        # Compute gradients\n",
        "        trainable_vars = self.student.trainable_variables\n",
        "        gradients = tape.gradient(loss, trainable_vars)\n",
        "\n",
        "        # Update weights\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "\n",
        "        # Update the metrics configured in `compile()`.\n",
        "        self.compiled_metrics.update_state(y, student_predictions)\n",
        "\n",
        "        # Return a dict of performance\n",
        "        results = {m.name: m.result() for m in self.metrics}\n",
        "        results.update(\n",
        "            {\"student_loss\": student_loss, \"distillation_loss\": distillation_loss}\n",
        "        )\n",
        "        return results\n",
        "\n",
        "    def test_step(self, data):\n",
        "        # Unpack the data\n",
        "        x, y = data\n",
        "\n",
        "        # Compute predictions\n",
        "        y_prediction = self.student(x, training=False)\n",
        "\n",
        "        # Calculate the loss\n",
        "        student_loss = self.student_loss_fn(y, y_prediction)\n",
        "\n",
        "        # Update the metrics.\n",
        "        self.compiled_metrics.update_state(y, y_prediction)\n",
        "\n",
        "        # Return a dict of performance\n",
        "        results = {m.name: m.result() for m in self.metrics}\n",
        "        results.update({\"student_loss\": student_loss})\n",
        "        return results\n",
        "\n",
        "    def call(self, inputs, *args, **kwargs):\n",
        "       return self.student(inputs)"
      ],
      "metadata": {
        "id": "3vClqBLTkEoT"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_random_eraser(p=0.5, s_l=0.02, s_h=0.4, r_1=0.3, r_2=1/0.3, v_l=0, v_h=255, pixel_level=False):\n",
        "    def eraser(input_img):\n",
        "        img_h, img_w, img_c = input_img.shape\n",
        "        p_1 = np.random.rand()\n",
        "\n",
        "        if p_1 > p:\n",
        "            return input_img\n",
        "\n",
        "        while True:\n",
        "            s = np.random.uniform(s_l, s_h) * img_h * img_w\n",
        "            r = np.random.uniform(r_1, r_2)\n",
        "            w = int(np.sqrt(s / r))\n",
        "            h = int(np.sqrt(s * r))\n",
        "            left = np.random.randint(0, img_w)\n",
        "            top = np.random.randint(0, img_h)\n",
        "\n",
        "            if left + w <= img_w and top + h <= img_h:\n",
        "                break\n",
        "\n",
        "        if pixel_level:\n",
        "            c = np.random.uniform(v_l, v_h, (h, w, img_c))\n",
        "        else:\n",
        "            c = np.random.uniform(v_l, v_h)\n",
        "\n",
        "        input_img[top:top + h, left:left + w, :] = c\n",
        "\n",
        "        return input_img\n",
        "\n",
        "    return eraser"
      ],
      "metadata": {
        "id": "_4ac45bEkIq9"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def select_dataset(index=1): #1 for cifar10 , 2 for cifar100 , 3 for fashion mnist\n",
        "  if(index == 1):\n",
        "    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "    channel = 3\n",
        "    num_classes = 10\n",
        "  if(index == 2):\n",
        "    (x_train, y_train), (x_test, y_test) = cifar100.load_data()\n",
        "    num_classes = 100\n",
        "    channel = 3\n",
        "  if(index == 3):\n",
        "    (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "    x_train = x_train.reshape((60000, 28, 28, 1))\n",
        "    x_test =  x_test.reshape((10000, 28, 28, 1))\n",
        "    num_classes = 10\n",
        "    y_train = np_utils.to_categorical(y_train, num_classes)\n",
        "    y_test = np_utils.to_categorical(y_test, num_classes)\n",
        "    channel = 1\n",
        "    return (x_train , y_train , x_test , y_test , num_classes , channel)\n",
        "\n",
        "  #Pre-process the data\n",
        "  x_train = preprocess_input(x_train)\n",
        "  x_test = preprocess_input(x_test)\n",
        "\n",
        "  datagen = ImageDataGenerator(preprocessing_function=get_random_eraser(v_l=0, v_h=1, pixel_level=True))\n",
        "  datagen.fit(x_train)\n",
        "  y_train = np_utils.to_categorical(y_train, num_classes)\n",
        "  y_test = np_utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "  return (x_train , y_train , x_test , y_test , num_classes , channel , datagen)"
      ],
      "metadata": {
        "id": "Ds4lOH1qmZpq"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = 1\n",
        "x_train , y_train , x_test , y_test , num_classes ,channel , datagen = select_dataset(index)\n",
        "model_name = 'desktop/Trained_models/defensive_distillation_keras_resnet50_cifar10'\n",
        "model_path = 'desktop/Trained_models/defensive_distillation_keras_resnet50_cifar10.h5'"
      ],
      "metadata": {
        "id": "8TbyWALmnNDC"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = 2\n",
        "x_train , y_train , x_test , y_test , num_classes ,channel , datagen = select_dataset(index)\n",
        "model = Resnet50(num_classes,channel)\n",
        "model_name = 'desktop/Trained_models/defensive_distillation_keras_resnet50_cifar100'\n",
        "model_path = 'desktop/Trained_models/defensive_distillation_keras_resnet50_cifar100.h5'"
      ],
      "metadata": {
        "id": "2lRB-7LZncsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp = 200\n",
        "resnet_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "for layer in resnet_model.layers:\n",
        "    if isinstance(layer, BatchNormalization):\n",
        "      layer.trainable = True\n",
        "    else:\n",
        "      layer.trainable = False\n",
        "teacher = keras.Sequential(\n",
        "    [\n",
        "        layers.UpSampling2D((7,7)),\n",
        "        resnet_model,\n",
        "        layers.GlobalAveragePooling2D(),\n",
        "        layers.Dense(256),\n",
        "        layers.ReLU(),\n",
        "        layers.Dropout(0.25),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dense(32),\n",
        "        layers.ReLU(),\n",
        "        layers.Dropout(0.1),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dense(num_classes),\n",
        "        layers.Lambda((lambda x: x / temp)),\n",
        "    ],\n",
        "    name=\"teacher\",\n",
        ")\n",
        "\n",
        "# Create the student\n",
        "student = keras.Sequential(\n",
        "    [\n",
        "        layers.UpSampling2D((7,7)),\n",
        "        resnet_model,\n",
        "        layers.GlobalAveragePooling2D(),\n",
        "        layers.Dense(256),\n",
        "        layers.ReLU(),\n",
        "        layers.Dropout(0.25),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dense(32),\n",
        "        layers.ReLU(),\n",
        "        layers.Dropout(0.1),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dense(num_classes),\n",
        "        layers.Lambda((lambda x: x / temp)),\n",
        "    ],\n",
        "    name=\"student\",\n",
        ")\n"
      ],
      "metadata": {
        "id": "cMXAsAZXnp9y"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 40\n",
        "callbacks = [ \n",
        "    tf.keras.callbacks.ModelCheckpoint(model_path, monitor='val_accuracy', verbose=1, save_best_only=True),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau( factor = 0.7, patience = 5, min_lr = 0.000001, verbose = 1 ), #patience = 7 and 20 for cifar-100 , patience = 5 and 10 for cifar-10\n",
        "    #tf.keras.callbacks.EarlyStopping(monitor = 'val_accuracy' , patience = 10)\n",
        "  ]\n",
        "batch_size = 128\n",
        "teacher.compile(\n",
        "    optimizer=keras.optimizers.Adam(),\n",
        "    loss=keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[keras.metrics.CategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "# Train and evaluate teacher on data.\n",
        "teacher.fit_generator(datagen.flow(x_train, y_train,\n",
        "                                  batch_size=batch_size),\n",
        "                                  steps_per_epoch=x_train.shape[0] // batch_size,\n",
        "                                  epochs=epochs,\n",
        "                                  validation_data=(x_test, y_test))\n",
        "teacher.evaluate(x_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQUorMgFnyxf",
        "outputId": "45c17929-c6e4-4451-f879-6645283d8e8e"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "390/390 [==============================] - 165s 423ms/step - loss: 2.2475 - categorical_accuracy: 0.7730 - val_loss: 2.1710 - val_categorical_accuracy: 0.9027\n",
            "Epoch 2/40\n",
            "390/390 [==============================] - 166s 426ms/step - loss: 2.1114 - categorical_accuracy: 0.8909 - val_loss: 2.0177 - val_categorical_accuracy: 0.9282\n",
            "Epoch 3/40\n",
            "390/390 [==============================] - 167s 428ms/step - loss: 1.9085 - categorical_accuracy: 0.9163 - val_loss: 1.7841 - val_categorical_accuracy: 0.9404\n",
            "Epoch 4/40\n",
            "390/390 [==============================] - 167s 427ms/step - loss: 1.6590 - categorical_accuracy: 0.9261 - val_loss: 1.5065 - val_categorical_accuracy: 0.9396\n",
            "Epoch 5/40\n",
            "390/390 [==============================] - 166s 426ms/step - loss: 1.3901 - categorical_accuracy: 0.9361 - val_loss: 1.2248 - val_categorical_accuracy: 0.9422\n",
            "Epoch 6/40\n",
            "390/390 [==============================] - 166s 427ms/step - loss: 1.1284 - categorical_accuracy: 0.9411 - val_loss: 0.9909 - val_categorical_accuracy: 0.9482\n",
            "Epoch 7/40\n",
            "390/390 [==============================] - 166s 426ms/step - loss: 0.8995 - categorical_accuracy: 0.9442 - val_loss: 0.8075 - val_categorical_accuracy: 0.9483\n",
            "Epoch 8/40\n",
            "390/390 [==============================] - 166s 426ms/step - loss: 0.7061 - categorical_accuracy: 0.9498 - val_loss: 0.5763 - val_categorical_accuracy: 0.9538\n",
            "Epoch 9/40\n",
            "390/390 [==============================] - 166s 427ms/step - loss: 0.5602 - categorical_accuracy: 0.9529 - val_loss: 0.4810 - val_categorical_accuracy: 0.9523\n",
            "Epoch 10/40\n",
            "390/390 [==============================] - 166s 426ms/step - loss: 0.4477 - categorical_accuracy: 0.9558 - val_loss: 0.3928 - val_categorical_accuracy: 0.9514\n",
            "Epoch 11/40\n",
            "390/390 [==============================] - 166s 426ms/step - loss: 0.3634 - categorical_accuracy: 0.9588 - val_loss: 0.3241 - val_categorical_accuracy: 0.9511\n",
            "Epoch 12/40\n",
            "390/390 [==============================] - 166s 427ms/step - loss: 0.3016 - categorical_accuracy: 0.9615 - val_loss: 0.2707 - val_categorical_accuracy: 0.9531\n",
            "Epoch 13/40\n",
            "390/390 [==============================] - 166s 427ms/step - loss: 0.2574 - categorical_accuracy: 0.9620 - val_loss: 0.2319 - val_categorical_accuracy: 0.9551\n",
            "Epoch 14/40\n",
            "390/390 [==============================] - 166s 426ms/step - loss: 0.2271 - categorical_accuracy: 0.9625 - val_loss: 0.2154 - val_categorical_accuracy: 0.9544\n",
            "Epoch 15/40\n",
            "390/390 [==============================] - 166s 426ms/step - loss: 0.1974 - categorical_accuracy: 0.9646 - val_loss: 0.2067 - val_categorical_accuracy: 0.9524\n",
            "Epoch 16/40\n",
            "390/390 [==============================] - 166s 427ms/step - loss: 0.1752 - categorical_accuracy: 0.9656 - val_loss: 0.1868 - val_categorical_accuracy: 0.9525\n",
            "Epoch 17/40\n",
            "390/390 [==============================] - 166s 426ms/step - loss: 0.1598 - categorical_accuracy: 0.9667 - val_loss: 0.1809 - val_categorical_accuracy: 0.9534\n",
            "Epoch 18/40\n",
            "390/390 [==============================] - 166s 426ms/step - loss: 0.1472 - categorical_accuracy: 0.9679 - val_loss: 0.1760 - val_categorical_accuracy: 0.9528\n",
            "Epoch 19/40\n",
            "390/390 [==============================] - 166s 427ms/step - loss: 0.1349 - categorical_accuracy: 0.9700 - val_loss: 0.1670 - val_categorical_accuracy: 0.9525\n",
            "Epoch 20/40\n",
            "390/390 [==============================] - 166s 426ms/step - loss: 0.1282 - categorical_accuracy: 0.9693 - val_loss: 0.1562 - val_categorical_accuracy: 0.9569\n",
            "Epoch 21/40\n",
            "390/390 [==============================] - 166s 426ms/step - loss: 0.1231 - categorical_accuracy: 0.9685 - val_loss: 0.1599 - val_categorical_accuracy: 0.9544\n",
            "Epoch 22/40\n",
            "390/390 [==============================] - 166s 426ms/step - loss: 0.1172 - categorical_accuracy: 0.9696 - val_loss: 0.1600 - val_categorical_accuracy: 0.9549\n",
            "Epoch 23/40\n",
            "390/390 [==============================] - 166s 426ms/step - loss: 0.1102 - categorical_accuracy: 0.9711 - val_loss: 0.1529 - val_categorical_accuracy: 0.9546\n",
            "Epoch 24/40\n",
            "390/390 [==============================] - 166s 426ms/step - loss: 0.1068 - categorical_accuracy: 0.9714 - val_loss: 0.1576 - val_categorical_accuracy: 0.9529\n",
            "Epoch 25/40\n",
            "390/390 [==============================] - 166s 426ms/step - loss: 0.1016 - categorical_accuracy: 0.9726 - val_loss: 0.1577 - val_categorical_accuracy: 0.9547\n",
            "Epoch 26/40\n",
            "390/390 [==============================] - 166s 426ms/step - loss: 0.0961 - categorical_accuracy: 0.9732 - val_loss: 0.1504 - val_categorical_accuracy: 0.9559\n",
            "Epoch 27/40\n",
            "390/390 [==============================] - 166s 426ms/step - loss: 0.0935 - categorical_accuracy: 0.9734 - val_loss: 0.1576 - val_categorical_accuracy: 0.9542\n",
            "Epoch 28/40\n",
            "390/390 [==============================] - 166s 426ms/step - loss: 0.0947 - categorical_accuracy: 0.9726 - val_loss: 0.1483 - val_categorical_accuracy: 0.9560\n",
            "Epoch 29/40\n",
            "390/390 [==============================] - 166s 426ms/step - loss: 0.0918 - categorical_accuracy: 0.9741 - val_loss: 0.1500 - val_categorical_accuracy: 0.9565\n",
            "Epoch 30/40\n",
            "390/390 [==============================] - 166s 426ms/step - loss: 0.0918 - categorical_accuracy: 0.9739 - val_loss: 0.1531 - val_categorical_accuracy: 0.9568\n",
            "Epoch 31/40\n",
            "390/390 [==============================] - 166s 426ms/step - loss: 0.0879 - categorical_accuracy: 0.9743 - val_loss: 0.1652 - val_categorical_accuracy: 0.9510\n",
            "Epoch 32/40\n",
            "390/390 [==============================] - 166s 426ms/step - loss: 0.0902 - categorical_accuracy: 0.9734 - val_loss: 0.1514 - val_categorical_accuracy: 0.9547\n",
            "Epoch 33/40\n",
            "390/390 [==============================] - 166s 426ms/step - loss: 0.0845 - categorical_accuracy: 0.9752 - val_loss: 0.1601 - val_categorical_accuracy: 0.9553\n",
            "Epoch 34/40\n",
            "390/390 [==============================] - 166s 426ms/step - loss: 0.0850 - categorical_accuracy: 0.9748 - val_loss: 0.1479 - val_categorical_accuracy: 0.9585\n",
            "Epoch 35/40\n",
            "390/390 [==============================] - 166s 426ms/step - loss: 0.0813 - categorical_accuracy: 0.9760 - val_loss: 0.1471 - val_categorical_accuracy: 0.9572\n",
            "Epoch 36/40\n",
            "390/390 [==============================] - 166s 426ms/step - loss: 0.0822 - categorical_accuracy: 0.9759 - val_loss: 0.1547 - val_categorical_accuracy: 0.9564\n",
            "Epoch 37/40\n",
            "390/390 [==============================] - 166s 426ms/step - loss: 0.0812 - categorical_accuracy: 0.9758 - val_loss: 0.1528 - val_categorical_accuracy: 0.9558\n",
            "Epoch 38/40\n",
            "390/390 [==============================] - 166s 426ms/step - loss: 0.0774 - categorical_accuracy: 0.9767 - val_loss: 0.1584 - val_categorical_accuracy: 0.9542\n",
            "Epoch 39/40\n",
            "390/390 [==============================] - 166s 427ms/step - loss: 0.0791 - categorical_accuracy: 0.9768 - val_loss: 0.1546 - val_categorical_accuracy: 0.9569\n",
            "Epoch 40/40\n",
            "390/390 [==============================] - 166s 427ms/step - loss: 0.0762 - categorical_accuracy: 0.9777 - val_loss: 0.1618 - val_categorical_accuracy: 0.9526\n",
            "313/313 [==============================] - 14s 44ms/step - loss: 0.1618 - categorical_accuracy: 0.9526\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.16183745861053467, 0.9526000022888184]"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "2L3ki1WWzaSw"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "distiller = Distiller(student=student, teacher=teacher)\n",
        "distiller.compile(\n",
        "    optimizer=keras.optimizers.Adam(),\n",
        "    metrics=[keras.metrics.CategoricalAccuracy()],\n",
        "    student_loss_fn=keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "    distillation_loss_fn=keras.losses.KLDivergence(),\n",
        "    alpha=0.2,\n",
        "    temperature=200,\n",
        ")\n",
        "\n",
        "# Distill teacher to student\n",
        "distiller.fit_generator(datagen.flow(x_train, y_train,\n",
        "                                  batch_size=batch_size),\n",
        "                                  steps_per_epoch=x_train.shape[0] // batch_size,\n",
        "                                  epochs=epochs,\n",
        "                                  validation_data=(x_test, y_test),\n",
        "                                  callbacks = callbacks)\n",
        "\n",
        "# Evaluate student on test dataset\n",
        "distiller.evaluate(x_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-u-x9gYOoCW0",
        "outputId": "934a6380-7351-4d31-beaa-77ff3c379eda"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.9300 - student_loss: 2.2402 - distillation_loss: 1.1958e-04WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: categorical_accuracy,student_loss,distillation_loss,val_categorical_accuracy,val_student_loss,lr\n",
            "390/390 [==============================] - 227s 583ms/step - categorical_accuracy: 0.9300 - student_loss: 2.2400 - distillation_loss: 1.1957e-04 - val_categorical_accuracy: 0.9400 - val_student_loss: 2.1615 - lr: 0.0010\n",
            "Epoch 2/40\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.9542 - student_loss: 2.1035 - distillation_loss: 1.1785e-04WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: categorical_accuracy,student_loss,distillation_loss,val_categorical_accuracy,val_student_loss,lr\n",
            "390/390 [==============================] - 227s 581ms/step - categorical_accuracy: 0.9542 - student_loss: 2.1033 - distillation_loss: 1.1784e-04 - val_categorical_accuracy: 0.9547 - val_student_loss: 2.0042 - lr: 0.0010\n",
            "Epoch 3/40\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.9583 - student_loss: 1.8967 - distillation_loss: 1.1286e-04WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: categorical_accuracy,student_loss,distillation_loss,val_categorical_accuracy,val_student_loss,lr\n",
            "390/390 [==============================] - 227s 581ms/step - categorical_accuracy: 0.9583 - student_loss: 1.8964 - distillation_loss: 1.1284e-04 - val_categorical_accuracy: 0.9456 - val_student_loss: 1.7503 - lr: 0.0010\n",
            "Epoch 4/40\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.9644 - student_loss: 1.6390 - distillation_loss: 1.0554e-04WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: categorical_accuracy,student_loss,distillation_loss,val_categorical_accuracy,val_student_loss,lr\n",
            "390/390 [==============================] - 226s 580ms/step - categorical_accuracy: 0.9644 - student_loss: 1.6386 - distillation_loss: 1.0553e-04 - val_categorical_accuracy: 0.9556 - val_student_loss: 1.4695 - lr: 0.0010\n",
            "Epoch 5/40\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.9642 - student_loss: 1.3619 - distillation_loss: 9.7407e-05WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: categorical_accuracy,student_loss,distillation_loss,val_categorical_accuracy,val_student_loss,lr\n",
            "390/390 [==============================] - 226s 581ms/step - categorical_accuracy: 0.9642 - student_loss: 1.3615 - distillation_loss: 9.7400e-05 - val_categorical_accuracy: 0.9514 - val_student_loss: 1.1311 - lr: 0.0010\n",
            "Epoch 6/40\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.9666 - student_loss: 1.0901 - distillation_loss: 8.9502e-05WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: categorical_accuracy,student_loss,distillation_loss,val_categorical_accuracy,val_student_loss,lr\n",
            "390/390 [==============================] - 227s 581ms/step - categorical_accuracy: 0.9666 - student_loss: 1.0898 - distillation_loss: 8.9487e-05 - val_categorical_accuracy: 0.9549 - val_student_loss: 0.8891 - lr: 0.0010\n",
            "Epoch 7/40\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.9686 - student_loss: 0.8506 - distillation_loss: 8.0934e-05WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: categorical_accuracy,student_loss,distillation_loss,val_categorical_accuracy,val_student_loss,lr\n",
            "390/390 [==============================] - 226s 580ms/step - categorical_accuracy: 0.9686 - student_loss: 0.8503 - distillation_loss: 8.0915e-05 - val_categorical_accuracy: 0.9568 - val_student_loss: 0.7002 - lr: 0.0010\n",
            "Epoch 8/40\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.9706 - student_loss: 0.6547 - distillation_loss: 7.4092e-05WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: categorical_accuracy,student_loss,distillation_loss,val_categorical_accuracy,val_student_loss,lr\n",
            "390/390 [==============================] - 226s 581ms/step - categorical_accuracy: 0.9706 - student_loss: 0.6544 - distillation_loss: 7.4092e-05 - val_categorical_accuracy: 0.9555 - val_student_loss: 0.5929 - lr: 0.0010\n",
            "Epoch 9/40\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.9710 - student_loss: 0.5074 - distillation_loss: 6.7629e-05WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: categorical_accuracy,student_loss,distillation_loss,val_categorical_accuracy,val_student_loss,lr\n",
            "390/390 [==============================] - 226s 581ms/step - categorical_accuracy: 0.9710 - student_loss: 0.5073 - distillation_loss: 6.7619e-05 - val_categorical_accuracy: 0.9561 - val_student_loss: 0.4356 - lr: 0.0010\n",
            "Epoch 10/40\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.9723 - student_loss: 0.3989 - distillation_loss: 6.2151e-05WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: categorical_accuracy,student_loss,distillation_loss,val_categorical_accuracy,val_student_loss,lr\n",
            "390/390 [==============================] - 226s 581ms/step - categorical_accuracy: 0.9723 - student_loss: 0.3988 - distillation_loss: 6.2150e-05 - val_categorical_accuracy: 0.9547 - val_student_loss: 0.3447 - lr: 0.0010\n",
            "Epoch 11/40\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.9744 - student_loss: 0.3150 - distillation_loss: 5.7528e-05WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: categorical_accuracy,student_loss,distillation_loss,val_categorical_accuracy,val_student_loss,lr\n",
            "390/390 [==============================] - 226s 581ms/step - categorical_accuracy: 0.9744 - student_loss: 0.3149 - distillation_loss: 5.7517e-05 - val_categorical_accuracy: 0.9571 - val_student_loss: 0.2650 - lr: 0.0010\n",
            "Epoch 12/40\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.9741 - student_loss: 0.2578 - distillation_loss: 5.3176e-05WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: categorical_accuracy,student_loss,distillation_loss,val_categorical_accuracy,val_student_loss,lr\n",
            "390/390 [==============================] - 226s 581ms/step - categorical_accuracy: 0.9741 - student_loss: 0.2578 - distillation_loss: 5.3175e-05 - val_categorical_accuracy: 0.9555 - val_student_loss: 0.1203 - lr: 0.0010\n",
            "Epoch 13/40\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.9750 - student_loss: 0.2149 - distillation_loss: 4.9415e-05WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: categorical_accuracy,student_loss,distillation_loss,val_categorical_accuracy,val_student_loss,lr\n",
            "390/390 [==============================] - 226s 581ms/step - categorical_accuracy: 0.9750 - student_loss: 0.2148 - distillation_loss: 4.9412e-05 - val_categorical_accuracy: 0.9542 - val_student_loss: 0.2936 - lr: 0.0010\n",
            "Epoch 14/40\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.9762 - student_loss: 0.1824 - distillation_loss: 4.6291e-05WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: categorical_accuracy,student_loss,distillation_loss,val_categorical_accuracy,val_student_loss,lr\n",
            "390/390 [==============================] - 226s 580ms/step - categorical_accuracy: 0.9762 - student_loss: 0.1824 - distillation_loss: 4.6290e-05 - val_categorical_accuracy: 0.9566 - val_student_loss: 0.1554 - lr: 0.0010\n",
            "Epoch 15/40\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.9779 - student_loss: 0.1569 - distillation_loss: 4.3485e-05WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: categorical_accuracy,student_loss,distillation_loss,val_categorical_accuracy,val_student_loss,lr\n",
            "390/390 [==============================] - 226s 581ms/step - categorical_accuracy: 0.9779 - student_loss: 0.1568 - distillation_loss: 4.3480e-05 - val_categorical_accuracy: 0.9550 - val_student_loss: 0.1647 - lr: 0.0010\n",
            "Epoch 16/40\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.9785 - student_loss: 0.1358 - distillation_loss: 4.1209e-05WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: categorical_accuracy,student_loss,distillation_loss,val_categorical_accuracy,val_student_loss,lr\n",
            "390/390 [==============================] - 226s 580ms/step - categorical_accuracy: 0.9785 - student_loss: 0.1358 - distillation_loss: 4.1204e-05 - val_categorical_accuracy: 0.9543 - val_student_loss: 0.0685 - lr: 0.0010\n",
            "Epoch 17/40\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.9777 - student_loss: 0.1238 - distillation_loss: 3.9050e-05WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: categorical_accuracy,student_loss,distillation_loss,val_categorical_accuracy,val_student_loss,lr\n",
            "390/390 [==============================] - 226s 580ms/step - categorical_accuracy: 0.9777 - student_loss: 0.1237 - distillation_loss: 3.9042e-05 - val_categorical_accuracy: 0.9558 - val_student_loss: 0.0750 - lr: 0.0010\n",
            "Epoch 18/40\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.9786 - student_loss: 0.1132 - distillation_loss: 3.7171e-05WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: categorical_accuracy,student_loss,distillation_loss,val_categorical_accuracy,val_student_loss,lr\n",
            "390/390 [==============================] - 226s 581ms/step - categorical_accuracy: 0.9786 - student_loss: 0.1132 - distillation_loss: 3.7169e-05 - val_categorical_accuracy: 0.9585 - val_student_loss: 0.0405 - lr: 0.0010\n",
            "Epoch 19/40\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.9786 - student_loss: 0.1045 - distillation_loss: 3.5643e-05WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: categorical_accuracy,student_loss,distillation_loss,val_categorical_accuracy,val_student_loss,lr\n",
            "390/390 [==============================] - 227s 581ms/step - categorical_accuracy: 0.9786 - student_loss: 0.1044 - distillation_loss: 3.5648e-05 - val_categorical_accuracy: 0.9561 - val_student_loss: 0.2820 - lr: 0.0010\n",
            "Epoch 20/40\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.9772 - student_loss: 0.1030 - distillation_loss: 3.4215e-05WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: categorical_accuracy,student_loss,distillation_loss,val_categorical_accuracy,val_student_loss,lr\n",
            "390/390 [==============================] - 226s 581ms/step - categorical_accuracy: 0.9772 - student_loss: 0.1029 - distillation_loss: 3.4216e-05 - val_categorical_accuracy: 0.9568 - val_student_loss: 0.1433 - lr: 0.0010\n",
            "Epoch 21/40\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.9786 - student_loss: 0.0923 - distillation_loss: 3.2979e-05WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: categorical_accuracy,student_loss,distillation_loss,val_categorical_accuracy,val_student_loss,lr\n",
            "390/390 [==============================] - 226s 581ms/step - categorical_accuracy: 0.9786 - student_loss: 0.0924 - distillation_loss: 3.2981e-05 - val_categorical_accuracy: 0.9563 - val_student_loss: 0.0302 - lr: 0.0010\n",
            "Epoch 22/40\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.9801 - student_loss: 0.0870 - distillation_loss: 3.2138e-05WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: categorical_accuracy,student_loss,distillation_loss,val_categorical_accuracy,val_student_loss,lr\n",
            "390/390 [==============================] - 227s 581ms/step - categorical_accuracy: 0.9801 - student_loss: 0.0871 - distillation_loss: 3.2141e-05 - val_categorical_accuracy: 0.9566 - val_student_loss: 0.0204 - lr: 0.0010\n",
            "Epoch 23/40\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.9796 - student_loss: 0.0839 - distillation_loss: 3.1263e-05WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: categorical_accuracy,student_loss,distillation_loss,val_categorical_accuracy,val_student_loss,lr\n",
            "390/390 [==============================] - 227s 581ms/step - categorical_accuracy: 0.9796 - student_loss: 0.0842 - distillation_loss: 3.1260e-05 - val_categorical_accuracy: 0.9556 - val_student_loss: 0.0727 - lr: 0.0010\n",
            "Epoch 24/40\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.9804 - student_loss: 0.0797 - distillation_loss: 3.0699e-05WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: categorical_accuracy,student_loss,distillation_loss,val_categorical_accuracy,val_student_loss,lr\n",
            "390/390 [==============================] - 228s 585ms/step - categorical_accuracy: 0.9804 - student_loss: 0.0798 - distillation_loss: 3.0699e-05 - val_categorical_accuracy: 0.9562 - val_student_loss: 0.1192 - lr: 0.0010\n",
            "Epoch 25/40\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.9799 - student_loss: 0.0777 - distillation_loss: 3.0097e-05WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: categorical_accuracy,student_loss,distillation_loss,val_categorical_accuracy,val_student_loss,lr\n",
            "390/390 [==============================] - 232s 594ms/step - categorical_accuracy: 0.9799 - student_loss: 0.0777 - distillation_loss: 3.0095e-05 - val_categorical_accuracy: 0.9554 - val_student_loss: 0.1354 - lr: 0.0010\n",
            "Epoch 26/40\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.9814 - student_loss: 0.0733 - distillation_loss: 2.9679e-05WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: categorical_accuracy,student_loss,distillation_loss,val_categorical_accuracy,val_student_loss,lr\n",
            "390/390 [==============================] - 229s 586ms/step - categorical_accuracy: 0.9814 - student_loss: 0.0733 - distillation_loss: 2.9680e-05 - val_categorical_accuracy: 0.9540 - val_student_loss: 0.0406 - lr: 0.0010\n",
            "Epoch 27/40\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.9804 - student_loss: 0.0756 - distillation_loss: 2.8991e-05WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: categorical_accuracy,student_loss,distillation_loss,val_categorical_accuracy,val_student_loss,lr\n",
            "390/390 [==============================] - 228s 584ms/step - categorical_accuracy: 0.9804 - student_loss: 0.0757 - distillation_loss: 2.8990e-05 - val_categorical_accuracy: 0.9545 - val_student_loss: 0.0982 - lr: 0.0010\n",
            "Epoch 28/40\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.9806 - student_loss: 0.0710 - distillation_loss: 2.8891e-05WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: categorical_accuracy,student_loss,distillation_loss,val_categorical_accuracy,val_student_loss,lr\n",
            "390/390 [==============================] - 226s 580ms/step - categorical_accuracy: 0.9806 - student_loss: 0.0709 - distillation_loss: 2.8888e-05 - val_categorical_accuracy: 0.9573 - val_student_loss: 0.0610 - lr: 0.0010\n",
            "Epoch 29/40\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.9796 - student_loss: 0.0730 - distillation_loss: 2.8779e-05WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: categorical_accuracy,student_loss,distillation_loss,val_categorical_accuracy,val_student_loss,lr\n",
            "390/390 [==============================] - 226s 580ms/step - categorical_accuracy: 0.9796 - student_loss: 0.0732 - distillation_loss: 2.8772e-05 - val_categorical_accuracy: 0.9559 - val_student_loss: 0.0595 - lr: 0.0010\n",
            "Epoch 30/40\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.9805 - student_loss: 0.0690 - distillation_loss: 2.8991e-05WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: categorical_accuracy,student_loss,distillation_loss,val_categorical_accuracy,val_student_loss,lr\n",
            "390/390 [==============================] - 227s 581ms/step - categorical_accuracy: 0.9805 - student_loss: 0.0690 - distillation_loss: 2.8988e-05 - val_categorical_accuracy: 0.9572 - val_student_loss: 0.2567 - lr: 0.0010\n",
            "Epoch 31/40\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.9800 - student_loss: 0.0696 - distillation_loss: 2.8357e-05WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: categorical_accuracy,student_loss,distillation_loss,val_categorical_accuracy,val_student_loss,lr\n",
            "390/390 [==============================] - 226s 581ms/step - categorical_accuracy: 0.9800 - student_loss: 0.0697 - distillation_loss: 2.8363e-05 - val_categorical_accuracy: 0.9560 - val_student_loss: 0.3283 - lr: 0.0010\n",
            "Epoch 32/40\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.9811 - student_loss: 0.0686 - distillation_loss: 2.8831e-05WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: categorical_accuracy,student_loss,distillation_loss,val_categorical_accuracy,val_student_loss,lr\n",
            "390/390 [==============================] - 226s 580ms/step - categorical_accuracy: 0.9811 - student_loss: 0.0686 - distillation_loss: 2.8831e-05 - val_categorical_accuracy: 0.9527 - val_student_loss: 0.1208 - lr: 0.0010\n",
            "Epoch 33/40\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.9815 - student_loss: 0.0652 - distillation_loss: 2.8690e-05WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: categorical_accuracy,student_loss,distillation_loss,val_categorical_accuracy,val_student_loss,lr\n",
            "390/390 [==============================] - 227s 583ms/step - categorical_accuracy: 0.9815 - student_loss: 0.0651 - distillation_loss: 2.8688e-05 - val_categorical_accuracy: 0.9584 - val_student_loss: 0.1579 - lr: 0.0010\n",
            "Epoch 34/40\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.9806 - student_loss: 0.0681 - distillation_loss: 2.8800e-05WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: categorical_accuracy,student_loss,distillation_loss,val_categorical_accuracy,val_student_loss,lr\n",
            "390/390 [==============================] - 229s 586ms/step - categorical_accuracy: 0.9806 - student_loss: 0.0681 - distillation_loss: 2.8800e-05 - val_categorical_accuracy: 0.9561 - val_student_loss: 0.0957 - lr: 0.0010\n",
            "Epoch 35/40\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.9811 - student_loss: 0.0634 - distillation_loss: 2.9098e-05WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: categorical_accuracy,student_loss,distillation_loss,val_categorical_accuracy,val_student_loss,lr\n",
            "390/390 [==============================] - 228s 584ms/step - categorical_accuracy: 0.9811 - student_loss: 0.0634 - distillation_loss: 2.9105e-05 - val_categorical_accuracy: 0.9585 - val_student_loss: 0.0293 - lr: 0.0010\n",
            "Epoch 36/40\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.9810 - student_loss: 0.0659 - distillation_loss: 2.9299e-05WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: categorical_accuracy,student_loss,distillation_loss,val_categorical_accuracy,val_student_loss,lr\n",
            "390/390 [==============================] - 227s 581ms/step - categorical_accuracy: 0.9810 - student_loss: 0.0660 - distillation_loss: 2.9298e-05 - val_categorical_accuracy: 0.9557 - val_student_loss: 0.1971 - lr: 0.0010\n",
            "Epoch 37/40\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.9812 - student_loss: 0.0648 - distillation_loss: 2.9375e-05WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: categorical_accuracy,student_loss,distillation_loss,val_categorical_accuracy,val_student_loss,lr\n",
            "390/390 [==============================] - 226s 581ms/step - categorical_accuracy: 0.9812 - student_loss: 0.0648 - distillation_loss: 2.9382e-05 - val_categorical_accuracy: 0.9575 - val_student_loss: 0.2143 - lr: 0.0010\n",
            "Epoch 38/40\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.9809 - student_loss: 0.0641 - distillation_loss: 2.9960e-05WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: categorical_accuracy,student_loss,distillation_loss,val_categorical_accuracy,val_student_loss,lr\n",
            "390/390 [==============================] - 226s 581ms/step - categorical_accuracy: 0.9809 - student_loss: 0.0642 - distillation_loss: 2.9964e-05 - val_categorical_accuracy: 0.9608 - val_student_loss: 0.0863 - lr: 0.0010\n",
            "Epoch 39/40\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.9814 - student_loss: 0.0631 - distillation_loss: 3.0049e-05WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: categorical_accuracy,student_loss,distillation_loss,val_categorical_accuracy,val_student_loss,lr\n",
            "390/390 [==============================] - 226s 581ms/step - categorical_accuracy: 0.9814 - student_loss: 0.0630 - distillation_loss: 3.0055e-05 - val_categorical_accuracy: 0.9584 - val_student_loss: 0.0498 - lr: 0.0010\n",
            "Epoch 40/40\n",
            "390/390 [==============================] - ETA: 0s - categorical_accuracy: 0.9826 - student_loss: 0.0582 - distillation_loss: 3.0070e-05WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: categorical_accuracy,student_loss,distillation_loss,val_categorical_accuracy,val_student_loss,lr\n",
            "390/390 [==============================] - 226s 581ms/step - categorical_accuracy: 0.9826 - student_loss: 0.0582 - distillation_loss: 3.0077e-05 - val_categorical_accuracy: 0.9578 - val_student_loss: 0.1991 - lr: 0.0010\n",
            "313/313 [==============================] - 14s 44ms/step - categorical_accuracy: 0.9578 - student_loss: 0.1537\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9577999711036682"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "l6ftcZ7WoKSS"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def make_prediction(model , image , true_value):\n",
        "  true_label_index = -1\n",
        "  for i in range(len(true_value)):\n",
        "\n",
        "    if(true_value[i]==1):\n",
        "      true_label_index = i\n",
        "      break\n",
        "\n",
        "  prediction = model.predict(image)[0]\n",
        "  probability = float('-inf')\n",
        "  predicted_label_index = -1\n",
        "\n",
        "  for i in range(len(prediction)):\n",
        "\n",
        "    if(prediction[i]>probability):\n",
        "      probability = prediction[i]\n",
        "      predicted_label_index = i\n",
        "\n",
        "  if(true_label_index!=predicted_label_index):\n",
        "    return 1\n",
        "  return 0\n",
        "\n"
      ],
      "metadata": {
        "id": "HPpOPmwZoKMp"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras,os\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential,Model\n",
        "from keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D , Dropout\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from keras.initializers import glorot_uniform\n",
        "from keras import applications\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50 , preprocess_input\n",
        "from keras import regularizers\n",
        "from absl import app, flags\n",
        "\n",
        "# Install bleeding edge version of cleverhans\n",
        "!pip install git+https://github.com/tensorflow/cleverhans.git#egg=cleverhans\n",
        "from cleverhans.tf2.attacks.projected_gradient_descent import projected_gradient_descent\n",
        "from cleverhans.tf2.attacks.fast_gradient_method import fast_gradient_method"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3phCvWqUjWD",
        "outputId": "15c32bc6-2487-4cc8-f115-ea39831497ef"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: cleverhans from git+https://github.com/tensorflow/cleverhans.git#egg=cleverhans in ./.local/lib/python3.6/site-packages (4.0.0)\n",
            "Requirement already satisfied: mnist in ./.local/lib/python3.6/site-packages (from cleverhans) (0.2.2)\n",
            "Requirement already satisfied: pycodestyle in ./.local/lib/python3.6/site-packages (from cleverhans) (2.8.0)\n",
            "Requirement already satisfied: easydict in ./.local/lib/python3.6/site-packages (from cleverhans) (1.9)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from cleverhans) (3.3.1)\n",
            "Requirement already satisfied: joblib in ./.local/lib/python3.6/site-packages (from cleverhans) (1.1.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from cleverhans) (1.4.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from cleverhans) (0.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from cleverhans) (1.19.1)\n",
            "Requirement already satisfied: nose in ./.local/lib/python3.6/site-packages (from cleverhans) (1.3.7)\n",
            "Requirement already satisfied: tensorflow-probability in ./.local/lib/python3.6/site-packages (from cleverhans) (0.14.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cleverhans) (1.15.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.6/dist-packages (from matplotlib->cleverhans) (7.2.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.6/dist-packages (from matplotlib->cleverhans) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2020.06.20 in /usr/local/lib/python3.6/dist-packages (from matplotlib->cleverhans) (2020.6.20)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->cleverhans) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->cleverhans) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->cleverhans) (2.8.1)\n",
            "Requirement already satisfied: decorator in ./.local/lib/python3.6/site-packages (from tensorflow-probability->cleverhans) (4.4.2)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in ./.local/lib/python3.6/site-packages (from tensorflow-probability->cleverhans) (2.0.0)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability->cleverhans) (0.3.3)\n",
            "Requirement already satisfied: dm-tree in ./.local/lib/python3.6/site-packages (from tensorflow-probability->cleverhans) (0.1.6)\n",
            "\u001b[33mWARNING: You are using pip version 20.2.2; however, version 21.3.1 is available.\n",
            "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import img_to_array\n",
        "\n",
        "epsilon_end = 0.8\n",
        "#logits_model = tf.keras.Model(model.input,model.layers[-1].output)\n",
        "total_images = 200\n",
        "\n",
        "result={}\n",
        "epsilon = 0.1\n",
        "while(epsilon <= epsilon_end):\n",
        "\n",
        "  fgsm_counter = 0\n",
        "  pgd_counter = 0\n",
        "  print(\"Epsilon value - \",epsilon)\n",
        "  print()\n",
        "  for image_index in range(total_images):\n",
        "  \n",
        "  \n",
        "    image = x_test[image_index]\n",
        "    image = img_to_array(image)\n",
        "    image = image.reshape(1, 32, 32, 3)\n",
        "    true_value = y_test[image_index]\n",
        "    original_prediction = make_prediction(distiller,image,true_value)\n",
        "    fgsm_sample = fast_gradient_method(distiller, image, epsilon, np.inf, targeted=False)\n",
        "    pgd_sample = projected_gradient_descent(distiller, image, epsilon, 0.01, 40, np.inf)\n",
        "    print(epsilon,' ',image_index)\n",
        "    fgsm_prediction = make_prediction(distiller , fgsm_sample , true_value)\n",
        "    pgd_prediction = make_prediction(distiller , pgd_sample , true_value)\n",
        "    pgd_counter+=pgd_prediction\n",
        "    fgsm_counter+=fgsm_prediction\n",
        "    print(pgd_counter,' ',fgsm_counter)\n",
        "  result[epsilon] = (fgsm_counter , pgd_counter)\n",
        "  epsilon+=0.1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LjVpMPnXURRO",
        "outputId": "6c0791b1-6334-4281-a4f3-01396a27307b"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epsilon value -  0.1\n",
            "\n",
            "0.1   0\n",
            "0   0\n",
            "0.1   1\n",
            "0   0\n",
            "0.1   2\n",
            "0   0\n",
            "0.1   3\n",
            "0   0\n",
            "0.1   4\n",
            "0   0\n",
            "0.1   5\n",
            "0   0\n",
            "0.1   6\n",
            "0   0\n",
            "0.1   7\n",
            "0   0\n",
            "0.1   8\n",
            "0   0\n",
            "0.1   9\n",
            "0   0\n",
            "0.1   10\n",
            "0   0\n",
            "0.1   11\n",
            "0   0\n",
            "0.1   12\n",
            "0   0\n",
            "0.1   13\n",
            "0   0\n",
            "0.1   14\n",
            "0   0\n",
            "0.1   15\n",
            "0   0\n",
            "0.1   16\n",
            "0   0\n",
            "0.1   17\n",
            "0   0\n",
            "0.1   18\n",
            "0   0\n",
            "0.1   19\n",
            "0   0\n",
            "0.1   20\n",
            "0   0\n",
            "0.1   21\n",
            "0   0\n",
            "0.1   22\n",
            "0   0\n",
            "0.1   23\n",
            "0   0\n",
            "0.1   24\n",
            "0   0\n",
            "0.1   25\n",
            "0   0\n",
            "0.1   26\n",
            "0   0\n",
            "0.1   27\n",
            "0   0\n",
            "0.1   28\n",
            "0   0\n",
            "0.1   29\n",
            "0   0\n",
            "0.1   30\n",
            "0   0\n",
            "0.1   31\n",
            "0   0\n",
            "0.1   32\n",
            "0   0\n",
            "0.1   33\n",
            "0   0\n",
            "0.1   34\n",
            "0   0\n",
            "0.1   35\n",
            "0   0\n",
            "0.1   36\n",
            "0   0\n",
            "0.1   37\n",
            "0   0\n",
            "0.1   38\n",
            "0   0\n",
            "0.1   39\n",
            "0   0\n",
            "0.1   40\n",
            "0   0\n",
            "0.1   41\n",
            "0   0\n",
            "0.1   42\n",
            "0   0\n",
            "0.1   43\n",
            "0   0\n",
            "0.1   44\n",
            "0   0\n",
            "0.1   45\n",
            "0   0\n",
            "0.1   46\n",
            "0   0\n",
            "0.1   47\n",
            "0   0\n",
            "0.1   48\n",
            "0   0\n",
            "0.1   49\n",
            "0   0\n",
            "0.1   50\n",
            "0   0\n",
            "0.1   51\n",
            "0   0\n",
            "0.1   52\n",
            "0   0\n",
            "0.1   53\n",
            "0   0\n",
            "0.1   54\n",
            "0   0\n",
            "0.1   55\n",
            "0   0\n",
            "0.1   56\n",
            "0   0\n",
            "0.1   57\n",
            "1   1\n",
            "0.1   58\n",
            "1   1\n",
            "0.1   59\n",
            "2   2\n",
            "0.1   60\n",
            "2   2\n",
            "0.1   61\n",
            "2   3\n",
            "0.1   62\n",
            "2   3\n",
            "0.1   63\n",
            "2   3\n",
            "0.1   64\n",
            "2   3\n",
            "0.1   65\n",
            "2   3\n",
            "0.1   66\n",
            "2   3\n",
            "0.1   67\n",
            "2   3\n",
            "0.1   68\n",
            "3   4\n",
            "0.1   69\n",
            "3   4\n",
            "0.1   70\n",
            "3   4\n",
            "0.1   71\n",
            "3   4\n",
            "0.1   72\n",
            "3   4\n",
            "0.1   73\n",
            "3   4\n",
            "0.1   74\n",
            "3   4\n",
            "0.1   75\n",
            "3   4\n",
            "0.1   76\n",
            "3   4\n",
            "0.1   77\n",
            "3   4\n",
            "0.1   78\n",
            "3   4\n",
            "0.1   79\n",
            "3   4\n",
            "0.1   80\n",
            "3   4\n",
            "0.1   81\n",
            "3   4\n",
            "0.1   82\n",
            "3   4\n",
            "0.1   83\n",
            "3   4\n",
            "0.1   84\n",
            "3   4\n",
            "0.1   85\n",
            "3   4\n",
            "0.1   86\n",
            "4   5\n",
            "0.1   87\n",
            "5   5\n",
            "0.1   88\n",
            "5   5\n",
            "0.1   89\n",
            "5   5\n",
            "0.1   90\n",
            "5   5\n",
            "0.1   91\n",
            "5   5\n",
            "0.1   92\n",
            "5   5\n",
            "0.1   93\n",
            "5   5\n",
            "0.1   94\n",
            "5   5\n",
            "0.1   95\n",
            "5   5\n",
            "0.1   96\n",
            "5   5\n",
            "0.1   97\n",
            "6   5\n",
            "0.1   98\n",
            "6   5\n",
            "0.1   99\n",
            "6   5\n",
            "0.1   100\n",
            "6   5\n",
            "0.1   101\n",
            "6   5\n",
            "0.1   102\n",
            "6   5\n",
            "0.1   103\n",
            "6   5\n",
            "0.1   104\n",
            "6   5\n",
            "0.1   105\n",
            "6   5\n",
            "0.1   106\n",
            "6   5\n",
            "0.1   107\n",
            "6   5\n",
            "0.1   108\n",
            "6   5\n",
            "0.1   109\n",
            "6   5\n",
            "0.1   110\n",
            "6   5\n",
            "0.1   111\n",
            "6   5\n",
            "0.1   112\n",
            "6   5\n",
            "0.1   113\n",
            "6   5\n",
            "0.1   114\n",
            "6   5\n",
            "0.1   115\n",
            "6   5\n",
            "0.1   116\n",
            "6   5\n",
            "0.1   117\n",
            "6   5\n",
            "0.1   118\n",
            "7   6\n",
            "0.1   119\n",
            "7   6\n",
            "0.1   120\n",
            "7   6\n",
            "0.1   121\n",
            "8   6\n",
            "0.1   122\n",
            "8   6\n",
            "0.1   123\n",
            "8   6\n",
            "0.1   124\n",
            "8   6\n",
            "0.1   125\n",
            "9   7\n",
            "0.1   126\n",
            "9   7\n",
            "0.1   127\n",
            "9   7\n",
            "0.1   128\n",
            "9   7\n",
            "0.1   129\n",
            "9   7\n",
            "0.1   130\n",
            "9   7\n",
            "0.1   131\n",
            "9   7\n",
            "0.1   132\n",
            "9   7\n",
            "0.1   133\n",
            "9   7\n",
            "0.1   134\n",
            "9   7\n",
            "0.1   135\n",
            "9   7\n",
            "0.1   136\n",
            "9   7\n",
            "0.1   137\n",
            "9   7\n",
            "0.1   138\n",
            "9   7\n",
            "0.1   139\n",
            "10   8\n",
            "0.1   140\n",
            "10   8\n",
            "0.1   141\n",
            "10   8\n",
            "0.1   142\n",
            "10   8\n",
            "0.1   143\n",
            "10   8\n",
            "0.1   144\n",
            "10   8\n",
            "0.1   145\n",
            "10   8\n",
            "0.1   146\n",
            "10   8\n",
            "0.1   147\n",
            "11   9\n",
            "0.1   148\n",
            "12   10\n",
            "0.1   149\n",
            "13   10\n",
            "0.1   150\n",
            "13   10\n",
            "0.1   151\n",
            "13   10\n",
            "0.1   152\n",
            "13   10\n",
            "0.1   153\n",
            "13   10\n",
            "0.1   154\n",
            "13   10\n",
            "0.1   155\n",
            "13   10\n",
            "0.1   156\n",
            "14   11\n",
            "0.1   157\n",
            "14   11\n",
            "0.1   158\n",
            "14   11\n",
            "0.1   159\n",
            "14   11\n",
            "0.1   160\n",
            "15   12\n",
            "0.1   161\n",
            "15   12\n",
            "0.1   162\n",
            "16   12\n",
            "0.1   163\n",
            "16   12\n",
            "0.1   164\n",
            "17   13\n",
            "0.1   165\n",
            "17   13\n",
            "0.1   166\n",
            "17   13\n",
            "0.1   167\n",
            "17   13\n",
            "0.1   168\n",
            "17   13\n",
            "0.1   169\n",
            "17   13\n",
            "0.1   170\n",
            "17   13\n",
            "0.1   171\n",
            "17   13\n",
            "0.1   172\n",
            "17   13\n",
            "0.1   173\n",
            "17   13\n",
            "0.1   174\n",
            "17   13\n",
            "0.1   175\n",
            "17   13\n",
            "0.1   176\n",
            "17   13\n",
            "0.1   177\n",
            "17   13\n",
            "0.1   178\n",
            "18   14\n",
            "0.1   179\n",
            "18   14\n",
            "0.1   180\n",
            "18   14\n",
            "0.1   181\n",
            "18   14\n",
            "0.1   182\n",
            "18   14\n",
            "0.1   183\n",
            "18   14\n",
            "0.1   184\n",
            "18   14\n",
            "0.1   185\n",
            "18   14\n",
            "0.1   186\n",
            "18   14\n",
            "0.1   187\n",
            "18   14\n",
            "0.1   188\n",
            "19   15\n",
            "0.1   189\n",
            "19   15\n",
            "0.1   190\n",
            "19   15\n",
            "0.1   191\n",
            "19   15\n",
            "0.1   192\n",
            "19   15\n",
            "0.1   193\n",
            "19   15\n",
            "0.1   194\n",
            "19   15\n",
            "0.1   195\n",
            "19   15\n",
            "0.1   196\n",
            "19   15\n",
            "0.1   197\n",
            "19   15\n",
            "0.1   198\n",
            "19   15\n",
            "0.1   199\n",
            "19   15\n",
            "Epsilon value -  0.2\n",
            "\n",
            "0.2   0\n",
            "0   0\n",
            "0.2   1\n",
            "0   0\n",
            "0.2   2\n",
            "0   0\n",
            "0.2   3\n",
            "1   0\n",
            "0.2   4\n",
            "1   0\n",
            "0.2   5\n",
            "1   0\n",
            "0.2   6\n",
            "1   0\n",
            "0.2   7\n",
            "1   0\n",
            "0.2   8\n",
            "1   0\n",
            "0.2   9\n",
            "1   0\n",
            "0.2   10\n",
            "2   0\n",
            "0.2   11\n",
            "2   0\n",
            "0.2   12\n",
            "2   0\n",
            "0.2   13\n",
            "2   0\n",
            "0.2   14\n",
            "2   0\n",
            "0.2   15\n",
            "2   0\n",
            "0.2   16\n",
            "2   0\n",
            "0.2   17\n",
            "2   0\n",
            "0.2   18\n",
            "2   0\n",
            "0.2   19\n",
            "2   0\n",
            "0.2   20\n",
            "3   1\n",
            "0.2   21\n",
            "3   1\n",
            "0.2   22\n",
            "3   1\n",
            "0.2   23\n",
            "3   1\n",
            "0.2   24\n",
            "3   1\n",
            "0.2   25\n",
            "4   2\n",
            "0.2   26\n",
            "5   2\n",
            "0.2   27\n",
            "5   2\n",
            "0.2   28\n",
            "5   2\n",
            "0.2   29\n",
            "5   2\n",
            "0.2   30\n",
            "5   2\n",
            "0.2   31\n",
            "5   2\n",
            "0.2   32\n",
            "6   3\n",
            "0.2   33\n",
            "6   3\n",
            "0.2   34\n",
            "6   3\n",
            "0.2   35\n",
            "6   3\n",
            "0.2   36\n",
            "6   3\n",
            "0.2   37\n",
            "6   3\n",
            "0.2   38\n",
            "6   3\n",
            "0.2   39\n",
            "6   3\n",
            "0.2   40\n",
            "6   3\n",
            "0.2   41\n",
            "6   3\n",
            "0.2   42\n",
            "6   3\n",
            "0.2   43\n",
            "6   3\n",
            "0.2   44\n",
            "6   3\n",
            "0.2   45\n",
            "6   3\n",
            "0.2   46\n",
            "6   3\n",
            "0.2   47\n",
            "6   3\n",
            "0.2   48\n",
            "6   3\n",
            "0.2   49\n",
            "6   3\n",
            "0.2   50\n",
            "6   3\n",
            "0.2   51\n",
            "6   3\n",
            "0.2   52\n",
            "7   3\n",
            "0.2   53\n",
            "7   3\n",
            "0.2   54\n",
            "7   3\n",
            "0.2   55\n",
            "7   3\n",
            "0.2   56\n",
            "7   3\n",
            "0.2   57\n",
            "8   4\n",
            "0.2   58\n",
            "8   4\n",
            "0.2   59\n",
            "9   5\n",
            "0.2   60\n",
            "9   5\n",
            "0.2   61\n",
            "9   5\n",
            "0.2   62\n",
            "9   5\n",
            "0.2   63\n",
            "10   5\n",
            "0.2   64\n",
            "10   5\n",
            "0.2   65\n",
            "10   5\n",
            "0.2   66\n",
            "10   5\n",
            "0.2   67\n",
            "10   5\n",
            "0.2   68\n",
            "11   6\n",
            "0.2   69\n",
            "11   6\n",
            "0.2   70\n",
            "12   6\n",
            "0.2   71\n",
            "13   6\n",
            "0.2   72\n",
            "13   6\n",
            "0.2   73\n",
            "13   6\n",
            "0.2   74\n",
            "13   6\n",
            "0.2   75\n",
            "13   6\n",
            "0.2   76\n",
            "13   6\n",
            "0.2   77\n",
            "13   6\n",
            "0.2   78\n",
            "13   6\n",
            "0.2   79\n",
            "13   6\n",
            "0.2   80\n",
            "13   6\n",
            "0.2   81\n",
            "13   6\n",
            "0.2   82\n",
            "13   6\n",
            "0.2   83\n",
            "13   6\n",
            "0.2   84\n",
            "13   6\n",
            "0.2   85\n",
            "13   6\n",
            "0.2   86\n",
            "14   7\n",
            "0.2   87\n",
            "15   8\n",
            "0.2   88\n",
            "15   8\n",
            "0.2   89\n",
            "15   8\n",
            "0.2   90\n",
            "15   8\n",
            "0.2   91\n",
            "15   8\n",
            "0.2   92\n",
            "15   8\n",
            "0.2   93\n",
            "15   8\n",
            "0.2   94\n",
            "15   8\n",
            "0.2   95\n",
            "15   8\n",
            "0.2   96\n",
            "15   8\n",
            "0.2   97\n",
            "16   8\n",
            "0.2   98\n",
            "16   8\n",
            "0.2   99\n",
            "16   8\n",
            "0.2   100\n",
            "16   8\n",
            "0.2   101\n",
            "16   8\n",
            "0.2   102\n",
            "16   8\n",
            "0.2   103\n",
            "16   8\n",
            "0.2   104\n",
            "16   8\n",
            "0.2   105\n",
            "16   8\n",
            "0.2   106\n",
            "16   8\n",
            "0.2   107\n",
            "16   8\n",
            "0.2   108\n",
            "16   8\n",
            "0.2   109\n",
            "16   8\n",
            "0.2   110\n",
            "16   8\n",
            "0.2   111\n",
            "16   8\n",
            "0.2   112\n",
            "16   8\n",
            "0.2   113\n",
            "16   8\n",
            "0.2   114\n",
            "16   8\n",
            "0.2   115\n",
            "16   8\n",
            "0.2   116\n",
            "16   8\n",
            "0.2   117\n",
            "16   8\n",
            "0.2   118\n",
            "17   9\n",
            "0.2   119\n",
            "18   9\n",
            "0.2   120\n",
            "18   9\n",
            "0.2   121\n",
            "19   10\n",
            "0.2   122\n",
            "19   10\n",
            "0.2   123\n",
            "19   10\n",
            "0.2   124\n",
            "19   10\n",
            "0.2   125\n",
            "20   11\n",
            "0.2   126\n",
            "20   11\n",
            "0.2   127\n",
            "21   12\n",
            "0.2   128\n",
            "21   12\n",
            "0.2   129\n",
            "21   12\n",
            "0.2   130\n",
            "21   12\n",
            "0.2   131\n",
            "21   12\n",
            "0.2   132\n",
            "21   12\n",
            "0.2   133\n",
            "21   12\n",
            "0.2   134\n",
            "21   12\n",
            "0.2   135\n",
            "21   12\n",
            "0.2   136\n",
            "21   12\n",
            "0.2   137\n",
            "21   12\n",
            "0.2   138\n",
            "21   12\n",
            "0.2   139\n",
            "22   13\n",
            "0.2   140\n",
            "23   14\n",
            "0.2   141\n",
            "23   14\n",
            "0.2   142\n",
            "23   14\n",
            "0.2   143\n",
            "24   15\n",
            "0.2   144\n",
            "24   15\n",
            "0.2   145\n",
            "24   15\n",
            "0.2   146\n",
            "24   15\n",
            "0.2   147\n",
            "25   16\n",
            "0.2   148\n",
            "26   17\n",
            "0.2   149\n",
            "27   18\n",
            "0.2   150\n",
            "27   18\n",
            "0.2   151\n",
            "27   18\n",
            "0.2   152\n",
            "27   18\n",
            "0.2   153\n",
            "27   18\n",
            "0.2   154\n",
            "27   18\n",
            "0.2   155\n",
            "27   18\n",
            "0.2   156\n",
            "28   19\n",
            "0.2   157\n",
            "28   19\n",
            "0.2   158\n",
            "28   19\n",
            "0.2   159\n",
            "28   19\n",
            "0.2   160\n",
            "29   20\n",
            "0.2   161\n",
            "29   20\n",
            "0.2   162\n",
            "30   21\n",
            "0.2   163\n",
            "30   21\n",
            "0.2   164\n",
            "31   22\n",
            "0.2   165\n",
            "32   22\n",
            "0.2   166\n",
            "32   22\n",
            "0.2   167\n",
            "32   22\n",
            "0.2   168\n",
            "32   22\n",
            "0.2   169\n",
            "33   23\n",
            "0.2   170\n",
            "33   23\n",
            "0.2   171\n",
            "34   23\n",
            "0.2   172\n",
            "34   23\n",
            "0.2   173\n",
            "35   23\n",
            "0.2   174\n",
            "35   23\n",
            "0.2   175\n",
            "35   23\n",
            "0.2   176\n",
            "35   23\n",
            "0.2   177\n",
            "35   23\n",
            "0.2   178\n",
            "35   23\n",
            "0.2   179\n",
            "35   23\n",
            "0.2   180\n",
            "36   24\n",
            "0.2   181\n",
            "36   24\n",
            "0.2   182\n",
            "36   24\n",
            "0.2   183\n",
            "37   24\n",
            "0.2   184\n",
            "37   24\n",
            "0.2   185\n",
            "37   24\n",
            "0.2   186\n",
            "37   24\n",
            "0.2   187\n",
            "37   24\n",
            "0.2   188\n",
            "38   25\n",
            "0.2   189\n",
            "39   26\n",
            "0.2   190\n",
            "39   26\n",
            "0.2   191\n",
            "39   26\n",
            "0.2   192\n",
            "40   27\n",
            "0.2   193\n",
            "40   27\n",
            "0.2   194\n",
            "40   27\n",
            "0.2   195\n",
            "41   28\n",
            "0.2   196\n",
            "41   28\n",
            "0.2   197\n",
            "41   28\n",
            "0.2   198\n",
            "41   28\n",
            "0.2   199\n",
            "41   28\n",
            "Epsilon value -  0.30000000000000004\n",
            "\n",
            "0.30000000000000004   0\n",
            "0   0\n",
            "0.30000000000000004   1\n",
            "0   0\n",
            "0.30000000000000004   2\n",
            "1   0\n",
            "0.30000000000000004   3\n",
            "2   0\n",
            "0.30000000000000004   4\n",
            "2   0\n",
            "0.30000000000000004   5\n",
            "3   0\n",
            "0.30000000000000004   6\n",
            "3   0\n",
            "0.30000000000000004   7\n",
            "3   0\n",
            "0.30000000000000004   8\n",
            "3   0\n",
            "0.30000000000000004   9\n",
            "3   0\n",
            "0.30000000000000004   10\n",
            "4   0\n",
            "0.30000000000000004   11\n",
            "5   0\n",
            "0.30000000000000004   12\n",
            "5   0\n",
            "0.30000000000000004   13\n",
            "5   0\n",
            "0.30000000000000004   14\n",
            "5   0\n",
            "0.30000000000000004   15\n",
            "5   0\n",
            "0.30000000000000004   16\n",
            "5   0\n",
            "0.30000000000000004   17\n",
            "5   0\n",
            "0.30000000000000004   18\n",
            "5   0\n",
            "0.30000000000000004   19\n",
            "5   0\n",
            "0.30000000000000004   20\n",
            "6   1\n",
            "0.30000000000000004   21\n",
            "6   1\n",
            "0.30000000000000004   22\n",
            "6   1\n",
            "0.30000000000000004   23\n",
            "6   1\n",
            "0.30000000000000004   24\n",
            "6   1\n",
            "0.30000000000000004   25\n",
            "7   2\n",
            "0.30000000000000004   26\n",
            "8   2\n",
            "0.30000000000000004   27\n",
            "9   2\n",
            "0.30000000000000004   28\n",
            "9   2\n",
            "0.30000000000000004   29\n",
            "9   2\n",
            "0.30000000000000004   30\n",
            "9   2\n",
            "0.30000000000000004   31\n",
            "9   2\n",
            "0.30000000000000004   32\n",
            "10   3\n",
            "0.30000000000000004   33\n",
            "11   3\n",
            "0.30000000000000004   34\n",
            "11   3\n",
            "0.30000000000000004   35\n",
            "11   3\n",
            "0.30000000000000004   36\n",
            "12   3\n",
            "0.30000000000000004   37\n",
            "12   3\n",
            "0.30000000000000004   38\n",
            "12   3\n",
            "0.30000000000000004   39\n",
            "12   3\n",
            "0.30000000000000004   40\n",
            "12   3\n",
            "0.30000000000000004   41\n",
            "12   3\n",
            "0.30000000000000004   42\n",
            "12   3\n",
            "0.30000000000000004   43\n",
            "12   3\n",
            "0.30000000000000004   44\n",
            "12   3\n",
            "0.30000000000000004   45\n",
            "12   3\n",
            "0.30000000000000004   46\n",
            "13   3\n",
            "0.30000000000000004   47\n",
            "13   3\n",
            "0.30000000000000004   48\n",
            "13   3\n",
            "0.30000000000000004   49\n",
            "14   3\n",
            "0.30000000000000004   50\n",
            "14   3\n",
            "0.30000000000000004   51\n",
            "14   3\n",
            "0.30000000000000004   52\n",
            "15   4\n",
            "0.30000000000000004   53\n",
            "16   5\n",
            "0.30000000000000004   54\n",
            "16   5\n",
            "0.30000000000000004   55\n",
            "16   5\n",
            "0.30000000000000004   56\n",
            "16   5\n",
            "0.30000000000000004   57\n",
            "17   6\n",
            "0.30000000000000004   58\n",
            "17   6\n",
            "0.30000000000000004   59\n",
            "18   7\n",
            "0.30000000000000004   60\n",
            "18   7\n",
            "0.30000000000000004   61\n",
            "18   7\n",
            "0.30000000000000004   62\n",
            "18   7\n",
            "0.30000000000000004   63\n",
            "19   8\n",
            "0.30000000000000004   64\n",
            "19   8\n",
            "0.30000000000000004   65\n",
            "19   8\n",
            "0.30000000000000004   66\n",
            "19   8\n",
            "0.30000000000000004   67\n",
            "19   8\n",
            "0.30000000000000004   68\n",
            "20   9\n",
            "0.30000000000000004   69\n",
            "20   9\n",
            "0.30000000000000004   70\n",
            "21   10\n",
            "0.30000000000000004   71\n",
            "22   11\n",
            "0.30000000000000004   72\n",
            "22   11\n",
            "0.30000000000000004   73\n",
            "22   11\n",
            "0.30000000000000004   74\n",
            "23   11\n",
            "0.30000000000000004   75\n",
            "23   11\n",
            "0.30000000000000004   76\n",
            "23   11\n",
            "0.30000000000000004   77\n",
            "24   11\n",
            "0.30000000000000004   78\n",
            "24   11\n",
            "0.30000000000000004   79\n",
            "24   11\n",
            "0.30000000000000004   80\n",
            "24   11\n",
            "0.30000000000000004   81\n",
            "25   11\n",
            "0.30000000000000004   82\n",
            "25   11\n",
            "0.30000000000000004   83\n",
            "25   11\n",
            "0.30000000000000004   84\n",
            "25   11\n",
            "0.30000000000000004   85\n",
            "25   11\n",
            "0.30000000000000004   86\n",
            "26   12\n",
            "0.30000000000000004   87\n",
            "27   13\n",
            "0.30000000000000004   88\n",
            "27   13\n",
            "0.30000000000000004   89\n",
            "27   13\n",
            "0.30000000000000004   90\n",
            "27   13\n",
            "0.30000000000000004   91\n",
            "27   13\n",
            "0.30000000000000004   92\n",
            "27   13\n",
            "0.30000000000000004   93\n",
            "27   13\n",
            "0.30000000000000004   94\n",
            "27   13\n",
            "0.30000000000000004   95\n",
            "28   14\n",
            "0.30000000000000004   96\n",
            "28   14\n",
            "0.30000000000000004   97\n",
            "29   15\n",
            "0.30000000000000004   98\n",
            "29   15\n",
            "0.30000000000000004   99\n",
            "29   15\n",
            "0.30000000000000004   100\n",
            "29   15\n",
            "0.30000000000000004   101\n",
            "29   15\n",
            "0.30000000000000004   102\n",
            "29   15\n",
            "0.30000000000000004   103\n",
            "29   15\n",
            "0.30000000000000004   104\n",
            "29   15\n",
            "0.30000000000000004   105\n",
            "29   15\n",
            "0.30000000000000004   106\n",
            "29   15\n",
            "0.30000000000000004   107\n",
            "29   15\n",
            "0.30000000000000004   108\n",
            "29   15\n",
            "0.30000000000000004   109\n",
            "29   15\n",
            "0.30000000000000004   110\n",
            "29   15\n",
            "0.30000000000000004   111\n",
            "29   15\n",
            "0.30000000000000004   112\n",
            "30   16\n",
            "0.30000000000000004   113\n",
            "30   16\n",
            "0.30000000000000004   114\n",
            "30   16\n",
            "0.30000000000000004   115\n",
            "30   16\n",
            "0.30000000000000004   116\n",
            "30   16\n",
            "0.30000000000000004   117\n",
            "31   16\n",
            "0.30000000000000004   118\n",
            "32   17\n",
            "0.30000000000000004   119\n",
            "33   18\n",
            "0.30000000000000004   120\n",
            "33   18\n",
            "0.30000000000000004   121\n",
            "34   19\n",
            "0.30000000000000004   122\n",
            "34   19\n",
            "0.30000000000000004   123\n",
            "34   19\n",
            "0.30000000000000004   124\n",
            "34   19\n",
            "0.30000000000000004   125\n",
            "35   20\n",
            "0.30000000000000004   126\n",
            "35   20\n",
            "0.30000000000000004   127\n",
            "36   21\n",
            "0.30000000000000004   128\n",
            "37   21\n",
            "0.30000000000000004   129\n",
            "38   21\n",
            "0.30000000000000004   130\n",
            "38   21\n",
            "0.30000000000000004   131\n",
            "38   21\n",
            "0.30000000000000004   132\n",
            "38   21\n",
            "0.30000000000000004   133\n",
            "38   21\n",
            "0.30000000000000004   134\n",
            "39   21\n",
            "0.30000000000000004   135\n",
            "39   21\n",
            "0.30000000000000004   136\n",
            "39   21\n",
            "0.30000000000000004   137\n",
            "39   21\n",
            "0.30000000000000004   138\n",
            "39   21\n",
            "0.30000000000000004   139\n",
            "40   22\n",
            "0.30000000000000004   140\n",
            "41   23\n",
            "0.30000000000000004   141\n",
            "41   23\n",
            "0.30000000000000004   142\n",
            "42   23\n",
            "0.30000000000000004   143\n",
            "43   24\n",
            "0.30000000000000004   144\n",
            "43   24\n",
            "0.30000000000000004   145\n",
            "44   24\n",
            "0.30000000000000004   146\n",
            "44   24\n",
            "0.30000000000000004   147\n",
            "45   25\n",
            "0.30000000000000004   148\n",
            "46   26\n",
            "0.30000000000000004   149\n",
            "47   27\n",
            "0.30000000000000004   150\n",
            "47   27\n",
            "0.30000000000000004   151\n",
            "48   28\n",
            "0.30000000000000004   152\n",
            "48   28\n",
            "0.30000000000000004   153\n",
            "48   28\n",
            "0.30000000000000004   154\n",
            "48   28\n",
            "0.30000000000000004   155\n",
            "48   28\n",
            "0.30000000000000004   156\n",
            "49   29\n",
            "0.30000000000000004   157\n",
            "49   29\n",
            "0.30000000000000004   158\n",
            "49   29\n",
            "0.30000000000000004   159\n",
            "49   29\n",
            "0.30000000000000004   160\n",
            "50   30\n",
            "0.30000000000000004   161\n",
            "50   30\n",
            "0.30000000000000004   162\n",
            "51   31\n",
            "0.30000000000000004   163\n",
            "51   31\n",
            "0.30000000000000004   164\n",
            "52   32\n",
            "0.30000000000000004   165\n",
            "53   33\n",
            "0.30000000000000004   166\n",
            "53   33\n",
            "0.30000000000000004   167\n",
            "54   33\n",
            "0.30000000000000004   168\n",
            "54   33\n",
            "0.30000000000000004   169\n",
            "55   34\n",
            "0.30000000000000004   170\n",
            "55   34\n",
            "0.30000000000000004   171\n",
            "56   35\n",
            "0.30000000000000004   172\n",
            "56   35\n",
            "0.30000000000000004   173\n",
            "57   35\n",
            "0.30000000000000004   174\n",
            "57   35\n",
            "0.30000000000000004   175\n",
            "57   35\n",
            "0.30000000000000004   176\n",
            "57   35\n",
            "0.30000000000000004   177\n",
            "57   35\n",
            "0.30000000000000004   178\n",
            "57   35\n",
            "0.30000000000000004   179\n",
            "57   35\n",
            "0.30000000000000004   180\n",
            "58   36\n",
            "0.30000000000000004   181\n",
            "58   36\n",
            "0.30000000000000004   182\n",
            "58   36\n",
            "0.30000000000000004   183\n",
            "59   37\n",
            "0.30000000000000004   184\n",
            "59   37\n",
            "0.30000000000000004   185\n",
            "59   37\n",
            "0.30000000000000004   186\n",
            "59   37\n",
            "0.30000000000000004   187\n",
            "60   37\n",
            "0.30000000000000004   188\n",
            "61   38\n",
            "0.30000000000000004   189\n",
            "62   39\n",
            "0.30000000000000004   190\n",
            "62   39\n",
            "0.30000000000000004   191\n",
            "62   39\n",
            "0.30000000000000004   192\n",
            "63   40\n",
            "0.30000000000000004   193\n",
            "63   40\n",
            "0.30000000000000004   194\n",
            "63   40\n",
            "0.30000000000000004   195\n",
            "64   41\n",
            "0.30000000000000004   196\n",
            "64   41\n",
            "0.30000000000000004   197\n",
            "64   41\n",
            "0.30000000000000004   198\n",
            "64   41\n",
            "0.30000000000000004   199\n",
            "64   41\n",
            "Epsilon value -  0.4\n",
            "\n",
            "0.4   0\n",
            "0   0\n",
            "0.4   1\n",
            "0   0\n",
            "0.4   2\n",
            "1   1\n",
            "0.4   3\n",
            "2   1\n",
            "0.4   4\n",
            "2   1\n",
            "0.4   5\n",
            "3   2\n",
            "0.4   6\n",
            "4   2\n",
            "0.4   7\n",
            "4   2\n",
            "0.4   8\n",
            "4   2\n",
            "0.4   9\n",
            "4   2\n",
            "0.4   10\n",
            "5   2\n",
            "0.4   11\n",
            "6   3\n",
            "0.4   12\n",
            "6   3\n",
            "0.4   13\n",
            "6   3\n",
            "0.4   14\n",
            "6   3\n",
            "0.4   15\n",
            "6   3\n",
            "0.4   16\n",
            "6   3\n",
            "0.4   17\n",
            "6   3\n",
            "0.4   18\n",
            "6   3\n",
            "0.4   19\n",
            "6   3\n",
            "0.4   20\n",
            "7   4\n",
            "0.4   21\n",
            "7   4\n",
            "0.4   22\n",
            "7   4\n",
            "0.4   23\n",
            "7   4\n",
            "0.4   24\n",
            "7   4\n",
            "0.4   25\n",
            "8   5\n",
            "0.4   26\n",
            "9   6\n",
            "0.4   27\n",
            "10   6\n",
            "0.4   28\n",
            "10   6\n",
            "0.4   29\n",
            "10   6\n",
            "0.4   30\n",
            "10   6\n",
            "0.4   31\n",
            "10   6\n",
            "0.4   32\n",
            "11   7\n",
            "0.4   33\n",
            "12   8\n",
            "0.4   34\n",
            "12   8\n",
            "0.4   35\n",
            "12   8\n",
            "0.4   36\n",
            "13   8\n",
            "0.4   37\n",
            "13   8\n",
            "0.4   38\n",
            "13   8\n",
            "0.4   39\n",
            "13   8\n",
            "0.4   40\n",
            "13   8\n",
            "0.4   41\n",
            "13   8\n",
            "0.4   42\n",
            "13   8\n",
            "0.4   43\n",
            "14   8\n",
            "0.4   44\n",
            "14   8\n",
            "0.4   45\n",
            "14   8\n",
            "0.4   46\n",
            "15   9\n",
            "0.4   47\n",
            "15   9\n",
            "0.4   48\n",
            "15   9\n",
            "0.4   49\n",
            "16   10\n",
            "0.4   50\n",
            "16   10\n",
            "0.4   51\n",
            "16   10\n",
            "0.4   52\n",
            "17   11\n",
            "0.4   53\n",
            "18   12\n",
            "0.4   54\n",
            "18   12\n",
            "0.4   55\n",
            "18   12\n",
            "0.4   56\n",
            "18   12\n",
            "0.4   57\n",
            "19   13\n",
            "0.4   58\n",
            "19   13\n",
            "0.4   59\n",
            "20   14\n",
            "0.4   60\n",
            "20   14\n",
            "0.4   61\n",
            "20   14\n",
            "0.4   62\n",
            "21   14\n",
            "0.4   63\n",
            "22   15\n",
            "0.4   64\n",
            "22   15\n",
            "0.4   65\n",
            "22   15\n",
            "0.4   66\n",
            "23   15\n",
            "0.4   67\n",
            "23   15\n",
            "0.4   68\n",
            "24   16\n",
            "0.4   69\n",
            "24   16\n",
            "0.4   70\n",
            "25   17\n",
            "0.4   71\n",
            "26   18\n",
            "0.4   72\n",
            "27   18\n",
            "0.4   73\n",
            "27   18\n",
            "0.4   74\n",
            "28   19\n",
            "0.4   75\n",
            "28   19\n",
            "0.4   76\n",
            "29   19\n",
            "0.4   77\n",
            "30   20\n",
            "0.4   78\n",
            "30   20\n",
            "0.4   79\n",
            "30   20\n",
            "0.4   80\n",
            "30   20\n",
            "0.4   81\n",
            "31   20\n",
            "0.4   82\n",
            "31   20\n",
            "0.4   83\n",
            "31   20\n",
            "0.4   84\n",
            "31   20\n",
            "0.4   85\n",
            "32   20\n",
            "0.4   86\n",
            "33   21\n",
            "0.4   87\n",
            "34   22\n",
            "0.4   88\n",
            "34   22\n",
            "0.4   89\n",
            "34   22\n",
            "0.4   90\n",
            "35   22\n",
            "0.4   91\n",
            "35   22\n",
            "0.4   92\n",
            "35   22\n",
            "0.4   93\n",
            "35   22\n",
            "0.4   94\n",
            "35   22\n",
            "0.4   95\n",
            "36   23\n",
            "0.4   96\n",
            "36   23\n",
            "0.4   97\n",
            "37   24\n",
            "0.4   98\n",
            "37   24\n",
            "0.4   99\n",
            "37   24\n",
            "0.4   100\n",
            "37   24\n",
            "0.4   101\n",
            "37   24\n",
            "0.4   102\n",
            "37   24\n",
            "0.4   103\n",
            "37   24\n",
            "0.4   104\n",
            "37   24\n",
            "0.4   105\n",
            "37   24\n",
            "0.4   106\n",
            "37   24\n",
            "0.4   107\n",
            "37   24\n",
            "0.4   108\n",
            "37   24\n",
            "0.4   109\n",
            "37   24\n",
            "0.4   110\n",
            "37   24\n",
            "0.4   111\n",
            "37   24\n",
            "0.4   112\n",
            "38   25\n",
            "0.4   113\n",
            "38   25\n",
            "0.4   114\n",
            "38   25\n",
            "0.4   115\n",
            "38   25\n",
            "0.4   116\n",
            "38   25\n",
            "0.4   117\n",
            "39   25\n",
            "0.4   118\n",
            "40   26\n",
            "0.4   119\n",
            "41   27\n",
            "0.4   120\n",
            "41   27\n",
            "0.4   121\n",
            "42   28\n",
            "0.4   122\n",
            "42   28\n",
            "0.4   123\n",
            "42   28\n",
            "0.4   124\n",
            "43   28\n",
            "0.4   125\n",
            "44   29\n",
            "0.4   126\n",
            "44   29\n",
            "0.4   127\n",
            "45   30\n",
            "0.4   128\n",
            "46   31\n",
            "0.4   129\n",
            "47   32\n",
            "0.4   130\n",
            "47   32\n",
            "0.4   131\n",
            "47   32\n",
            "0.4   132\n",
            "47   32\n",
            "0.4   133\n",
            "47   32\n",
            "0.4   134\n",
            "48   32\n",
            "0.4   135\n",
            "48   32\n",
            "0.4   136\n",
            "48   32\n",
            "0.4   137\n",
            "48   32\n",
            "0.4   138\n",
            "48   32\n",
            "0.4   139\n",
            "49   33\n",
            "0.4   140\n",
            "50   34\n",
            "0.4   141\n",
            "50   34\n",
            "0.4   142\n",
            "51   34\n",
            "0.4   143\n",
            "52   35\n",
            "0.4   144\n",
            "52   35\n",
            "0.4   145\n",
            "53   36\n",
            "0.4   146\n",
            "53   36\n",
            "0.4   147\n",
            "54   37\n",
            "0.4   148\n",
            "55   38\n",
            "0.4   149\n",
            "56   39\n",
            "0.4   150\n",
            "57   39\n",
            "0.4   151\n",
            "58   40\n",
            "0.4   152\n",
            "58   40\n",
            "0.4   153\n",
            "59   40\n",
            "0.4   154\n",
            "59   40\n",
            "0.4   155\n",
            "59   40\n",
            "0.4   156\n",
            "60   41\n",
            "0.4   157\n",
            "60   41\n",
            "0.4   158\n",
            "60   41\n",
            "0.4   159\n",
            "60   41\n",
            "0.4   160\n",
            "61   42\n",
            "0.4   161\n",
            "61   42\n",
            "0.4   162\n",
            "62   43\n",
            "0.4   163\n",
            "62   43\n",
            "0.4   164\n",
            "63   44\n",
            "0.4   165\n",
            "64   45\n",
            "0.4   166\n",
            "64   45\n",
            "0.4   167\n",
            "65   45\n",
            "0.4   168\n",
            "65   45\n",
            "0.4   169\n",
            "66   46\n",
            "0.4   170\n",
            "66   46\n",
            "0.4   171\n",
            "67   47\n",
            "0.4   172\n",
            "68   47\n",
            "0.4   173\n",
            "69   48\n",
            "0.4   174\n",
            "69   48\n",
            "0.4   175\n",
            "69   48\n",
            "0.4   176\n",
            "69   48\n",
            "0.4   177\n",
            "69   48\n",
            "0.4   178\n",
            "69   48\n",
            "0.4   179\n",
            "70   48\n",
            "0.4   180\n",
            "71   49\n",
            "0.4   181\n",
            "71   49\n",
            "0.4   182\n",
            "71   49\n",
            "0.4   183\n",
            "72   50\n",
            "0.4   184\n",
            "72   50\n",
            "0.4   185\n",
            "72   50\n",
            "0.4   186\n",
            "73   50\n",
            "0.4   187\n",
            "74   50\n",
            "0.4   188\n",
            "75   51\n",
            "0.4   189\n",
            "76   52\n",
            "0.4   190\n",
            "76   52\n",
            "0.4   191\n",
            "77   52\n",
            "0.4   192\n",
            "78   53\n",
            "0.4   193\n",
            "78   53\n",
            "0.4   194\n",
            "78   53\n",
            "0.4   195\n",
            "79   54\n",
            "0.4   196\n",
            "79   54\n",
            "0.4   197\n",
            "79   54\n",
            "0.4   198\n",
            "79   54\n",
            "0.4   199\n",
            "79   54\n",
            "Epsilon value -  0.5\n",
            "\n",
            "0.5   0\n",
            "0   0\n",
            "0.5   1\n",
            "0   0\n",
            "0.5   2\n",
            "1   1\n",
            "0.5   3\n",
            "2   2\n",
            "0.5   4\n",
            "2   2\n",
            "0.5   5\n",
            "3   3\n",
            "0.5   6\n",
            "4   3\n",
            "0.5   7\n",
            "4   3\n",
            "0.5   8\n",
            "4   3\n",
            "0.5   9\n",
            "4   3\n",
            "0.5   10\n",
            "5   3\n",
            "0.5   11\n",
            "6   4\n",
            "0.5   12\n",
            "6   4\n",
            "0.5   13\n",
            "6   4\n",
            "0.5   14\n",
            "6   4\n",
            "0.5   15\n",
            "6   4\n",
            "0.5   16\n",
            "6   4\n",
            "0.5   17\n",
            "6   4\n",
            "0.5   18\n",
            "6   4\n",
            "0.5   19\n",
            "6   4\n",
            "0.5   20\n",
            "7   5\n",
            "0.5   21\n",
            "7   5\n",
            "0.5   22\n",
            "7   5\n",
            "0.5   23\n",
            "7   5\n",
            "0.5   24\n",
            "7   5\n",
            "0.5   25\n",
            "8   6\n",
            "0.5   26\n",
            "9   7\n",
            "0.5   27\n",
            "10   8\n",
            "0.5   28\n",
            "10   8\n",
            "0.5   29\n",
            "10   8\n",
            "0.5   30\n",
            "10   8\n",
            "0.5   31\n",
            "10   8\n",
            "0.5   32\n",
            "11   9\n",
            "0.5   33\n",
            "12   10\n",
            "0.5   34\n",
            "12   10\n",
            "0.5   35\n",
            "12   10\n",
            "0.5   36\n",
            "13   11\n",
            "0.5   37\n",
            "13   11\n",
            "0.5   38\n",
            "13   11\n",
            "0.5   39\n",
            "13   11\n",
            "0.5   40\n",
            "13   11\n",
            "0.5   41\n",
            "13   11\n",
            "0.5   42\n",
            "13   11\n",
            "0.5   43\n",
            "14   11\n",
            "0.5   44\n",
            "14   11\n",
            "0.5   45\n",
            "14   11\n",
            "0.5   46\n",
            "15   12\n",
            "0.5   47\n",
            "15   12\n",
            "0.5   48\n",
            "15   12\n",
            "0.5   49\n",
            "16   13\n",
            "0.5   50\n",
            "16   13\n",
            "0.5   51\n",
            "16   13\n",
            "0.5   52\n",
            "17   14\n",
            "0.5   53\n",
            "18   15\n",
            "0.5   54\n",
            "18   15\n",
            "0.5   55\n",
            "18   15\n",
            "0.5   56\n",
            "18   15\n",
            "0.5   57\n",
            "19   16\n",
            "0.5   58\n",
            "19   16\n",
            "0.5   59\n",
            "20   17\n",
            "0.5   60\n",
            "20   17\n",
            "0.5   61\n",
            "20   17\n",
            "0.5   62\n",
            "21   17\n",
            "0.5   63\n",
            "22   18\n",
            "0.5   64\n",
            "22   18\n",
            "0.5   65\n",
            "22   18\n",
            "0.5   66\n",
            "23   18\n",
            "0.5   67\n",
            "23   18\n",
            "0.5   68\n",
            "24   19\n",
            "0.5   69\n",
            "24   19\n",
            "0.5   70\n",
            "25   20\n",
            "0.5   71\n",
            "26   21\n",
            "0.5   72\n",
            "27   21\n",
            "0.5   73\n",
            "27   21\n",
            "0.5   74\n",
            "28   22\n",
            "0.5   75\n",
            "28   22\n",
            "0.5   76\n",
            "29   23\n",
            "0.5   77\n",
            "30   24\n",
            "0.5   78\n",
            "30   24\n",
            "0.5   79\n",
            "30   24\n",
            "0.5   80\n",
            "30   24\n",
            "0.5   81\n",
            "31   24\n",
            "0.5   82\n",
            "31   24\n",
            "0.5   83\n",
            "31   24\n",
            "0.5   84\n",
            "31   24\n",
            "0.5   85\n",
            "32   24\n",
            "0.5   86\n",
            "33   25\n",
            "0.5   87\n",
            "34   26\n",
            "0.5   88\n",
            "34   26\n",
            "0.5   89\n",
            "34   26\n",
            "0.5   90\n",
            "35   26\n",
            "0.5   91\n",
            "35   26\n",
            "0.5   92\n",
            "35   26\n",
            "0.5   93\n",
            "35   26\n",
            "0.5   94\n",
            "35   26\n",
            "0.5   95\n",
            "36   27\n",
            "0.5   96\n",
            "36   27\n",
            "0.5   97\n",
            "37   28\n",
            "0.5   98\n",
            "37   28\n",
            "0.5   99\n",
            "37   28\n",
            "0.5   100\n",
            "37   28\n",
            "0.5   101\n",
            "37   28\n",
            "0.5   102\n",
            "37   28\n",
            "0.5   103\n",
            "37   28\n",
            "0.5   104\n",
            "37   28\n",
            "0.5   105\n",
            "37   28\n",
            "0.5   106\n",
            "37   28\n",
            "0.5   107\n",
            "37   28\n",
            "0.5   108\n",
            "37   28\n",
            "0.5   109\n",
            "37   28\n",
            "0.5   110\n",
            "37   28\n",
            "0.5   111\n",
            "37   28\n",
            "0.5   112\n",
            "38   29\n",
            "0.5   113\n",
            "38   29\n",
            "0.5   114\n",
            "38   29\n",
            "0.5   115\n",
            "38   29\n",
            "0.5   116\n",
            "38   29\n",
            "0.5   117\n",
            "39   30\n",
            "0.5   118\n",
            "40   31\n",
            "0.5   119\n",
            "41   32\n",
            "0.5   120\n",
            "41   32\n",
            "0.5   121\n",
            "42   33\n",
            "0.5   122\n",
            "42   33\n",
            "0.5   123\n",
            "42   33\n",
            "0.5   124\n",
            "43   33\n",
            "0.5   125\n",
            "44   34\n",
            "0.5   126\n",
            "44   34\n",
            "0.5   127\n",
            "45   35\n",
            "0.5   128\n",
            "46   36\n",
            "0.5   129\n",
            "47   37\n",
            "0.5   130\n",
            "47   37\n",
            "0.5   131\n",
            "47   37\n",
            "0.5   132\n",
            "47   37\n",
            "0.5   133\n",
            "47   37\n",
            "0.5   134\n",
            "48   38\n",
            "0.5   135\n",
            "48   38\n",
            "0.5   136\n",
            "48   38\n",
            "0.5   137\n",
            "48   38\n",
            "0.5   138\n",
            "48   38\n",
            "0.5   139\n",
            "49   39\n",
            "0.5   140\n",
            "50   40\n",
            "0.5   141\n",
            "50   40\n",
            "0.5   142\n",
            "51   40\n",
            "0.5   143\n",
            "52   41\n",
            "0.5   144\n",
            "52   41\n",
            "0.5   145\n",
            "53   42\n",
            "0.5   146\n",
            "53   42\n",
            "0.5   147\n",
            "54   43\n",
            "0.5   148\n",
            "55   44\n",
            "0.5   149\n",
            "56   45\n",
            "0.5   150\n",
            "57   46\n",
            "0.5   151\n",
            "58   47\n",
            "0.5   152\n",
            "58   47\n",
            "0.5   153\n",
            "59   47\n",
            "0.5   154\n",
            "59   47\n",
            "0.5   155\n",
            "59   47\n",
            "0.5   156\n",
            "60   48\n",
            "0.5   157\n",
            "60   48\n",
            "0.5   158\n",
            "60   48\n",
            "0.5   159\n",
            "60   48\n",
            "0.5   160\n",
            "61   49\n",
            "0.5   161\n",
            "61   49\n",
            "0.5   162\n",
            "62   50\n",
            "0.5   163\n",
            "62   50\n",
            "0.5   164\n",
            "63   51\n",
            "0.5   165\n",
            "64   52\n",
            "0.5   166\n",
            "64   52\n",
            "0.5   167\n",
            "65   52\n",
            "0.5   168\n",
            "65   52\n",
            "0.5   169\n",
            "66   53\n",
            "0.5   170\n",
            "66   53\n",
            "0.5   171\n",
            "67   54\n",
            "0.5   172\n",
            "68   54\n",
            "0.5   173\n",
            "69   55\n",
            "0.5   174\n",
            "69   55\n",
            "0.5   175\n",
            "69   55\n",
            "0.5   176\n",
            "69   55\n",
            "0.5   177\n",
            "69   55\n",
            "0.5   178\n",
            "69   55\n",
            "0.5   179\n",
            "70   55\n",
            "0.5   180\n",
            "71   56\n",
            "0.5   181\n",
            "71   56\n",
            "0.5   182\n",
            "71   56\n",
            "0.5   183\n",
            "72   57\n",
            "0.5   184\n",
            "72   57\n",
            "0.5   185\n",
            "72   57\n",
            "0.5   186\n",
            "73   57\n",
            "0.5   187\n",
            "74   57\n",
            "0.5   188\n",
            "75   58\n",
            "0.5   189\n",
            "76   59\n",
            "0.5   190\n",
            "76   59\n",
            "0.5   191\n",
            "77   59\n",
            "0.5   192\n",
            "78   60\n",
            "0.5   193\n",
            "78   60\n",
            "0.5   194\n",
            "78   60\n",
            "0.5   195\n",
            "79   61\n",
            "0.5   196\n",
            "79   61\n",
            "0.5   197\n",
            "79   61\n",
            "0.5   198\n",
            "79   61\n",
            "0.5   199\n",
            "79   61\n",
            "Epsilon value -  0.6\n",
            "\n",
            "0.6   0\n",
            "0   0\n",
            "0.6   1\n",
            "0   0\n",
            "0.6   2\n",
            "1   1\n",
            "0.6   3\n",
            "2   2\n",
            "0.6   4\n",
            "2   2\n",
            "0.6   5\n",
            "3   3\n",
            "0.6   6\n",
            "4   3\n",
            "0.6   7\n",
            "4   3\n",
            "0.6   8\n",
            "4   3\n",
            "0.6   9\n",
            "4   3\n",
            "0.6   10\n",
            "5   3\n",
            "0.6   11\n",
            "6   4\n",
            "0.6   12\n",
            "6   4\n",
            "0.6   13\n",
            "6   4\n",
            "0.6   14\n",
            "6   4\n",
            "0.6   15\n",
            "6   4\n",
            "0.6   16\n",
            "6   4\n",
            "0.6   17\n",
            "6   4\n",
            "0.6   18\n",
            "6   4\n",
            "0.6   19\n",
            "6   4\n",
            "0.6   20\n",
            "7   5\n",
            "0.6   21\n",
            "7   5\n",
            "0.6   22\n",
            "7   5\n",
            "0.6   23\n",
            "7   5\n",
            "0.6   24\n",
            "7   5\n",
            "0.6   25\n",
            "8   6\n",
            "0.6   26\n",
            "9   7\n",
            "0.6   27\n",
            "10   8\n",
            "0.6   28\n",
            "10   8\n",
            "0.6   29\n",
            "10   8\n",
            "0.6   30\n",
            "10   8\n",
            "0.6   31\n",
            "10   8\n",
            "0.6   32\n",
            "11   9\n",
            "0.6   33\n",
            "12   10\n",
            "0.6   34\n",
            "12   10\n",
            "0.6   35\n",
            "12   10\n",
            "0.6   36\n",
            "13   11\n",
            "0.6   37\n",
            "13   11\n",
            "0.6   38\n",
            "13   11\n",
            "0.6   39\n",
            "13   11\n",
            "0.6   40\n",
            "13   11\n",
            "0.6   41\n",
            "13   11\n",
            "0.6   42\n",
            "13   11\n",
            "0.6   43\n",
            "14   12\n",
            "0.6   44\n",
            "14   12\n",
            "0.6   45\n",
            "14   12\n",
            "0.6   46\n",
            "15   13\n",
            "0.6   47\n",
            "15   13\n",
            "0.6   48\n",
            "15   13\n",
            "0.6   49\n",
            "16   14\n",
            "0.6   50\n",
            "16   14\n",
            "0.6   51\n",
            "16   14\n",
            "0.6   52\n",
            "17   15\n",
            "0.6   53\n",
            "18   16\n",
            "0.6   54\n",
            "18   16\n",
            "0.6   55\n",
            "18   16\n",
            "0.6   56\n",
            "18   16\n",
            "0.6   57\n",
            "19   17\n",
            "0.6   58\n",
            "19   17\n",
            "0.6   59\n",
            "20   18\n",
            "0.6   60\n",
            "20   18\n",
            "0.6   61\n",
            "20   18\n",
            "0.6   62\n",
            "21   18\n",
            "0.6   63\n",
            "22   19\n",
            "0.6   64\n",
            "22   19\n",
            "0.6   65\n",
            "22   19\n",
            "0.6   66\n",
            "23   19\n",
            "0.6   67\n",
            "23   19\n",
            "0.6   68\n",
            "24   20\n",
            "0.6   69\n",
            "24   20\n",
            "0.6   70\n",
            "25   21\n",
            "0.6   71\n",
            "26   22\n",
            "0.6   72\n",
            "27   22\n",
            "0.6   73\n",
            "27   22\n",
            "0.6   74\n",
            "28   23\n",
            "0.6   75\n",
            "28   23\n",
            "0.6   76\n",
            "29   24\n",
            "0.6   77\n",
            "30   25\n",
            "0.6   78\n",
            "30   25\n",
            "0.6   79\n",
            "30   25\n",
            "0.6   80\n",
            "30   25\n",
            "0.6   81\n",
            "31   26\n",
            "0.6   82\n",
            "31   26\n",
            "0.6   83\n",
            "31   26\n",
            "0.6   84\n",
            "31   26\n",
            "0.6   85\n",
            "32   27\n",
            "0.6   86\n",
            "33   28\n",
            "0.6   87\n",
            "34   29\n",
            "0.6   88\n",
            "34   29\n",
            "0.6   89\n",
            "34   29\n",
            "0.6   90\n",
            "35   29\n",
            "0.6   91\n",
            "35   29\n",
            "0.6   92\n",
            "35   29\n",
            "0.6   93\n",
            "35   29\n",
            "0.6   94\n",
            "35   29\n",
            "0.6   95\n",
            "36   30\n",
            "0.6   96\n",
            "36   30\n",
            "0.6   97\n",
            "37   31\n",
            "0.6   98\n",
            "37   31\n",
            "0.6   99\n",
            "37   31\n",
            "0.6   100\n",
            "37   31\n",
            "0.6   101\n",
            "37   31\n",
            "0.6   102\n",
            "37   31\n",
            "0.6   103\n",
            "37   31\n",
            "0.6   104\n",
            "37   31\n",
            "0.6   105\n",
            "37   31\n",
            "0.6   106\n",
            "37   31\n",
            "0.6   107\n",
            "37   31\n",
            "0.6   108\n",
            "37   31\n",
            "0.6   109\n",
            "37   31\n",
            "0.6   110\n",
            "37   32\n",
            "0.6   111\n",
            "37   32\n",
            "0.6   112\n",
            "38   33\n",
            "0.6   113\n",
            "38   33\n",
            "0.6   114\n",
            "38   33\n",
            "0.6   115\n",
            "38   33\n",
            "0.6   116\n",
            "38   33\n",
            "0.6   117\n",
            "39   34\n",
            "0.6   118\n",
            "40   35\n",
            "0.6   119\n",
            "41   36\n",
            "0.6   120\n",
            "41   36\n",
            "0.6   121\n",
            "42   37\n",
            "0.6   122\n",
            "42   37\n",
            "0.6   123\n",
            "42   37\n",
            "0.6   124\n",
            "43   37\n",
            "0.6   125\n",
            "44   38\n",
            "0.6   126\n",
            "44   38\n",
            "0.6   127\n",
            "45   39\n",
            "0.6   128\n",
            "46   40\n",
            "0.6   129\n",
            "47   41\n",
            "0.6   130\n",
            "47   41\n",
            "0.6   131\n",
            "47   41\n",
            "0.6   132\n",
            "47   41\n",
            "0.6   133\n",
            "47   41\n",
            "0.6   134\n",
            "48   42\n",
            "0.6   135\n",
            "48   42\n",
            "0.6   136\n",
            "48   42\n",
            "0.6   137\n",
            "48   42\n",
            "0.6   138\n",
            "48   42\n",
            "0.6   139\n",
            "49   43\n",
            "0.6   140\n",
            "50   44\n",
            "0.6   141\n",
            "50   44\n",
            "0.6   142\n",
            "51   45\n",
            "0.6   143\n",
            "52   46\n",
            "0.6   144\n",
            "52   46\n",
            "0.6   145\n",
            "53   47\n",
            "0.6   146\n",
            "53   47\n",
            "0.6   147\n",
            "54   48\n",
            "0.6   148\n",
            "55   49\n",
            "0.6   149\n",
            "56   50\n",
            "0.6   150\n",
            "57   51\n",
            "0.6   151\n",
            "58   52\n",
            "0.6   152\n",
            "58   52\n",
            "0.6   153\n",
            "59   52\n",
            "0.6   154\n",
            "59   52\n",
            "0.6   155\n",
            "59   52\n",
            "0.6   156\n",
            "60   53\n",
            "0.6   157\n",
            "60   53\n",
            "0.6   158\n",
            "60   53\n",
            "0.6   159\n",
            "60   53\n",
            "0.6   160\n",
            "61   54\n",
            "0.6   161\n",
            "61   54\n",
            "0.6   162\n",
            "62   55\n",
            "0.6   163\n",
            "62   55\n",
            "0.6   164\n",
            "63   56\n",
            "0.6   165\n",
            "64   57\n",
            "0.6   166\n",
            "64   57\n",
            "0.6   167\n",
            "65   57\n",
            "0.6   168\n",
            "65   57\n",
            "0.6   169\n",
            "66   58\n",
            "0.6   170\n",
            "66   58\n",
            "0.6   171\n",
            "67   59\n",
            "0.6   172\n",
            "68   59\n",
            "0.6   173\n",
            "69   60\n",
            "0.6   174\n",
            "69   60\n",
            "0.6   175\n",
            "69   60\n",
            "0.6   176\n",
            "69   60\n",
            "0.6   177\n",
            "69   60\n",
            "0.6   178\n",
            "69   60\n",
            "0.6   179\n",
            "70   61\n",
            "0.6   180\n",
            "71   62\n",
            "0.6   181\n",
            "71   62\n",
            "0.6   182\n",
            "71   62\n",
            "0.6   183\n",
            "72   63\n",
            "0.6   184\n",
            "72   63\n",
            "0.6   185\n",
            "72   63\n",
            "0.6   186\n",
            "73   64\n",
            "0.6   187\n",
            "74   64\n",
            "0.6   188\n",
            "75   65\n",
            "0.6   189\n",
            "76   66\n",
            "0.6   190\n",
            "76   66\n",
            "0.6   191\n",
            "77   66\n",
            "0.6   192\n",
            "78   67\n",
            "0.6   193\n",
            "78   67\n",
            "0.6   194\n",
            "78   67\n",
            "0.6   195\n",
            "79   68\n",
            "0.6   196\n",
            "79   68\n",
            "0.6   197\n",
            "79   68\n",
            "0.6   198\n",
            "79   68\n",
            "0.6   199\n",
            "79   68\n",
            "Epsilon value -  0.7\n",
            "\n",
            "0.7   0\n",
            "0   0\n",
            "0.7   1\n",
            "0   0\n",
            "0.7   2\n",
            "1   1\n",
            "0.7   3\n",
            "2   2\n",
            "0.7   4\n",
            "2   2\n",
            "0.7   5\n",
            "3   3\n",
            "0.7   6\n",
            "4   4\n",
            "0.7   7\n",
            "4   4\n",
            "0.7   8\n",
            "4   4\n",
            "0.7   9\n",
            "4   4\n",
            "0.7   10\n",
            "5   5\n",
            "0.7   11\n",
            "6   6\n",
            "0.7   12\n",
            "6   7\n",
            "0.7   13\n",
            "6   7\n",
            "0.7   14\n",
            "6   7\n",
            "0.7   15\n",
            "6   7\n",
            "0.7   16\n",
            "6   7\n",
            "0.7   17\n",
            "6   7\n",
            "0.7   18\n",
            "6   7\n",
            "0.7   19\n",
            "6   7\n",
            "0.7   20\n",
            "7   8\n",
            "0.7   21\n",
            "7   8\n",
            "0.7   22\n",
            "7   8\n",
            "0.7   23\n",
            "7   8\n",
            "0.7   24\n",
            "7   8\n",
            "0.7   25\n",
            "8   9\n",
            "0.7   26\n",
            "9   10\n",
            "0.7   27\n",
            "10   11\n",
            "0.7   28\n",
            "10   11\n",
            "0.7   29\n",
            "10   11\n",
            "0.7   30\n",
            "10   11\n",
            "0.7   31\n",
            "10   11\n",
            "0.7   32\n",
            "11   12\n",
            "0.7   33\n",
            "12   13\n",
            "0.7   34\n",
            "12   13\n",
            "0.7   35\n",
            "12   13\n",
            "0.7   36\n",
            "13   14\n",
            "0.7   37\n",
            "13   14\n",
            "0.7   38\n",
            "13   14\n",
            "0.7   39\n",
            "13   14\n",
            "0.7   40\n",
            "13   14\n",
            "0.7   41\n",
            "13   14\n",
            "0.7   42\n",
            "13   14\n",
            "0.7   43\n",
            "14   15\n",
            "0.7   44\n",
            "14   15\n",
            "0.7   45\n",
            "14   15\n",
            "0.7   46\n",
            "15   16\n",
            "0.7   47\n",
            "15   16\n",
            "0.7   48\n",
            "15   16\n",
            "0.7   49\n",
            "16   17\n",
            "0.7   50\n",
            "16   17\n",
            "0.7   51\n",
            "16   17\n",
            "0.7   52\n",
            "17   18\n",
            "0.7   53\n",
            "18   19\n",
            "0.7   54\n",
            "18   19\n",
            "0.7   55\n",
            "18   19\n",
            "0.7   56\n",
            "18   19\n",
            "0.7   57\n",
            "19   20\n",
            "0.7   58\n",
            "19   20\n",
            "0.7   59\n",
            "20   21\n",
            "0.7   60\n",
            "20   21\n",
            "0.7   61\n",
            "20   21\n",
            "0.7   62\n",
            "21   21\n",
            "0.7   63\n",
            "22   22\n",
            "0.7   64\n",
            "22   22\n",
            "0.7   65\n",
            "22   22\n",
            "0.7   66\n",
            "23   22\n",
            "0.7   67\n",
            "23   22\n",
            "0.7   68\n",
            "24   23\n",
            "0.7   69\n",
            "24   24\n",
            "0.7   70\n",
            "25   25\n",
            "0.7   71\n",
            "26   26\n",
            "0.7   72\n",
            "27   26\n",
            "0.7   73\n",
            "27   26\n",
            "0.7   74\n",
            "28   27\n",
            "0.7   75\n",
            "28   27\n",
            "0.7   76\n",
            "29   28\n",
            "0.7   77\n",
            "30   29\n",
            "0.7   78\n",
            "30   29\n",
            "0.7   79\n",
            "30   29\n",
            "0.7   80\n",
            "30   29\n",
            "0.7   81\n",
            "31   30\n",
            "0.7   82\n",
            "31   30\n",
            "0.7   83\n",
            "31   30\n",
            "0.7   84\n",
            "31   30\n",
            "0.7   85\n",
            "32   31\n",
            "0.7   86\n",
            "33   32\n",
            "0.7   87\n",
            "34   33\n",
            "0.7   88\n",
            "34   33\n",
            "0.7   89\n",
            "34   33\n",
            "0.7   90\n",
            "35   33\n",
            "0.7   91\n",
            "35   33\n",
            "0.7   92\n",
            "35   33\n",
            "0.7   93\n",
            "35   33\n",
            "0.7   94\n",
            "35   33\n",
            "0.7   95\n",
            "36   34\n",
            "0.7   96\n",
            "36   34\n",
            "0.7   97\n",
            "37   35\n",
            "0.7   98\n",
            "37   35\n",
            "0.7   99\n",
            "37   35\n",
            "0.7   100\n",
            "37   35\n",
            "0.7   101\n",
            "37   35\n",
            "0.7   102\n",
            "37   35\n",
            "0.7   103\n",
            "37   35\n",
            "0.7   104\n",
            "37   35\n",
            "0.7   105\n",
            "37   35\n",
            "0.7   106\n",
            "37   35\n",
            "0.7   107\n",
            "37   35\n",
            "0.7   108\n",
            "37   35\n",
            "0.7   109\n",
            "37   35\n",
            "0.7   110\n",
            "37   36\n",
            "0.7   111\n",
            "37   36\n",
            "0.7   112\n",
            "38   37\n",
            "0.7   113\n",
            "38   37\n",
            "0.7   114\n",
            "38   37\n",
            "0.7   115\n",
            "38   37\n",
            "0.7   116\n",
            "38   37\n",
            "0.7   117\n",
            "39   38\n",
            "0.7   118\n",
            "40   39\n",
            "0.7   119\n",
            "41   40\n",
            "0.7   120\n",
            "41   40\n",
            "0.7   121\n",
            "42   41\n",
            "0.7   122\n",
            "42   41\n",
            "0.7   123\n",
            "42   41\n",
            "0.7   124\n",
            "43   41\n",
            "0.7   125\n",
            "44   42\n",
            "0.7   126\n",
            "44   42\n",
            "0.7   127\n",
            "45   43\n",
            "0.7   128\n",
            "46   44\n",
            "0.7   129\n",
            "47   45\n",
            "0.7   130\n",
            "47   45\n",
            "0.7   131\n",
            "47   45\n",
            "0.7   132\n",
            "47   45\n",
            "0.7   133\n",
            "47   45\n",
            "0.7   134\n",
            "48   46\n",
            "0.7   135\n",
            "48   46\n",
            "0.7   136\n",
            "48   46\n",
            "0.7   137\n",
            "48   46\n",
            "0.7   138\n",
            "48   46\n",
            "0.7   139\n",
            "49   47\n",
            "0.7   140\n",
            "50   48\n",
            "0.7   141\n",
            "50   48\n",
            "0.7   142\n",
            "51   49\n",
            "0.7   143\n",
            "52   50\n",
            "0.7   144\n",
            "52   50\n",
            "0.7   145\n",
            "53   51\n",
            "0.7   146\n",
            "53   52\n",
            "0.7   147\n",
            "54   53\n",
            "0.7   148\n",
            "55   54\n",
            "0.7   149\n",
            "56   55\n",
            "0.7   150\n",
            "57   56\n",
            "0.7   151\n",
            "58   57\n",
            "0.7   152\n",
            "58   57\n",
            "0.7   153\n",
            "59   58\n",
            "0.7   154\n",
            "59   58\n",
            "0.7   155\n",
            "59   58\n",
            "0.7   156\n",
            "60   59\n",
            "0.7   157\n",
            "60   59\n",
            "0.7   158\n",
            "60   59\n",
            "0.7   159\n",
            "60   59\n",
            "0.7   160\n",
            "61   60\n",
            "0.7   161\n",
            "61   60\n",
            "0.7   162\n",
            "62   61\n",
            "0.7   163\n",
            "62   61\n",
            "0.7   164\n",
            "63   62\n",
            "0.7   165\n",
            "64   63\n",
            "0.7   166\n",
            "64   63\n",
            "0.7   167\n",
            "65   63\n",
            "0.7   168\n",
            "65   63\n",
            "0.7   169\n",
            "66   64\n",
            "0.7   170\n",
            "66   64\n",
            "0.7   171\n",
            "67   65\n",
            "0.7   172\n",
            "68   65\n",
            "0.7   173\n",
            "69   66\n",
            "0.7   174\n",
            "69   66\n",
            "0.7   175\n",
            "69   66\n",
            "0.7   176\n",
            "69   66\n",
            "0.7   177\n",
            "69   66\n",
            "0.7   178\n",
            "69   66\n",
            "0.7   179\n",
            "70   67\n",
            "0.7   180\n",
            "71   68\n",
            "0.7   181\n",
            "71   68\n",
            "0.7   182\n",
            "71   68\n",
            "0.7   183\n",
            "72   69\n",
            "0.7   184\n",
            "72   69\n",
            "0.7   185\n",
            "72   69\n",
            "0.7   186\n",
            "73   70\n",
            "0.7   187\n",
            "74   70\n",
            "0.7   188\n",
            "75   71\n",
            "0.7   189\n",
            "76   72\n",
            "0.7   190\n",
            "76   72\n",
            "0.7   191\n",
            "77   72\n",
            "0.7   192\n",
            "78   73\n",
            "0.7   193\n",
            "78   73\n",
            "0.7   194\n",
            "78   74\n",
            "0.7   195\n",
            "79   75\n",
            "0.7   196\n",
            "79   75\n",
            "0.7   197\n",
            "79   75\n",
            "0.7   198\n",
            "79   75\n",
            "0.7   199\n",
            "79   75\n",
            "Epsilon value -  0.7999999999999999\n",
            "\n",
            "0.7999999999999999   0\n",
            "0   0\n",
            "0.7999999999999999   1\n",
            "0   0\n",
            "0.7999999999999999   2\n",
            "1   1\n",
            "0.7999999999999999   3\n",
            "2   2\n",
            "0.7999999999999999   4\n",
            "2   2\n",
            "0.7999999999999999   5\n",
            "3   3\n",
            "0.7999999999999999   6\n",
            "4   4\n",
            "0.7999999999999999   7\n",
            "4   4\n",
            "0.7999999999999999   8\n",
            "4   4\n",
            "0.7999999999999999   9\n",
            "4   4\n",
            "0.7999999999999999   10\n",
            "5   5\n",
            "0.7999999999999999   11\n",
            "6   6\n",
            "0.7999999999999999   12\n",
            "6   7\n",
            "0.7999999999999999   13\n",
            "6   7\n",
            "0.7999999999999999   14\n",
            "6   7\n",
            "0.7999999999999999   15\n",
            "6   7\n",
            "0.7999999999999999   16\n",
            "6   7\n",
            "0.7999999999999999   17\n",
            "6   7\n",
            "0.7999999999999999   18\n",
            "6   7\n",
            "0.7999999999999999   19\n",
            "6   7\n",
            "0.7999999999999999   20\n",
            "7   8\n",
            "0.7999999999999999   21\n",
            "7   8\n",
            "0.7999999999999999   22\n",
            "7   8\n",
            "0.7999999999999999   23\n",
            "7   8\n",
            "0.7999999999999999   24\n",
            "7   8\n",
            "0.7999999999999999   25\n",
            "8   9\n",
            "0.7999999999999999   26\n",
            "9   10\n",
            "0.7999999999999999   27\n",
            "10   11\n",
            "0.7999999999999999   28\n",
            "10   11\n",
            "0.7999999999999999   29\n",
            "10   11\n",
            "0.7999999999999999   30\n",
            "10   12\n",
            "0.7999999999999999   31\n",
            "10   12\n",
            "0.7999999999999999   32\n",
            "11   13\n",
            "0.7999999999999999   33\n",
            "12   14\n",
            "0.7999999999999999   34\n",
            "12   14\n",
            "0.7999999999999999   35\n",
            "12   15\n",
            "0.7999999999999999   36\n",
            "13   16\n",
            "0.7999999999999999   37\n",
            "13   16\n",
            "0.7999999999999999   38\n",
            "13   16\n",
            "0.7999999999999999   39\n",
            "13   16\n",
            "0.7999999999999999   40\n",
            "13   16\n",
            "0.7999999999999999   41\n",
            "13   16\n",
            "0.7999999999999999   42\n",
            "13   16\n",
            "0.7999999999999999   43\n",
            "14   17\n",
            "0.7999999999999999   44\n",
            "14   17\n",
            "0.7999999999999999   45\n",
            "14   17\n",
            "0.7999999999999999   46\n",
            "15   18\n",
            "0.7999999999999999   47\n",
            "15   18\n",
            "0.7999999999999999   48\n",
            "15   18\n",
            "0.7999999999999999   49\n",
            "16   19\n",
            "0.7999999999999999   50\n",
            "16   19\n",
            "0.7999999999999999   51\n",
            "16   19\n",
            "0.7999999999999999   52\n",
            "17   20\n",
            "0.7999999999999999   53\n",
            "18   21\n",
            "0.7999999999999999   54\n",
            "18   21\n",
            "0.7999999999999999   55\n",
            "18   21\n",
            "0.7999999999999999   56\n",
            "18   21\n",
            "0.7999999999999999   57\n",
            "19   22\n",
            "0.7999999999999999   58\n",
            "19   22\n",
            "0.7999999999999999   59\n",
            "20   23\n",
            "0.7999999999999999   60\n",
            "20   23\n",
            "0.7999999999999999   61\n",
            "20   23\n",
            "0.7999999999999999   62\n",
            "21   23\n",
            "0.7999999999999999   63\n",
            "22   24\n",
            "0.7999999999999999   64\n",
            "22   24\n",
            "0.7999999999999999   65\n",
            "22   24\n",
            "0.7999999999999999   66\n",
            "23   24\n",
            "0.7999999999999999   67\n",
            "23   24\n",
            "0.7999999999999999   68\n",
            "24   25\n",
            "0.7999999999999999   69\n",
            "24   26\n",
            "0.7999999999999999   70\n",
            "25   27\n",
            "0.7999999999999999   71\n",
            "26   28\n",
            "0.7999999999999999   72\n",
            "27   28\n",
            "0.7999999999999999   73\n",
            "27   28\n",
            "0.7999999999999999   74\n",
            "28   29\n",
            "0.7999999999999999   75\n",
            "28   29\n",
            "0.7999999999999999   76\n",
            "29   30\n",
            "0.7999999999999999   77\n",
            "30   31\n",
            "0.7999999999999999   78\n",
            "30   31\n",
            "0.7999999999999999   79\n",
            "30   31\n",
            "0.7999999999999999   80\n",
            "30   31\n",
            "0.7999999999999999   81\n",
            "31   32\n",
            "0.7999999999999999   82\n",
            "31   32\n",
            "0.7999999999999999   83\n",
            "31   32\n",
            "0.7999999999999999   84\n",
            "31   32\n",
            "0.7999999999999999   85\n",
            "32   33\n",
            "0.7999999999999999   86\n",
            "33   34\n",
            "0.7999999999999999   87\n",
            "34   35\n",
            "0.7999999999999999   88\n",
            "34   35\n",
            "0.7999999999999999   89\n",
            "34   35\n",
            "0.7999999999999999   90\n",
            "35   35\n",
            "0.7999999999999999   91\n",
            "35   35\n",
            "0.7999999999999999   92\n",
            "35   35\n",
            "0.7999999999999999   93\n",
            "35   35\n",
            "0.7999999999999999   94\n",
            "35   35\n",
            "0.7999999999999999   95\n",
            "36   36\n",
            "0.7999999999999999   96\n",
            "36   36\n",
            "0.7999999999999999   97\n",
            "37   37\n",
            "0.7999999999999999   98\n",
            "37   37\n",
            "0.7999999999999999   99\n",
            "37   37\n",
            "0.7999999999999999   100\n",
            "37   37\n",
            "0.7999999999999999   101\n",
            "37   37\n",
            "0.7999999999999999   102\n",
            "37   37\n",
            "0.7999999999999999   103\n",
            "37   37\n",
            "0.7999999999999999   104\n",
            "37   37\n",
            "0.7999999999999999   105\n",
            "37   37\n",
            "0.7999999999999999   106\n",
            "37   37\n",
            "0.7999999999999999   107\n",
            "37   37\n",
            "0.7999999999999999   108\n",
            "37   37\n",
            "0.7999999999999999   109\n",
            "37   37\n",
            "0.7999999999999999   110\n",
            "37   38\n",
            "0.7999999999999999   111\n",
            "37   38\n",
            "0.7999999999999999   112\n",
            "38   39\n",
            "0.7999999999999999   113\n",
            "38   39\n",
            "0.7999999999999999   114\n",
            "38   39\n",
            "0.7999999999999999   115\n",
            "38   39\n",
            "0.7999999999999999   116\n",
            "38   39\n",
            "0.7999999999999999   117\n",
            "39   40\n",
            "0.7999999999999999   118\n",
            "40   41\n",
            "0.7999999999999999   119\n",
            "41   42\n",
            "0.7999999999999999   120\n",
            "41   42\n",
            "0.7999999999999999   121\n",
            "42   43\n",
            "0.7999999999999999   122\n",
            "42   43\n",
            "0.7999999999999999   123\n",
            "42   43\n",
            "0.7999999999999999   124\n",
            "43   43\n",
            "0.7999999999999999   125\n",
            "44   44\n",
            "0.7999999999999999   126\n",
            "44   44\n",
            "0.7999999999999999   127\n",
            "45   45\n",
            "0.7999999999999999   128\n",
            "46   46\n",
            "0.7999999999999999   129\n",
            "47   47\n",
            "0.7999999999999999   130\n",
            "47   47\n",
            "0.7999999999999999   131\n",
            "47   47\n",
            "0.7999999999999999   132\n",
            "47   47\n",
            "0.7999999999999999   133\n",
            "47   47\n",
            "0.7999999999999999   134\n",
            "48   48\n",
            "0.7999999999999999   135\n",
            "48   48\n",
            "0.7999999999999999   136\n",
            "48   48\n",
            "0.7999999999999999   137\n",
            "48   48\n",
            "0.7999999999999999   138\n",
            "48   48\n",
            "0.7999999999999999   139\n",
            "49   49\n",
            "0.7999999999999999   140\n",
            "50   50\n",
            "0.7999999999999999   141\n",
            "50   50\n",
            "0.7999999999999999   142\n",
            "51   51\n",
            "0.7999999999999999   143\n",
            "52   52\n",
            "0.7999999999999999   144\n",
            "52   52\n",
            "0.7999999999999999   145\n",
            "53   53\n",
            "0.7999999999999999   146\n",
            "53   54\n",
            "0.7999999999999999   147\n",
            "54   55\n",
            "0.7999999999999999   148\n",
            "55   56\n",
            "0.7999999999999999   149\n",
            "56   57\n",
            "0.7999999999999999   150\n",
            "57   58\n",
            "0.7999999999999999   151\n",
            "58   59\n",
            "0.7999999999999999   152\n",
            "58   59\n",
            "0.7999999999999999   153\n",
            "59   60\n",
            "0.7999999999999999   154\n",
            "59   60\n",
            "0.7999999999999999   155\n",
            "59   60\n",
            "0.7999999999999999   156\n",
            "60   61\n",
            "0.7999999999999999   157\n",
            "60   61\n",
            "0.7999999999999999   158\n",
            "60   61\n",
            "0.7999999999999999   159\n",
            "60   61\n",
            "0.7999999999999999   160\n",
            "61   62\n",
            "0.7999999999999999   161\n",
            "61   62\n",
            "0.7999999999999999   162\n",
            "62   63\n",
            "0.7999999999999999   163\n",
            "62   63\n",
            "0.7999999999999999   164\n",
            "63   64\n",
            "0.7999999999999999   165\n",
            "64   65\n",
            "0.7999999999999999   166\n",
            "64   65\n",
            "0.7999999999999999   167\n",
            "65   66\n",
            "0.7999999999999999   168\n",
            "65   66\n",
            "0.7999999999999999   169\n",
            "66   67\n",
            "0.7999999999999999   170\n",
            "66   67\n",
            "0.7999999999999999   171\n",
            "67   68\n",
            "0.7999999999999999   172\n",
            "68   68\n",
            "0.7999999999999999   173\n",
            "69   69\n",
            "0.7999999999999999   174\n",
            "69   69\n",
            "0.7999999999999999   175\n",
            "69   69\n",
            "0.7999999999999999   176\n",
            "69   69\n",
            "0.7999999999999999   177\n",
            "69   69\n",
            "0.7999999999999999   178\n",
            "69   69\n",
            "0.7999999999999999   179\n",
            "70   70\n",
            "0.7999999999999999   180\n",
            "71   71\n",
            "0.7999999999999999   181\n",
            "71   71\n",
            "0.7999999999999999   182\n",
            "71   71\n",
            "0.7999999999999999   183\n",
            "72   72\n",
            "0.7999999999999999   184\n",
            "72   72\n",
            "0.7999999999999999   185\n",
            "72   72\n",
            "0.7999999999999999   186\n",
            "73   73\n",
            "0.7999999999999999   187\n",
            "74   74\n",
            "0.7999999999999999   188\n",
            "75   75\n",
            "0.7999999999999999   189\n",
            "76   76\n",
            "0.7999999999999999   190\n",
            "76   76\n",
            "0.7999999999999999   191\n",
            "77   76\n",
            "0.7999999999999999   192\n",
            "78   77\n",
            "0.7999999999999999   193\n",
            "78   77\n",
            "0.7999999999999999   194\n",
            "78   78\n",
            "0.7999999999999999   195\n",
            "79   79\n",
            "0.7999999999999999   196\n",
            "79   79\n",
            "0.7999999999999999   197\n",
            "79   79\n",
            "0.7999999999999999   198\n",
            "79   79\n",
            "0.7999999999999999   199\n",
            "79   79\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for key in result.keys():\n",
        "  print(key,' ',result[key])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1JP3AMDHUXWK",
        "outputId": "82dd4314-489f-4ca8-d07f-fa8ca447945b"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.1   (15, 19)\n",
            "0.2   (28, 41)\n",
            "0.30000000000000004   (41, 64)\n",
            "0.4   (54, 79)\n",
            "0.5   (61, 79)\n",
            "0.6   (68, 79)\n",
            "0.7   (75, 79)\n",
            "0.7999999999999999   (79, 79)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "PUor0wovVRZQ"
      },
      "execution_count": 89,
      "outputs": []
    }
  ]
}